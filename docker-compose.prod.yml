# Production-optimized Docker Compose configuration
# Use with: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d

version: '3.9'

services:
  # Production overrides for API server
  api-server:
    image: claudeflow/claude-flow:${DOCKER_IMAGE_TAG:-latest}
    restart: always
    deploy:
      replicas: ${API_REPLICAS:-2}
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      restart_policy:
        condition: any
        delay: 5s
        max_attempts: 3
        window: 120s
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"
    environment:
      - NODE_ENV=production
      - LOG_LEVEL=warn
      - ENABLE_PROFILING=false
      - ENABLE_TRACING=true

  # Production overrides for MCP server
  mcp-server:
    image: claudeflow/claude-flow-mcp:${DOCKER_IMAGE_TAG:-latest}
    restart: always
    deploy:
      replicas: ${MCP_REPLICAS:-2}
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "10"

  # Production PostgreSQL with replication
  postgres:
    image: pgvector/pgvector:pg16
    restart: always
    shm_size: 256mb
    environment:
      - POSTGRES_INITDB_ARGS=--data-checksums
      - POSTGRES_HOST_AUTH_METHOD=scram-sha-256
    command: >
      postgres
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=4MB
      -c min_wal_size=1GB
      -c max_wal_size=4GB
      -c max_worker_processes=4
      -c max_parallel_workers_per_gather=2
      -c max_parallel_workers=4
      -c max_connections=200
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - postgres-backup:/backup
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
    profiles:
      - ""  # Always enabled in production

  # Production Redis with persistence
  redis:
    restart: always
    command: >
      redis-server
      --appendonly yes
      --appendfsync everysec
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --tcp-backlog 511
      --tcp-keepalive 60
      --timeout 300
      --requirepass ${REDIS_PASSWORD}
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    profiles:
      - ""  # Always enabled in production

  # Production Nginx with SSL
  nginx:
    restart: always
    volumes:
      - ./nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - /etc/letsencrypt:/etc/letsencrypt:ro
      - nginx-cache:/var/cache/nginx
      - nginx-logs:/var/log/nginx
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "10"
    profiles:
      - ""  # Always enabled in production

  # Production monitoring stack
  prometheus:
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./monitoring/prometheus.prod.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    profiles:
      - ""  # Always enabled in production

  grafana:
    restart: always
    environment:
      - GF_SERVER_ROOT_URL=https://${DOMAIN_NAME}/grafana
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_AUTH_BASIC_ENABLED=false
      - GF_AUTH_DISABLE_LOGIN_FORM=false
      - GF_SMTP_ENABLED=true
      - GF_SMTP_HOST=${SMTP_HOST}
      - GF_SMTP_USER=${SMTP_USER}
      - GF_SMTP_PASSWORD=${SMTP_PASSWORD}
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    profiles:
      - ""  # Always enabled in production

  # Production backup with cloud storage
  backup:
    restart: always
    environment:
      - BACKUP_SCHEDULE=0 2,14 * * *  # Twice daily
      - BACKUP_RETENTION_DAYS=30
      - BACKUP_COMPRESSION=true
      - BACKUP_ENCRYPTION=true
      - ENCRYPTION_KEY=${BACKUP_ENCRYPTION_KEY}
    volumes:
      - claude-flow-data:/app/data:ro
      - claude-flow-databases:/app/databases:ro
      - backup-data:/backups
      - ./backup-scripts:/scripts:ro
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    profiles:
      - ""  # Always enabled in production

  # Log aggregation with Loki
  loki:
    image: grafana/loki:latest
    container_name: claude-flow-loki
    restart: always
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./monitoring/loki.yml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    networks:
      - claude-flow-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    profiles:
      - ""  # Always enabled in production

  # Promtail for log collection
  promtail:
    image: grafana/promtail:latest
    container_name: claude-flow-promtail
    restart: always
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
    command: -config.file=/etc/promtail/config.yml
    networks:
      - claude-flow-network
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    profiles:
      - ""  # Always enabled in production

# Production volumes with backup
volumes:
  # Data volumes
  claude-flow-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATA_PATH:-/data/claude-flow/data}
  
  claude-flow-databases:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${DATABASE_PATH:-/data/claude-flow/databases}
  
  # PostgreSQL volumes
  postgres-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${POSTGRES_DATA_PATH:-/data/claude-flow/postgres}
  
  postgres-backup:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${POSTGRES_BACKUP_PATH:-/data/claude-flow/postgres-backup}
  
  # Monitoring volumes
  prometheus-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PROMETHEUS_DATA_PATH:-/data/claude-flow/prometheus}
  
  loki-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${LOKI_DATA_PATH:-/data/claude-flow/loki}
  
  # Other volumes
  nginx-logs:
    driver: local
  
  backup-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_PATH:-/data/claude-flow/backups}

# Production network with encryption
networks:
  claude-flow-network:
    driver: overlay
    driver_opts:
      encrypted: "true"
    ipam:
      config:
        - subnet: 172.30.0.0/16