{
  "version": 3,
  "sources": ["../../src/config/llm-providers.config.ts"],
  "sourcesContent": ["/**\n * LLM Provider Configuration\n * \n * Centralized configuration for all LLM providers including models,\n * rate limits, context sizes, and routing strategies.\n */\n\nexport interface ProviderConfig {\n  name: string;\n  displayName: string;\n  models: string[];\n  defaultModel: string;\n  maxContextTokens: number;\n  maxOutputTokens: number;\n  rateLimits?: {\n    requestsPerMinute?: number;\n    tokensPerMinute?: number;\n    cooldownMinutes?: number;\n  };\n  features: {\n    structuredOutput: boolean;\n    fileOperations: boolean;\n    codebaseAware: boolean;\n    streaming: boolean;\n  };\n  routing: {\n    priority: number; // 1 = highest priority\n    useForSmallContext: boolean; // < 10K tokens\n    useForLargeContext: boolean; // > 10K tokens\n    fallbackOrder: number;\n  };\n}\n\nexport const LLM_PROVIDER_CONFIG: Record<string, ProviderConfig> = {\n  'github-models': {\n    name: 'github-models',\n    displayName: 'GitHub Models (GPT-5)',\n    models: ['openai/gpt-5', 'openai/gpt-4o', 'mistralai/codestral'],\n    defaultModel: 'openai/gpt-5',\n    maxContextTokens: 4000,    // GPT-5 input limit\n    maxOutputTokens: 128000,   // GPT-5 output limit\n    rateLimits: {\n      requestsPerMinute: 60,\n      tokensPerMinute: 50000,\n      cooldownMinutes: 60\n    },\n    features: {\n      structuredOutput: true,\n      fileOperations: false,\n      codebaseAware: false,\n      streaming: false\n    },\n    routing: {\n      priority: 2,               // Lower priority than Copilot\n      useForSmallContext: true,  // Best for small, focused tasks\n      useForLargeContext: false, // Input limit too small for large contexts\n      fallbackOrder: 2           // Use after Copilot\n    }\n  },\n\n  'copilot': {\n    name: 'copilot',\n    displayName: 'GitHub Copilot (Enterprise)',\n    models: ['gpt-4.1', 'gpt-3.5-turbo'],\n    defaultModel: 'gpt-4.1',\n    maxContextTokens: 200000,  // GitHub Copilot's 200K context window\n    maxOutputTokens: 16000,    // Higher output limit for enterprise\n    rateLimits: {\n      requestsPerMinute: 300,  // Enterprise account - high limits\n      tokensPerMinute: 200000,\n      cooldownMinutes: 10      // Shorter cooldown for enterprise\n    },\n    features: {\n      structuredOutput: true,\n      fileOperations: false,\n      codebaseAware: false,\n      streaming: true\n    },\n    routing: {\n      priority: 1,               // High priority due to large context + enterprise\n      useForSmallContext: true,  // Can handle any size efficiently\n      useForLargeContext: true,  // Excellent for large contexts with 200K limit\n      fallbackOrder: 1           // Prefer over GitHub Models for large contexts\n    }\n  },\n\n  'claude-code': {\n    name: 'claude-code',\n    displayName: 'Claude Code CLI',\n    models: ['sonnet', 'haiku'],\n    defaultModel: 'sonnet',\n    maxContextTokens: 200000,  // Claude's large context\n    maxOutputTokens: 8000,\n    features: {\n      structuredOutput: true,\n      fileOperations: true,     // Can read/write files directly\n      codebaseAware: true,      // Best codebase understanding\n      streaming: false\n    },\n    routing: {\n      priority: 1,\n      useForSmallContext: true,\n      useForLargeContext: true,  // Excellent for codebase analysis\n      fallbackOrder: 0          // First preference when available\n    }\n  },\n\n  'gemini': {\n    name: 'gemini',\n    displayName: 'Gemini CLI',\n    models: ['gemini-pro', 'gemini-flash'],\n    defaultModel: 'gemini-pro',\n    maxContextTokens: 1000000, // Very large context\n    maxOutputTokens: 8000,\n    rateLimits: {\n      requestsPerMinute: 15,    // Conservative rate limits\n      tokensPerMinute: 32000,\n      cooldownMinutes: 60       // Long cooldown after rate limit\n    },\n    features: {\n      structuredOutput: true,\n      fileOperations: true,\n      codebaseAware: true,\n      streaming: false\n    },\n    routing: {\n      priority: 4,\n      useForSmallContext: false,\n      useForLargeContext: true,\n      fallbackOrder: 3          // Last resort due to rate limits\n    }\n  },\n\n  'gemini-direct': {\n    name: 'gemini-direct',\n    displayName: 'Gemini 2.5 Flash (Main)',\n    models: ['gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-1.5-pro'],\n    defaultModel: 'gemini-2.5-flash',  // Main workhorse model\n    maxContextTokens: 1000000,         // 1M context window\n    maxOutputTokens: 8192,\n    rateLimits: {\n      requestsPerMinute: 60,           // Higher limits with OAuth/API key\n      tokensPerMinute: 100000,\n      cooldownMinutes: 30              // Shorter cooldown than CLI\n    },\n    features: {\n      structuredOutput: true,\n      fileOperations: false,           // Direct API, no file operations\n      codebaseAware: false,           // No CLI context\n      streaming: true                 // Real-time streaming support\n    },\n    routing: {\n      priority: 2,                    // High priority - main model\n      useForSmallContext: true,       // Use for all regular tasks\n      useForLargeContext: true,       // 1M context handles everything\n      fallbackOrder: 1               // Primary after GitHub Models\n    }\n  },\n\n  'gemini-pro': {\n    name: 'gemini-pro',\n    displayName: 'Gemini 2.5 Pro (Complex)',\n    models: ['gemini-2.5-pro', 'gemini-1.5-pro'],\n    defaultModel: 'gemini-2.5-pro',    // High complexity only\n    maxContextTokens: 1000000,         // 1M context window\n    maxOutputTokens: 8192,\n    rateLimits: {\n      requestsPerMinute: 60,\n      tokensPerMinute: 100000,\n      cooldownMinutes: 30\n    },\n    features: {\n      structuredOutput: true,\n      fileOperations: false,\n      codebaseAware: false,\n      streaming: true\n    },\n    routing: {\n      priority: 4,                    // Lower priority - special use only\n      useForSmallContext: false,      // Don't use for regular tasks\n      useForLargeContext: false,      // Only for high complexity\n      fallbackOrder: 4               // Special use, not in regular rotation\n    }\n  }\n};\n\n/**\n * Routing Strategy Configuration\n */\nexport const ROUTING_STRATEGY = {\n  // Context size thresholds for routing decisions\n  SMALL_CONTEXT_THRESHOLD: 10000,   // < 10K chars = small context\n  LARGE_CONTEXT_THRESHOLD: 100000,  // > 100K chars = very large context\n  \n  // Provider selection strategy\n  STRATEGY: 'smart' as 'smart' | 'fallback' | 'round-robin',\n  \n  // Automatic failover settings\n  AUTO_FAILOVER: true,\n  MAX_RETRIES_PER_PROVIDER: 2,\n  \n  // Context-based routing rules\n  RULES: {\n    // Regular tasks: GitHub Models (free) \u2192 Gemini 2.5 Flash (main) \u2192 Copilot\n    smallContext: ['github-models', 'gemini-direct', 'copilot', 'claude-code'],\n    \n    // All contexts: Gemini 2.5 Flash is the main workhorse after GitHub Models\n    largeContext: ['github-models', 'gemini-direct', 'copilot', 'claude-code', 'gemini'],\n    \n    // File operations: Use CLI providers only\n    fileOperations: ['claude-code', 'gemini'],\n    \n    // Code analysis: Gemini 2.5 Flash main, Pro for complex reasoning\n    codeAnalysis: ['github-models', 'gemini-direct', 'gemini-pro', 'copilot', 'claude-code'],\n    \n    // Complex reasoning: Use Pro model specifically\n    complexReasoning: ['gemini-pro', 'copilot', 'claude-code'],\n    \n    // JSON responses: All providers support structured output\n    structuredOutput: ['github-models', 'gemini-direct', 'gemini-pro', 'copilot', 'claude-code', 'gemini']\n  }\n};\n\n/**\n * Get optimal provider for a given context and requirements\n */\nexport function getOptimalProvider(context: {\n  contentLength: number;\n  requiresFileOps: boolean;\n  requiresCodebaseAware: boolean;\n  requiresStructuredOutput: boolean;\n  taskType: 'analysis' | 'generation' | 'review' | 'custom';\n}): string[] {\n  const { contentLength, requiresFileOps, requiresCodebaseAware, requiresStructuredOutput } = context;\n  \n  // Determine context size category\n  const isSmallContext = contentLength < ROUTING_STRATEGY.SMALL_CONTEXT_THRESHOLD;\n  const isLargeContext = contentLength > ROUTING_STRATEGY.LARGE_CONTEXT_THRESHOLD;\n  const estimatedTokens = Math.ceil(contentLength / 4); // Rough estimation\n  \n  // Special routing for very large contexts\n  if (estimatedTokens > 150000) {\n    // Only Gemini and Claude Code can handle > 150K tokens\n    return ['gemini', 'claude-code'];\n  }\n  \n  // Get available providers based on requirements\n  let candidates: string[] = [];\n  \n  // Filter by context size capabilities and token limits\n  for (const [providerId, config] of Object.entries(LLM_PROVIDER_CONFIG)) {\n    const canHandleTokens = estimatedTokens <= config.maxContextTokens;\n    \n    const meetsContextRequirements = \n      (isSmallContext && config.routing.useForSmallContext) ||\n      (isLargeContext && config.routing.useForLargeContext) ||\n      (!isSmallContext && !isLargeContext); // Medium context\n      \n    const meetsFeatureRequirements =\n      (!requiresFileOps || config.features.fileOperations) &&\n      (!requiresCodebaseAware || config.features.codebaseAware) &&\n      (!requiresStructuredOutput || config.features.structuredOutput);\n      \n    if (canHandleTokens && meetsContextRequirements && meetsFeatureRequirements) {\n      candidates.push(providerId);\n    }\n  }\n  \n  // Sort by priority and fallback order\n  candidates.sort((a, b) => {\n    const configA = LLM_PROVIDER_CONFIG[a];\n    const configB = LLM_PROVIDER_CONFIG[b];\n    \n    // Primary sort: priority (lower number = higher priority)\n    if (configA.routing.priority !== configB.routing.priority) {\n      return configA.routing.priority - configB.routing.priority;\n    }\n    \n    // Secondary sort: fallback order\n    return configA.routing.fallbackOrder - configB.routing.fallbackOrder;\n  });\n  \n  return candidates;\n}\n\n/**\n * Export default configuration for easy import\n */\nexport default {\n  providers: LLM_PROVIDER_CONFIG,\n  routing: ROUTING_STRATEGY,\n  getOptimalProvider\n};"],
  "mappings": ";;;;;;;;;;;;;AAiCO,IAAM,sBAAsD;AAAA,EACjE,iBAAiB;AAAA,IACf,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,gBAAgB,iBAAiB,qBAAqB;AAAA,IAC/D,cAAc;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA;AAAA,IACjB,YAAY;AAAA,MACV,mBAAmB;AAAA,MACnB,iBAAiB;AAAA,MACjB,iBAAiB;AAAA,IACnB;AAAA,IACA,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,WAAW;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA;AAAA,MACV,oBAAoB;AAAA;AAAA,MACpB,oBAAoB;AAAA;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AAAA,EAEA,WAAW;AAAA,IACT,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,WAAW,eAAe;AAAA,IACnC,cAAc;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA;AAAA,IACjB,YAAY;AAAA,MACV,mBAAmB;AAAA;AAAA,MACnB,iBAAiB;AAAA,MACjB,iBAAiB;AAAA;AAAA,IACnB;AAAA,IACA,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,WAAW;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA;AAAA,MACV,oBAAoB;AAAA;AAAA,MACpB,oBAAoB;AAAA;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AAAA,EAEA,eAAe;AAAA,IACb,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,UAAU,OAAO;AAAA,IAC1B,cAAc;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA,IACjB,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA;AAAA,MAChB,eAAe;AAAA;AAAA,MACf,WAAW;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA,MACV,oBAAoB;AAAA,MACpB,oBAAoB;AAAA;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AAAA,EAEA,UAAU;AAAA,IACR,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,cAAc,cAAc;AAAA,IACrC,cAAc;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA,IACjB,YAAY;AAAA,MACV,mBAAmB;AAAA;AAAA,MACnB,iBAAiB;AAAA,MACjB,iBAAiB;AAAA;AAAA,IACnB;AAAA,IACA,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,WAAW;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA,MACV,oBAAoB;AAAA,MACpB,oBAAoB;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AAAA,EAEA,iBAAiB;AAAA,IACf,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,oBAAoB,kBAAkB,gBAAgB;AAAA,IAC/D,cAAc;AAAA;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA,IACjB,YAAY;AAAA,MACV,mBAAmB;AAAA;AAAA,MACnB,iBAAiB;AAAA,MACjB,iBAAiB;AAAA;AAAA,IACnB;AAAA,IACA,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA;AAAA,MAChB,eAAe;AAAA;AAAA,MACf,WAAW;AAAA;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA;AAAA,MACV,oBAAoB;AAAA;AAAA,MACpB,oBAAoB;AAAA;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AAAA,EAEA,cAAc;AAAA,IACZ,MAAM;AAAA,IACN,aAAa;AAAA,IACb,QAAQ,CAAC,kBAAkB,gBAAgB;AAAA,IAC3C,cAAc;AAAA;AAAA,IACd,kBAAkB;AAAA;AAAA,IAClB,iBAAiB;AAAA,IACjB,YAAY;AAAA,MACV,mBAAmB;AAAA,MACnB,iBAAiB;AAAA,MACjB,iBAAiB;AAAA,IACnB;AAAA,IACA,UAAU;AAAA,MACR,kBAAkB;AAAA,MAClB,gBAAgB;AAAA,MAChB,eAAe;AAAA,MACf,WAAW;AAAA,IACb;AAAA,IACA,SAAS;AAAA,MACP,UAAU;AAAA;AAAA,MACV,oBAAoB;AAAA;AAAA,MACpB,oBAAoB;AAAA;AAAA,MACpB,eAAe;AAAA;AAAA,IACjB;AAAA,EACF;AACF;AAKO,IAAM,mBAAmB;AAAA;AAAA,EAE9B,yBAAyB;AAAA;AAAA,EACzB,yBAAyB;AAAA;AAAA;AAAA,EAGzB,UAAU;AAAA;AAAA,EAGV,eAAe;AAAA,EACf,0BAA0B;AAAA;AAAA,EAG1B,OAAO;AAAA;AAAA,IAEL,cAAc,CAAC,iBAAiB,iBAAiB,WAAW,aAAa;AAAA;AAAA,IAGzE,cAAc,CAAC,iBAAiB,iBAAiB,WAAW,eAAe,QAAQ;AAAA;AAAA,IAGnF,gBAAgB,CAAC,eAAe,QAAQ;AAAA;AAAA,IAGxC,cAAc,CAAC,iBAAiB,iBAAiB,cAAc,WAAW,aAAa;AAAA;AAAA,IAGvF,kBAAkB,CAAC,cAAc,WAAW,aAAa;AAAA;AAAA,IAGzD,kBAAkB,CAAC,iBAAiB,iBAAiB,cAAc,WAAW,eAAe,QAAQ;AAAA,EACvG;AACF;AAKO,SAAS,mBAAmB,SAMtB;AACX,QAAM,EAAE,eAAe,iBAAiB,uBAAuB,yBAAyB,IAAI;AAG5F,QAAM,iBAAiB,gBAAgB,iBAAiB;AACxD,QAAM,iBAAiB,gBAAgB,iBAAiB;AACxD,QAAM,kBAAkB,KAAK,KAAK,gBAAgB,CAAC;AAGnD,MAAI,kBAAkB,MAAQ;AAE5B,WAAO,CAAC,UAAU,aAAa;AAAA,EACjC;AAGA,MAAI,aAAuB,CAAC;AAG5B,aAAW,CAAC,YAAY,MAAM,KAAK,OAAO,QAAQ,mBAAmB,GAAG;AACtE,UAAM,kBAAkB,mBAAmB,OAAO;AAElD,UAAM,2BACH,kBAAkB,OAAO,QAAQ,sBACjC,kBAAkB,OAAO,QAAQ,sBACjC,CAAC,kBAAkB,CAAC;AAEvB,UAAM,4BACH,CAAC,mBAAmB,OAAO,SAAS,oBACpC,CAAC,yBAAyB,OAAO,SAAS,mBAC1C,CAAC,4BAA4B,OAAO,SAAS;AAEhD,QAAI,mBAAmB,4BAA4B,0BAA0B;AAC3E,iBAAW,KAAK,UAAU;AAAA,IAC5B;AAAA,EACF;AAGA,aAAW,KAAK,CAAC,GAAG,MAAM;AACxB,UAAM,UAAU,oBAAoB,CAAC;AACrC,UAAM,UAAU,oBAAoB,CAAC;AAGrC,QAAI,QAAQ,QAAQ,aAAa,QAAQ,QAAQ,UAAU;AACzD,aAAO,QAAQ,QAAQ,WAAW,QAAQ,QAAQ;AAAA,IACpD;AAGA,WAAO,QAAQ,QAAQ,gBAAgB,QAAQ,QAAQ;AAAA,EACzD,CAAC;AAED,SAAO;AACT;AAzDgB;",
  "names": []
}
