/**
 * Comprehensive MCP Workflow Examples for zen-swarm
 * Demonstrates all MCP tool capabilities with real-world scenarios
 */

import WebSocket from 'ws';
import { v4 as uuidv4 } from 'uuid';

// MCP Client wrapper for easier use
class RuvSwarmMCP {
  constructor(url = 'ws://localhost:3000/mcp') {
    this.url = url;
    this.ws = null;
    this.requestId = 0;
    this.pendingRequests = new Map();
    this.eventHandlers = new Map();
  }

  async connect(clientInfo = { name: 'zen-swarm-client', version: '1.0.0' }) {
    return new Promise((resolve, reject) => {
      this.ws = new WebSocket(this.url);

      this.ws.on('open', async () => {
        try {
          const result = await this.call('initialize', { clientInfo });
          console.log(
            `‚úÖ Connected to RUV-Swarm MCP Server v${result.serverInfo.version}`,
          );
          resolve(result);
        } catch (error) {
          reject(error);
        }
      });

      this.ws.on('message', (data) => {
        const message = JSON.parse(data.toString());

        if (message.id && this.pendingRequests.has(message.id)) {
          const { resolve, reject } = this.pendingRequests.get(message.id);
          this.pendingRequests.delete(message.id);

          if (message.error) {
            reject(new Error(message.error.message));
          } else {
            resolve(message.result);
          }
        } else if (message.method) {
          // Handle notifications
          this.handleNotification(message);
        }
      });

      this.ws.on('error', reject);
    });
  }

  async disconnect() {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }

  async call(method, params = null) {
    const id = ++this.requestId;
    const request = { jsonrpc: '2.0', id, method, params };

    return new Promise((resolve, reject) => {
      this.pendingRequests.set(id, { resolve, reject });
      this.ws.send(JSON.stringify(request));

      setTimeout(() => {
        if (this.pendingRequests.has(id)) {
          this.pendingRequests.delete(id);
          reject(new Error(`Request ${id} timed out`));
        }
      }, 30000);
    });
  }

  async tool(name, args) {
    return this.call('tools/call', { name, arguments: args });
  }

  on(event, handler) {
    if (!this.eventHandlers.has(event)) {
      this.eventHandlers.set(event, []);
    }
    this.eventHandlers.get(event).push(handler);
  }

  handleNotification(message) {
    const handlers = this.eventHandlers.get(message.method) || [];
    handlers.forEach((handler) => handler(message.params));
  }

  // Helper methods for common operations
  async spawnAgent(type, name = null, capabilities = {}) {
    return this.tool('zen-swarm.spawn', {
      agent_type: type,
      name,
      capabilities,
    });
  }

  async orchestrate(objective, options = {}) {
    return this.tool('zen-swarm.orchestrate', {
      objective,
      strategy: options.strategy || 'development',
      mode: options.mode || 'hierarchical',
      max_agents: options.maxAgents || 5,
      parallel: options.parallel !== false,
    });
  }

  async storeMemory(key, value, ttl = null) {
    return this.tool('zen-swarm.memory.store', {
      key,
      value,
      ttl_secs: ttl,
    });
  }

  async getMemory(key) {
    return this.tool('zen-swarm.memory.get', { key });
  }

  async createTask(
    type,
    description,
    priority = 'medium',
    assignedAgent = null,
  ) {
    return this.tool('zen-swarm.task.create', {
      task_type: type,
      description,
      priority,
      assigned_agent: assignedAgent,
    });
  }

  async query(filter = {}, includeMetrics = false) {
    return this.tool('zen-swarm.query', {
      filter,
      include_metrics: includeMetrics,
    });
  }

  async monitor(eventTypes = ['all'], duration = 60) {
    return this.tool('zen-swarm.monitor', {
      event_types: eventTypes,
      duration_secs: duration,
    });
  }

  async optimize(
    targetMetric = 'throughput',
    constraints = {},
    autoApply = false,
  ) {
    return this.tool('zen-swarm.optimize', {
      target_metric: targetMetric,
      constraints,
      auto_apply: autoApply,
    });
  }

  async executeWorkflow(path, parameters = {}, async = false) {
    return this.tool('zen-swarm.workflow.execute', {
      workflow_path: path,
      parameters,
      async_execution: async,
    });
  }

  async listAgents(includeInactive = false, sortBy = 'created_at') {
    return this.tool('zen-swarm.agent.list', {
      include_inactive: includeInactive,
      sort_by: sortBy,
    });
  }
}

// Example Workflows

/**
 * Workflow 1: Complete Web Application Development
 * Demonstrates full development lifecycle with multiple agent types
 */
async function webAppDevelopmentWorkflow(client) {
  console.log('\nüöÄ Workflow 1: Complete Web Application Development\n');

  // Phase 1: Research and Planning
  console.log('üìã Phase 1: Research and Planning');

  // Spawn research team
  const researchLead = await client.spawnAgent('researcher', 'research-lead', {
    specialization: 'web_architecture',
    max_concurrent_tasks: 3,
  });
  console.log(`  ‚úÖ Spawned Research Lead: ${researchLead.agent_id}`);

  // Store project requirements
  await client.storeMemory('project_requirements', {
    name: 'E-Commerce Platform',
    features: [
      'User authentication',
      'Product catalog',
      'Shopping cart',
      'Payment integration',
      'Order management',
    ],
    tech_stack: {
      frontend: 'React + TypeScript',
      backend: 'Node.js + Express',
      database: 'PostgreSQL',
      cache: 'Redis',
    },
    constraints: {
      timeline: '3 months',
      budget: '$50,000',
      team_size: 5,
    },
  });
  console.log('  ‚úÖ Stored project requirements');

  // Create research tasks
  const researchTask = await client.createTask(
    'research',
    'Analyze best practices for e-commerce platform architecture',
    'high',
    researchLead.agent_id,
  );
  console.log(`  ‚úÖ Created research task: ${researchTask.task_id}`);

  // Phase 2: Architecture Design
  console.log('\nüèóÔ∏è  Phase 2: Architecture Design');

  // Orchestrate architecture design
  const architectureResult = await client.orchestrate(
    'Design scalable microservices architecture for e-commerce platform',
    {
      strategy: 'development',
      mode: 'centralized',
      maxAgents: 3,
    },
  );
  console.log(
    `  ‚úÖ Architecture orchestration started: ${architectureResult.task_id}`,
  );

  // Store architecture decisions
  await client.storeMemory('architecture_decisions', {
    pattern: 'microservices',
    services: [
      {
        name: 'auth-service',
        responsibility: 'User authentication and authorization',
      },
      { name: 'product-service', responsibility: 'Product catalog management' },
      { name: 'cart-service', responsibility: 'Shopping cart operations' },
      {
        name: 'order-service',
        responsibility: 'Order processing and management',
      },
      { name: 'payment-service', responsibility: 'Payment processing' },
    ],
    communication: 'REST API with message queue for async operations',
    deployment: 'Kubernetes with auto-scaling',
  });

  // Phase 3: Development Team Assembly
  console.log('\nüë• Phase 3: Development Team Assembly');

  const team = {
    frontend: [],
    backend: [],
    testers: [],
  };

  // Spawn frontend developers
  for (let i = 0; i < 2; i++) {
    const dev = await client.spawnAgent('coder', `frontend-dev-${i}`, {
      specialization: 'react',
      skills: ['typescript', 'redux', 'css-in-js'],
    });
    team.frontend.push(dev.agent_id);
  }
  console.log(`  ‚úÖ Spawned ${team.frontend.length} frontend developers`);

  // Spawn backend developers
  for (let i = 0; i < 3; i++) {
    const dev = await client.spawnAgent('coder', `backend-dev-${i}`, {
      specialization: 'nodejs',
      skills: ['express', 'postgresql', 'redis', 'microservices'],
    });
    team.backend.push(dev.agent_id);
  }
  console.log(`  ‚úÖ Spawned ${team.backend.length} backend developers`);

  // Spawn testers
  for (let i = 0; i < 2; i++) {
    const tester = await client.spawnAgent('tester', `qa-engineer-${i}`, {
      specialization: i === 0 ? 'unit_testing' : 'integration_testing',
      tools: ['jest', 'cypress', 'postman'],
    });
    team.testers.push(tester.agent_id);
  }
  console.log(`  ‚úÖ Spawned ${team.testers.length} QA engineers`);

  // Store team structure
  await client.storeMemory('development_team', team);

  // Phase 4: Sprint Planning and Task Distribution
  console.log('\nüìä Phase 4: Sprint Planning and Task Distribution');

  // Create development tasks for Sprint 1
  const sprint1Tasks = [
    {
      type: 'development',
      desc: 'Implement auth-service with JWT',
      assignee: team.backend[0],
    },
    {
      type: 'development',
      desc: 'Create user registration UI',
      assignee: team.frontend[0],
    },
    {
      type: 'development',
      desc: 'Setup PostgreSQL schemas',
      assignee: team.backend[1],
    },
    {
      type: 'testing',
      desc: 'Write auth-service unit tests',
      assignee: team.testers[0],
    },
    {
      type: 'development',
      desc: 'Implement product-service CRUD',
      assignee: team.backend[2],
    },
    {
      type: 'development',
      desc: 'Create product listing components',
      assignee: team.frontend[1],
    },
  ];

  const taskIds = [];
  for (const task of sprint1Tasks) {
    const result = await client.createTask(
      task.type,
      task.desc,
      'high',
      task.assignee,
    );
    taskIds.push(result.task_id);
  }
  console.log(`  ‚úÖ Created ${taskIds.length} tasks for Sprint 1`);

  // Phase 5: Development Monitoring
  console.log('\nüì° Phase 5: Development Monitoring');

  // Set up event monitoring
  client.on('zen-swarm/event', (event) => {
    console.log(
      `  üì¢ Event: ${event.event.type} - ${event.event.data.message || ''}`,
    );
  });

  // Start monitoring
  await client.monitor(['task_started', 'task_completed', 'agent_message'], 10);
  console.log('  ‚úÖ Monitoring active for 10 seconds');

  // Phase 6: Performance Optimization
  console.log('\n‚ö° Phase 6: Performance Optimization');

  const optimization = await client.optimize(
    'throughput',
    {
      max_memory_mb: 1024,
      max_cpu_percent: 75,
      min_agents: 5,
    },
    false,
  );

  console.log('  ‚úÖ Optimization recommendations:');
  optimization.recommendations.forEach((rec) => {
    console.log(`    - ${rec}`);
  });

  // Phase 7: Status Review
  console.log('\nüìà Phase 7: Status Review');

  const status = await client.query({}, true);
  console.log(`  Total Agents: ${status.total_agents}`);
  console.log(`  Active Tasks: ${status.active_tasks}`);
  console.log(`  Completed Tasks: ${status.completed_tasks}`);

  if (status.metrics) {
    console.log('  Performance Metrics:');
    console.log(`    - CPU Usage: ${status.metrics.cpu_usage || 'N/A'}%`);
    console.log(
      `    - Memory Usage: ${status.metrics.memory_usage || 'N/A'}MB`,
    );
    console.log(
      `    - Task Throughput: ${status.metrics.task_throughput || 'N/A'}/min`,
    );
  }
}

/**
 * Workflow 2: AI Research Swarm
 * Demonstrates distributed research with neural learning
 */
async function aiResearchSwarmWorkflow(client) {
  console.log('\nüß† Workflow 2: AI Research Swarm\n');

  // Initialize research context
  await client.storeMemory('research_context', {
    topic: 'Large Language Model Optimization Techniques',
    subtopics: [
      'Model compression and quantization',
      'Knowledge distillation',
      'Efficient attention mechanisms',
      'Parameter-efficient fine-tuning',
      'Inference optimization',
    ],
    output_format: 'comprehensive_report',
    deadline: new Date(Date.now() + 24 * 60 * 60 * 1000).toISOString(),
  });

  // Orchestrate research swarm
  const research = await client.orchestrate(
    'Conduct comprehensive research on LLM optimization techniques',
    {
      strategy: 'research',
      mode: 'mesh', // Mesh network for collaborative research
      maxAgents: 8,
      parallel: true,
    },
  );
  console.log(`  ‚úÖ Research swarm initiated: ${research.task_id}`);

  // Create specialized research agents
  const researchers = [];
  const specializations = [
    'model_compression',
    'attention_mechanisms',
    'training_efficiency',
    'hardware_optimization',
  ];

  for (const spec of specializations) {
    const agent = await client.spawnAgent('researcher', `${spec}-expert`, {
      specialization: spec,
      neural_enabled: true,
      learning_rate: 0.01,
      knowledge_base: 'academic_papers',
    });
    researchers.push(agent);
    console.log(`  ‚úÖ Spawned ${spec} expert: ${agent.agent_id}`);
  }

  // Create analysis tasks
  for (let i = 0; i < researchers.length; i++) {
    await client.createTask(
      'research',
      `Deep dive into ${specializations[i]} techniques and innovations`,
      'critical',
      researchers[i].agent_id,
    );
  }

  // Store intermediate findings
  await client.storeMemory('research_findings_compression', {
    techniques: {
      quantization: {
        methods: ['INT8', 'INT4', 'Mixed precision'],
        performance_impact: '2-4x speedup with <1% accuracy loss',
        best_practices: [
          'Post-training quantization',
          'Quantization-aware training',
        ],
      },
      pruning: {
        methods: [
          'Magnitude pruning',
          'Structured pruning',
          'Movement pruning',
        ],
        sparsity_levels: '50-90% achievable',
        hardware_support: ['NVIDIA Ampere', 'Intel VNNI'],
      },
    },
    timestamp: new Date().toISOString(),
  });

  // Neural network pattern learning simulation
  console.log('\nüî¨ Neural Pattern Learning');

  const analyst = await client.spawnAgent(
    'analyst',
    'neural-pattern-analyzer',
    {
      neural_enabled: true,
      pattern_recognition: true,
      learning_epochs: 1000,
    },
  );

  // Store pattern data for learning
  await client.storeMemory('optimization_patterns', {
    patterns: [
      { technique: 'quantization', model_size: 'large', speedup: 3.2 },
      { technique: 'pruning', model_size: 'large', speedup: 2.8 },
      { technique: 'distillation', model_size: 'medium', speedup: 1.5 },
      { technique: 'quantization', model_size: 'medium', speedup: 2.1 },
    ],
    target: 'predict optimal technique based on model characteristics',
  });

  await client.createTask(
    'analysis',
    'Learn optimization patterns and predict best techniques',
    'high',
    analyst.agent_id,
  );

  console.log(`  ‚úÖ Neural learning task created for pattern analysis`);
}

/**
 * Workflow 3: Continuous Integration/Deployment Pipeline
 * Demonstrates maintenance and optimization workflows
 */
async function cicdPipelineWorkflow(client) {
  console.log('\nüîÑ Workflow 3: CI/CD Pipeline Automation\n');

  // Setup pipeline configuration
  await client.storeMemory('pipeline_config', {
    stages: ['build', 'test', 'security_scan', 'deploy'],
    environments: ['dev', 'staging', 'production'],
    triggers: {
      push: 'feature/*',
      pull_request: 'main',
      schedule: '0 2 * * *', // Daily at 2 AM
    },
    notifications: {
      slack: '#deployments',
      email: 'devops@company.com',
    },
  });

  // Spawn CI/CD agents
  const cicdAgents = {
    builder: await client.spawnAgent('coder', 'build-agent', {
      tools: ['docker', 'npm', 'webpack'],
      parallel_builds: 4,
    }),
    tester: await client.spawnAgent('tester', 'test-runner', {
      frameworks: ['jest', 'cypress', 'playwright'],
      coverage_threshold: 80,
    }),
    security: await client.spawnAgent('analyst', 'security-scanner', {
      tools: ['snyk', 'sonarqube', 'owasp-zap'],
      vulnerability_threshold: 'medium',
    }),
    deployer: await client.spawnAgent('coder', 'deploy-agent', {
      platforms: ['kubernetes', 'aws', 'vercel'],
      rollback_enabled: true,
    }),
  };

  console.log('  ‚úÖ CI/CD agent team assembled');

  // Execute pipeline workflow
  const fs = require('fs').promises;
  const pipelineWorkflow = {
    name: 'ci-cd-pipeline',
    version: '1.0.0',
    stages: [
      {
        name: 'build',
        agent: cicdAgents.builder.agent_id,
        tasks: [
          'Pull latest code from repository',
          'Install dependencies',
          'Run build process',
          'Create Docker image',
        ],
        on_failure: 'abort',
      },
      {
        name: 'test',
        agent: cicdAgents.tester.agent_id,
        tasks: [
          'Run unit tests',
          'Run integration tests',
          'Generate coverage report',
          'Run E2E tests',
        ],
        parallel: true,
        on_failure: 'abort',
      },
      {
        name: 'security',
        agent: cicdAgents.security.agent_id,
        tasks: [
          'Scan dependencies for vulnerabilities',
          'Run static code analysis',
          'Perform security audit',
        ],
        on_failure: 'notify',
      },
      {
        name: 'deploy',
        agent: cicdAgents.deployer.agent_id,
        tasks: [
          'Deploy to staging environment',
          'Run smoke tests',
          'Deploy to production',
          'Verify deployment health',
        ],
        requires_approval: true,
      },
    ],
  };

  const workflowPath = '/tmp/cicd-workflow.json';
  await fs.writeFile(workflowPath, JSON.stringify(pipelineWorkflow, null, 2));

  const execution = await client.executeWorkflow(
    workflowPath,
    {
      branch: 'feature/user-auth',
      commit: 'abc123def',
      author: 'developer@company.com',
    },
    true,
  );

  console.log(`  ‚úÖ Pipeline workflow started: ${execution.workflow_id}`);

  // Monitor pipeline execution
  await client.monitor(
    ['task_started', 'task_completed', 'workflow_stage_complete'],
    15,
  );

  // Optimize pipeline performance
  const pipelineOptimization = await client.optimize(
    'latency',
    {
      max_parallel_tasks: 10,
      cache_dependencies: true,
      incremental_builds: true,
    },
    true,
  );

  console.log('  ‚úÖ Pipeline optimizations applied');

  // Clean up
  await fs.unlink(workflowPath);
}

/**
 * Workflow 4: Data Analysis and Machine Learning Pipeline
 * Demonstrates complex data processing with neural networks
 */
async function dataAnalysisPipeline(client) {
  console.log('\nüìä Workflow 4: Data Analysis and ML Pipeline\n');

  // Store dataset metadata
  await client.storeMemory('dataset_metadata', {
    name: 'customer_behavior_analysis',
    size: '10GB',
    records: 1000000,
    features: 50,
    target: 'customer_churn',
    split: { train: 0.7, validation: 0.15, test: 0.15 },
  });

  // Orchestrate data processing pipeline
  const pipeline = await client.orchestrate(
    'Process customer data and train churn prediction model',
    {
      strategy: 'analysis',
      mode: 'hierarchical',
      maxAgents: 6,
      parallel: true,
    },
  );

  console.log(`  ‚úÖ Data pipeline orchestration started: ${pipeline.task_id}`);

  // Create specialized data agents
  const dataTeam = {
    preprocessor: await client.spawnAgent('analyst', 'data-preprocessor', {
      skills: ['pandas', 'numpy', 'data_cleaning'],
      memory_limit: '8GB',
    }),
    feature_engineer: await client.spawnAgent('analyst', 'feature-engineer', {
      skills: ['feature_extraction', 'dimensionality_reduction'],
      techniques: ['PCA', 'LDA', 'autoencoders'],
    }),
    ml_engineer: await client.spawnAgent('analyst', 'ml-engineer', {
      neural_enabled: true,
      frameworks: ['tensorflow', 'pytorch', 'scikit-learn'],
      gpu_enabled: true,
    }),
    evaluator: await client.spawnAgent('tester', 'model-evaluator', {
      metrics: ['accuracy', 'precision', 'recall', 'f1', 'auc'],
      visualization: true,
    }),
  };

  // Create pipeline tasks
  const tasks = [
    {
      agent: dataTeam.preprocessor.agent_id,
      task: 'Clean and preprocess customer dataset',
    },
    {
      agent: dataTeam.feature_engineer.agent_id,
      task: 'Engineer features for churn prediction',
    },
    {
      agent: dataTeam.ml_engineer.agent_id,
      task: 'Train neural network for churn prediction',
    },
    {
      agent: dataTeam.evaluator.agent_id,
      task: 'Evaluate model performance and generate report',
    },
  ];

  for (const t of tasks) {
    await client.createTask('analysis', t.task, 'high', t.agent);
  }

  console.log(`  ‚úÖ Created ${tasks.length} pipeline tasks`);

  // Store model training configuration
  await client.storeMemory('model_config', {
    architecture: {
      type: 'feedforward_neural_network',
      layers: [
        { type: 'input', size: 50 },
        { type: 'dense', size: 128, activation: 'relu' },
        { type: 'dropout', rate: 0.3 },
        { type: 'dense', size: 64, activation: 'relu' },
        { type: 'dropout', rate: 0.2 },
        { type: 'dense', size: 1, activation: 'sigmoid' },
      ],
    },
    training: {
      optimizer: 'adam',
      learning_rate: 0.001,
      batch_size: 32,
      epochs: 100,
      early_stopping: { patience: 10, monitor: 'val_loss' },
    },
  });

  // Simulate training progress
  console.log('\n  üìà Training Progress:');
  const epochs = [1, 10, 25, 50, 75, 100];
  for (const epoch of epochs) {
    await client.storeMemory(`training_metrics_epoch_${epoch}`, {
      epoch,
      train_loss: 0.5 * Math.exp(-epoch / 50),
      val_loss: 0.52 * Math.exp(-epoch / 50),
      train_accuracy: 0.6 + 0.35 * (1 - Math.exp(-epoch / 30)),
      val_accuracy: 0.58 + 0.34 * (1 - Math.exp(-epoch / 30)),
    });
    console.log(
      `    Epoch ${epoch}: Accuracy ${(0.58 + 0.34 * (1 - Math.exp(-epoch / 30))).toFixed(3)}`,
    );
  }

  // Store final model results
  await client.storeMemory('model_results', {
    best_epoch: 87,
    metrics: {
      accuracy: 0.924,
      precision: 0.891,
      recall: 0.876,
      f1_score: 0.883,
      auc_roc: 0.956,
    },
    feature_importance: {
      days_since_last_purchase: 0.23,
      total_purchase_amount: 0.18,
      support_tickets: 0.15,
      login_frequency: 0.12,
      product_views: 0.09,
    },
    recommendations: [
      'Deploy model to production with A/B testing',
      'Monitor for data drift weekly',
      'Retrain model monthly with new data',
      'Consider ensemble with gradient boosting',
    ],
  });

  console.log('\n  ‚úÖ Model training completed successfully');
}

/**
 * Workflow 5: Multi-Agent Swarm Coordination
 * Demonstrates complex swarm behaviors and coordination patterns
 */
async function swarmCoordinationWorkflow(client) {
  console.log('\nüêù Workflow 5: Multi-Agent Swarm Coordination\n');

  // Define swarm mission
  await client.storeMemory('swarm_mission', {
    objective: 'Build comprehensive documentation system',
    components: [
      'API documentation generator',
      'Code example extractor',
      'Tutorial creator',
      'Interactive playground',
      'Search indexer',
    ],
    coordination_rules: {
      communication: 'broadcast',
      consensus: 'majority_vote',
      task_allocation: 'capability_based',
      conflict_resolution: 'priority_based',
    },
  });

  // Create swarm topology
  const swarmConfig = {
    topology: 'mesh', // Full mesh for maximum coordination
    size: 10,
    redundancy: 2, // Each task has 2 agents
    heartbeat_interval: 5000,
  };

  // Initialize swarm with diverse agent types
  const swarm = {
    coordinators: [],
    workers: [],
    reviewers: [],
  };

  // Spawn coordinator agents
  for (let i = 0; i < 2; i++) {
    const coordinator = await client.spawnAgent('analyst', `coordinator-${i}`, {
      role: 'swarm_coordinator',
      decision_making: 'consensus',
      max_subordinates: 5,
    });
    swarm.coordinators.push(coordinator);
  }
  console.log(`  ‚úÖ Spawned ${swarm.coordinators.length} coordinators`);

  // Spawn worker agents
  const workerTypes = ['coder', 'researcher', 'documenter'];
  for (let i = 0; i < 6; i++) {
    const type = workerTypes[i % workerTypes.length];
    const worker = await client.spawnAgent(type, `worker-${type}-${i}`, {
      swarm_role: 'worker',
      collaborative: true,
      skill_sharing: true,
    });
    swarm.workers.push(worker);
  }
  console.log(`  ‚úÖ Spawned ${swarm.workers.length} workers`);

  // Spawn reviewer agents
  for (let i = 0; i < 2; i++) {
    const reviewer = await client.spawnAgent('reviewer', `reviewer-${i}`, {
      standards: ['accuracy', 'completeness', 'clarity'],
      veto_power: true,
    });
    swarm.reviewers.push(reviewer);
  }
  console.log(`  ‚úÖ Spawned ${swarm.reviewers.length} reviewers`);

  // Store swarm structure
  await client.storeMemory('swarm_structure', swarm);

  // Create coordinated tasks
  const coordinatedTasks = [
    {
      id: 'task-cluster-1',
      description: 'Generate API documentation from source code',
      subtasks: [
        'Parse API endpoints',
        'Extract parameter schemas',
        'Generate OpenAPI spec',
        'Create interactive docs',
      ],
      coordinator: swarm.coordinators[0].agent_id,
      workers: swarm.workers.slice(0, 3).map((w) => w.agent_id),
    },
    {
      id: 'task-cluster-2',
      description: 'Create comprehensive tutorials',
      subtasks: [
        'Identify common use cases',
        'Write step-by-step guides',
        'Create code examples',
        'Add interactive demos',
      ],
      coordinator: swarm.coordinators[1].agent_id,
      workers: swarm.workers.slice(3, 6).map((w) => w.agent_id),
    },
  ];

  // Execute coordinated task clusters
  for (const cluster of coordinatedTasks) {
    console.log(`\n  üìã Executing: ${cluster.description}`);

    // Coordinator creates master task
    const masterTask = await client.createTask(
      'coordination',
      cluster.description,
      'critical',
      cluster.coordinator,
    );

    // Workers create subtasks
    for (let i = 0; i < cluster.subtasks.length; i++) {
      const workerId = cluster.workers[i % cluster.workers.length];
      await client.createTask(
        'development',
        cluster.subtasks[i],
        'high',
        workerId,
      );
    }

    console.log(
      `    ‚úÖ Created ${cluster.subtasks.length + 1} coordinated tasks`,
    );
  }

  // Monitor swarm behavior
  console.log('\n  üëÅÔ∏è  Monitoring Swarm Behavior');

  client.on('zen-swarm/event', (event) => {
    if (event.event.type === 'swarm_consensus') {
      console.log(`    ü§ù Consensus reached: ${event.event.data.decision}`);
    } else if (event.event.type === 'task_handoff') {
      console.log(
        `    üîÑ Task handoff: ${event.event.data.from} ‚Üí ${event.event.data.to}`,
      );
    }
  });

  await client.monitor(
    ['swarm_consensus', 'task_handoff', 'conflict_resolution'],
    10,
  );

  // Optimize swarm performance
  const swarmOptimization = await client.optimize(
    'throughput',
    {
      load_balancing: 'dynamic',
      communication_overhead: 'minimize',
      redundancy_level: 'adaptive',
    },
    true,
  );

  console.log('\n  ‚úÖ Swarm optimization completed');
  console.log(
    `    Recommendations applied: ${swarmOptimization.recommendations.length}`,
  );

  // Final swarm status
  const swarmStatus = await client.query({ swarm: true }, true);
  console.log('\n  üìä Final Swarm Status:');
  console.log(`    Active Agents: ${swarmStatus.total_agents}`);
  console.log(`    Tasks Completed: ${swarmStatus.completed_tasks}`);
  console.log(
    `    Coordination Efficiency: ${((swarmStatus.completed_tasks / swarmStatus.total_agents) * 100).toFixed(1)}%`,
  );
}

// Main execution function
async function main() {
  console.log('üöÄ RUV-SWARM MCP Workflow Examples');
  console.log('='.repeat(50));

  const client = new RuvSwarmMCP();

  try {
    // Connect to MCP server
    await client.connect();

    // Run workflows based on command line argument
    const workflow = process.argv[2] || 'all';

    switch (workflow) {
      case '1':
      case 'webapp':
        await webAppDevelopmentWorkflow(client);
        break;

      case '2':
      case 'research':
        await aiResearchSwarmWorkflow(client);
        break;

      case '3':
      case 'cicd':
        await cicdPipelineWorkflow(client);
        break;

      case '4':
      case 'data':
        await dataAnalysisPipeline(client);
        break;

      case '5':
      case 'swarm':
        await swarmCoordinationWorkflow(client);
        break;

      case 'all':
      default:
        await webAppDevelopmentWorkflow(client);
        await aiResearchSwarmWorkflow(client);
        await cicdPipelineWorkflow(client);
        await dataAnalysisPipeline(client);
        await swarmCoordinationWorkflow(client);
        break;
    }

    // Display final statistics
    console.log('\nüìä Workflow Execution Summary');
    console.log('='.repeat(50));

    const finalStatus = await client.query({}, true);
    console.log(`Total Agents Created: ${finalStatus.total_agents}`);
    console.log(
      `Total Tasks Executed: ${finalStatus.active_tasks + finalStatus.completed_tasks}`,
    );
    console.log(`Tasks Completed: ${finalStatus.completed_tasks}`);

    // List all stored memory keys
    const agents = await client.listAgents(true);
    console.log(`\nüìù Stored Memory Keys: ${agents.count} agents tracked`);
  } catch (error) {
    console.error('‚ùå Error:', error);
  } finally {
    await client.disconnect();
  }
}

// Export for use in other modules
module.exports = {
  RuvSwarmMCP,
  webAppDevelopmentWorkflow,
  aiResearchSwarmWorkflow,
  cicdPipelineWorkflow,
  dataAnalysisPipeline,
  swarmCoordinationWorkflow,
};

// Run if called directly
if (require.main === module) {
  console.log('\nUsage: node mcp-workflows.js [workflow]');
  console.log('Workflows:');
  console.log('  1 or webapp   - Web Application Development');
  console.log('  2 or research - AI Research Swarm');
  console.log('  3 or cicd     - CI/CD Pipeline');
  console.log('  4 or data     - Data Analysis Pipeline');
  console.log('  5 or swarm    - Swarm Coordination');
  console.log('  all           - Run all workflows (default)\n');

  main().catch(console.error);
}
