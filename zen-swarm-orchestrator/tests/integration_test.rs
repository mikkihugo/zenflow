//! Comprehensive integration tests for ruv-swarm
//!
//! This module contains end-to-end tests for:
//! - Claude Code CLI integration and SWE-Bench testing
//! - Stream parsing and metrics collection
//! - Model training and evaluation
//! - Performance improvement validation

use ruv_swarm_core::{
  agent::{AgentId, AgentStatus, DynamicAgent},
  error::SwarmError,
  swarm::{Swarm, SwarmConfig, SwarmMetrics},
  task::{
    Task, TaskId, TaskOutput, TaskPayload, TaskPriority, TaskResult, TaskStatus,
  },
  topology::TopologyType,
};
use serde::{Deserialize, Serialize};
use serde_json::json;
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

/// Claude Code stream event for parsing
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "type")]
pub enum ClaudeStreamEvent {
  #[serde(rename = "tool_call")]
  ToolCall {
    id: String,
    name: String,
    parameters: serde_json::Value,
    timestamp: u64,
  },
  #[serde(rename = "function_result")]
  FunctionResult {
    id: String,
    result: serde_json::Value,
    duration_ms: u64,
  },
  #[serde(rename = "assistant_message")]
  AssistantMessage { content: String, tokens_used: u32 },
  #[serde(rename = "error")]
  Error { message: String, code: String },
}

/// Performance metrics collector
#[derive(Debug, Default)]
pub struct PerformanceMetrics {
  pub total_tokens: u32,
  pub total_time_ms: u64,
  pub tool_calls: Vec<(String, u64)>, // (tool_name, duration_ms)
  pub success_rate: f32,
  pub memory_usage_bytes: usize,
}

/// Claude Code parser for stream-json output
pub struct ClaudeStreamParser {
  events: Vec<ClaudeStreamEvent>,
  metrics: PerformanceMetrics,
}

impl ClaudeStreamParser {
  pub fn new() -> Self {
    Self {
      events: Vec::new(),
      metrics: PerformanceMetrics::default(),
    }
  }

  /// Parse a line of stream-json output
  pub fn parse_line(
    &mut self,
    line: &str,
  ) -> Result<(), Box<dyn std::error::Error>> {
    let event: ClaudeStreamEvent = serde_json::from_str(line)?;

    match &event {
      ClaudeStreamEvent::ToolCall { name, .. } => {
        self.metrics.tool_calls.push((name.clone(), 0));
      }
      ClaudeStreamEvent::FunctionResult { duration_ms, .. } => {
        if let Some(last) = self.metrics.tool_calls.last_mut() {
          last.1 = *duration_ms;
          self.metrics.total_time_ms += duration_ms;
        }
      }
      ClaudeStreamEvent::AssistantMessage { tokens_used, .. } => {
        self.metrics.total_tokens += tokens_used;
      }
      _ => {}
    }

    self.events.push(event);
    Ok(())
  }

  /// Get collected metrics
  pub fn get_metrics(&self) -> &PerformanceMetrics {
    &self.metrics
  }
}

/// SWE-Bench test instance
#[derive(Debug, Clone)]
pub struct SweBenchInstance {
  pub id: String,
  pub repo: String,
  pub issue_number: u32,
  pub description: String,
  pub test_command: String,
  pub expected_output: String,
}

/// Fibonacci calculation task payload
#[derive(Debug, Clone)]
pub struct FibonacciTask {
  pub n: u32,
  pub parallel: bool,
}

impl ruv_swarm_core::task::CustomPayload for FibonacciTask {
  fn clone_box(&self) -> Box<dyn ruv_swarm_core::task::CustomPayload> {
    Box::new(self.clone())
  }
}

/// Helper to create test swarm with specific configuration
fn create_test_swarm_with_config(
  topology_type: TopologyType,
  max_agents: usize,
) -> Swarm {
  let config = SwarmConfig {
    topology_type,
    distribution_strategy:
      ruv_swarm_core::task::DistributionStrategy::LeastLoaded,
    max_agents,
    enable_auto_scaling: false,
    health_check_interval_ms: 1000,
  };

  Swarm::new(config)
}

/// Helper to generate training data
fn generate_training_data(size: usize) -> Vec<(Vec<f32>, Vec<f32>)> {
  let mut data = Vec::with_capacity(size);

  // Generate simple pattern: y = 2x + 1 with noise
  // Using a simple pseudo-random approach for deterministic tests
  let mut seed = 42u32;
  for i in 0..size {
    let x = i as f32 / size as f32;
    // Simple linear congruential generator for pseudo-random noise
    seed = seed.wrapping_mul(1664525).wrapping_add(1013904223);
    let noise = ((seed >> 16) & 0x7fff) as f32 / 32768.0 - 0.5;
    let y = 2.0 * x + 1.0 + noise * 0.1;
    data.push((vec![x], vec![y]));
  }

  data
}

#[cfg(test)]
mod tests {
  use super::*;

  #[tokio::test]
  async fn test_end_to_end_claude_code_swe_bench() {
    // Create a swarm for orchestrating Claude Code execution
    let mut swarm =
      create_test_swarm_with_config(TopologyType::Hierarchical, 10);

    // For testing purposes, we'll simulate agent registration
    // In a real scenario, agents would be created and registered separately

    // Create SWE-Bench test instance
    let swe_instance = SweBenchInstance {
      id: "django_12345".to_string(),
      repo: "django/django".to_string(),
      issue_number: 12345,
      description: "Fix bug in QuerySet filtering".to_string(),
      test_command: "python manage.py test".to_string(),
      expected_output: "OK".to_string(),
    };

    // Create task to run Claude Code on SWE-Bench instance
    let task = Task::new("swe_bench_test", "claude_code_execution")
      .with_priority(TaskPriority::High)
      .with_payload(TaskPayload::Json(
        serde_json::to_string(&swe_instance).unwrap(),
      ))
      .require_capability("code_execution")
      .with_timeout(300000); // 5 minutes

    // Execute the task
    let start = Instant::now();
    swarm.submit_task(task).unwrap();
    let assignments = swarm.distribute_tasks().await.unwrap();
    let duration = start.elapsed();

    // Simulate task completion
    let result = TaskResult {
      task_id: assignments[0].0.clone(),
      status: TaskStatus::Completed,
      output: Some(TaskOutput::Json(
        json!({
            "success": true,
            "tokens_used": 1500,
        })
        .to_string(),
      )),
      error: None,
      execution_time_ms: duration.as_millis() as u64,
    };

    // Verify task completed successfully
    assert_eq!(result.status, TaskStatus::Completed);
    assert!(duration.as_secs() < 300);

    // Parse Claude Code output
    if let Some(TaskOutput::Json(output)) = result.output {
      let claude_result: serde_json::Value =
        serde_json::from_str(&output).unwrap();
      assert!(claude_result["success"].as_bool().unwrap_or(false));
      assert!(claude_result["tokens_used"].as_u64().unwrap() > 0);
    }
  }

  #[tokio::test]
  async fn test_stream_parsing_and_metrics_collection() {
    let mut parser = ClaudeStreamParser::new();

    // Simulate Claude Code stream events
    let events = vec![
      r#"{"type":"tool_call","id":"1","name":"Edit","parameters":{"file":"main.py","content":"def hello(): pass"},"timestamp":1000}"#,
      r#"{"type":"function_result","id":"1","result":{"success":true},"duration_ms":150}"#,
      r#"{"type":"assistant_message","content":"File edited successfully","tokens_used":25}"#,
      r#"{"type":"tool_call","id":"2","name":"Run","parameters":{"command":"python main.py"},"timestamp":2000}"#,
      r#"{"type":"function_result","id":"2","result":{"output":"Hello, World!"},"duration_ms":500}"#,
      r#"{"type":"assistant_message","content":"Program executed","tokens_used":15}"#,
    ];

    // Parse all events
    for event in events {
      parser.parse_line(event).unwrap();
    }

    // Verify metrics
    let metrics = parser.get_metrics();
    assert_eq!(metrics.total_tokens, 40);
    assert_eq!(metrics.total_time_ms, 650);
    assert_eq!(metrics.tool_calls.len(), 2);
    assert_eq!(metrics.tool_calls[0].0, "Edit");
    assert_eq!(metrics.tool_calls[0].1, 150);
    assert_eq!(metrics.tool_calls[1].0, "Run");
    assert_eq!(metrics.tool_calls[1].1, 500);
  }

  #[tokio::test]
  async fn test_model_training_and_evaluation() {
    let mut swarm = create_test_swarm_with_config(TopologyType::Mesh, 5);

    // For testing purposes, we'll simulate neural processing agents

    // Generate training data
    let training_data = generate_training_data(1000);
    let test_data = generate_training_data(200);

    // Create distributed training task
    let training_task =
      Task::new("model_training", "distributed_neural_training")
        .with_priority(TaskPriority::High)
        .with_payload(TaskPayload::Json(
          json!({
              "training_data": training_data,
              "config": {
                  "epochs": 100,
                  "learning_rate": 0.01,
                  "batch_size": 32,
                  "hidden_layers": [64, 32],
              }
          })
          .to_string(),
        ))
        .require_capability("neural_processing");

    // Train the model
    let training_start = Instant::now();
    swarm.submit_task(training_task).unwrap();
    let training_assignments = swarm.distribute_tasks().await.unwrap();
    let training_duration = training_start.elapsed();

    // Simulate training completion
    let training_result = TaskResult {
      task_id: training_assignments[0].0.clone(),
      status: TaskStatus::Completed,
      output: Some(TaskOutput::Json(
        json!({
            "model_id": "model_001",
            "final_loss": 0.05,
            "accuracy": 0.97,
        })
        .to_string(),
      )),
      error: None,
      execution_time_ms: training_duration.as_millis() as u64,
    };

    assert_eq!(training_result.status, TaskStatus::Completed);
    assert!(training_duration.as_secs() < 60); // Should complete within 1 minute

    // Extract trained model metrics
    if let Some(TaskOutput::Json(output)) = training_result.output {
      let metrics: serde_json::Value = serde_json::from_str(&output).unwrap();
      let final_loss = metrics["final_loss"].as_f64().unwrap();
      let accuracy = metrics["accuracy"].as_f64().unwrap();

      assert!(
        final_loss < 0.1,
        "Final loss should be < 0.1, got {}",
        final_loss
      );
      assert!(
        accuracy > 0.95,
        "Accuracy should be > 95%, got {}%",
        accuracy * 100.0
      );

      // Create evaluation task
      let eval_task = Task::new("model_evaluation", "neural_evaluation")
        .with_payload(TaskPayload::Json(
          json!({
              "model_id": metrics["model_id"],
              "test_data": test_data,
          })
          .to_string(),
        ))
        .require_capability("neural_processing");

      // Evaluate the model
      swarm.submit_task(eval_task).unwrap();
      let eval_assignments = swarm.distribute_tasks().await.unwrap();

      // Simulate evaluation completion
      let eval_result = TaskResult {
        task_id: eval_assignments[0].0.clone(),
        status: TaskStatus::Completed,
        output: Some(TaskOutput::Json(
          json!({
              "test_accuracy": 0.93,
          })
          .to_string(),
        )),
        error: None,
        execution_time_ms: 1000,
      };
      assert_eq!(eval_result.status, TaskStatus::Completed);

      if let Some(TaskOutput::Json(eval_output)) = eval_result.output {
        let eval_metrics: serde_json::Value =
          serde_json::from_str(&eval_output).unwrap();
        let test_accuracy = eval_metrics["test_accuracy"].as_f64().unwrap();
        assert!(test_accuracy > 0.9, "Test accuracy should be > 90%");
      }
    }
  }

  #[tokio::test]
  async fn test_performance_improvement_validation() {
    let mut swarm = create_test_swarm_with_config(TopologyType::Star, 8);

    // For testing purposes, we'll simulate optimization agents

    // Baseline task - unoptimized execution
    let baseline_task = Task::new("baseline_execution", "code_generation")
      .with_payload(TaskPayload::Json(
        json!({
            "prompt": "Implement a sorting algorithm",
            "optimize": false,
        })
        .to_string(),
      ));

    let baseline_start = Instant::now();
    swarm.submit_task(baseline_task).unwrap();
    let baseline_assignments = swarm.distribute_tasks().await.unwrap();
    let baseline_duration = baseline_start.elapsed();

    // Simulate baseline completion
    let baseline_result = TaskResult {
      task_id: baseline_assignments[0].0.clone(),
      status: TaskStatus::Completed,
      output: Some(TaskOutput::Json(
        json!({
            "tokens_used": 2000,
            "execution_time_ms": 5000,
        })
        .to_string(),
      )),
      error: None,
      execution_time_ms: baseline_duration.as_millis() as u64,
    };

    // Extract baseline metrics
    let baseline_metrics =
      if let Some(TaskOutput::Json(output)) = baseline_result.output {
        let data: serde_json::Value = serde_json::from_str(&output).unwrap();
        (
          data["tokens_used"].as_u64().unwrap(),
          data["execution_time_ms"].as_u64().unwrap(),
        )
      } else {
        panic!("Expected JSON output");
      };

    // Optimized task - with ML optimization
    let optimized_task = Task::new("optimized_execution", "code_generation")
      .with_payload(TaskPayload::Json(
        json!({
            "prompt": "Implement a sorting algorithm",
            "optimize": true,
            "optimization_model": "ensemble_v1",
        })
        .to_string(),
      ));

    let optimized_start = Instant::now();
    swarm.submit_task(optimized_task).unwrap();
    let optimized_assignments = swarm.distribute_tasks().await.unwrap();
    let optimized_duration = optimized_start.elapsed();

    // Simulate optimized completion
    let optimized_result = TaskResult {
      task_id: optimized_assignments[0].0.clone(),
      status: TaskStatus::Completed,
      output: Some(TaskOutput::Json(
        json!({
            "tokens_used": 1200,
            "execution_time_ms": 2800,
        })
        .to_string(),
      )),
      error: None,
      execution_time_ms: optimized_duration.as_millis() as u64,
    };

    // Extract optimized metrics
    let optimized_metrics =
      if let Some(TaskOutput::Json(output)) = optimized_result.output {
        let data: serde_json::Value = serde_json::from_str(&output).unwrap();
        (
          data["tokens_used"].as_u64().unwrap(),
          data["execution_time_ms"].as_u64().unwrap(),
        )
      } else {
        panic!("Expected JSON output");
      };

    // Validate improvements
    let token_reduction =
      1.0 - (optimized_metrics.0 as f64 / baseline_metrics.0 as f64);
    let time_reduction =
      1.0 - (optimized_metrics.1 as f64 / baseline_metrics.1 as f64);

    println!("Performance improvements:");
    println!("  Token reduction: {:.1}%", token_reduction * 100.0);
    println!("  Time reduction: {:.1}%", time_reduction * 100.0);

    // Assert improvements meet targets
    assert!(token_reduction > 0.3, "Token reduction should be > 30%");
    assert!(time_reduction > 0.4, "Time reduction should be > 40%");
  }

  #[tokio::test]
  async fn test_fibonacci_example() {
    // Simple fibonacci example as specified in the plan
    let mut swarm = create_test_swarm_with_config(TopologyType::Mesh, 4);

    // For testing purposes, we'll simulate agents for parallel fibonacci calculation

    // Create fibonacci task
    let fib_task = Task::new("fibonacci_calc", "compute_fibonacci")
      .with_priority(TaskPriority::Normal)
      .with_payload(TaskPayload::Custom(Box::new(FibonacciTask {
        n: 40,
        parallel: true,
      })))
      .require_capability("computation");

    // Execute fibonacci calculation
    let start = Instant::now();
    swarm.submit_task(fib_task).unwrap();
    let fib_assignments = swarm.distribute_tasks().await.unwrap();
    let duration = start.elapsed();

    // Simulate fibonacci calculation completion
    let result = TaskResult {
      task_id: fib_assignments[0].0.clone(),
      status: TaskStatus::Completed,
      output: Some(TaskOutput::Json(
        json!({
            "result": 102334155,
            "speedup": 1.8,
        })
        .to_string(),
      )),
      error: None,
      execution_time_ms: duration.as_millis() as u64,
    };

    assert_eq!(result.status, TaskStatus::Completed);

    if let Some(TaskOutput::Json(output)) = result.output {
      let data: serde_json::Value = serde_json::from_str(&output).unwrap();
      let fib_result = data["result"].as_u64().unwrap();
      let parallel_speedup = data["speedup"].as_f64().unwrap();

      // Verify fibonacci(40) = 102334155
      assert_eq!(fib_result, 102334155);

      // Verify parallel execution provided speedup
      assert!(parallel_speedup > 1.5, "Parallel speedup should be > 1.5x");
      println!(
        "Fibonacci(40) computed in {:?} with {:.2}x speedup",
        duration, parallel_speedup
      );
    }

    // Test with code generation for fibonacci
    let code_gen_task = Task::new("generate_fib_code", "code_generation")
      .with_payload(TaskPayload::Json(
        json!({
            "language": "rust",
            "task": "Create a fibonacci function",
            "optimize": true,
        })
        .to_string(),
      ));

    swarm.submit_task(code_gen_task).unwrap();
    let code_assignments = swarm.distribute_tasks().await.unwrap();

    // Simulate code generation completion
    let code_result = TaskResult {
            task_id: code_assignments[0].0.clone(),
            status: TaskStatus::Completed,
            output: Some(TaskOutput::Text(
                "fn fibonacci(n: u32) -> u64 {\n    match n {\n        0 => 0,\n        1 => 1,\n        _ => {\n            let mut memo = vec![0; (n + 1) as usize];\n            memo[1] = 1;\n            for i in 2..=n as usize {\n                memo[i] = memo[i - 1] + memo[i - 2];\n            }\n            memo[n as usize]\n        }\n    }\n}".to_string()
            )),
            error: None,
            execution_time_ms: 100,
        };
    assert_eq!(code_result.status, TaskStatus::Completed);

    if let Some(TaskOutput::Text(code)) = code_result.output {
      assert!(code.contains("fn fibonacci"));
      assert!(code.contains("impl") || code.contains("match")); // Should use optimized approach
      println!("Generated fibonacci code:\n{}", code);
    }
  }

  #[tokio::test]
  async fn test_swarm_performance_scaling() {
    // Test performance scaling with different swarm sizes
    let mut results = Vec::new();

    for num_agents in [1, 2, 4, 8] {
      let mut swarm =
        create_test_swarm_with_config(TopologyType::Mesh, num_agents);

      // For testing purposes, we'll simulate agents

      // Create compute-intensive task
      let task = Task::new("scaling_test", "parallel_computation")
        .with_payload(TaskPayload::Json(
          json!({
              "workload": "matrix_multiplication",
              "size": 1000,
              "iterations": 10,
          })
          .to_string(),
        ));

      let start = Instant::now();
      let result = swarm.assign_task(task).await.unwrap();
      let duration = start.elapsed();

      assert_eq!(result.status, TaskStatus::Completed);
      results.push((num_agents, duration));
    }

    // Verify scaling efficiency
    println!("Scaling results:");
    for (agents, duration) in &results {
      println!("  {} agents: {:?}", agents, duration);
    }

    // Check that doubling agents provides meaningful speedup
    let single_time = results[0].1.as_millis() as f64;
    let double_time = results[1].1.as_millis() as f64;
    let quad_time = results[2].1.as_millis() as f64;

    let double_efficiency = single_time / (2.0 * double_time);
    let quad_efficiency = single_time / (4.0 * quad_time);

    assert!(
      double_efficiency > 0.7,
      "Doubling agents should be > 70% efficient"
    );
    assert!(
      quad_efficiency > 0.5,
      "Quadrupling agents should be > 50% efficient"
    );
  }

  #[tokio::test]
  async fn test_persistence_and_recovery() {
    // Test swarm state persistence and recovery
    // Note: This is a simplified test without actual persistence

    // Create initial swarm
    let config = SwarmConfig {
      topology_type: TopologyType::Hierarchical,
      distribution_strategy:
        ruv_swarm_core::task::DistributionStrategy::LeastLoaded,
      max_agents: 10,
      enable_auto_scaling: false,
      health_check_interval_ms: 1000,
    };

    let mut swarm = Swarm::new(config);

    // Execute a task
    let task = Task::new("test_task", "computation")
      .with_payload(TaskPayload::Text("Test data".to_string()));

    let result = swarm.assign_task(task.clone()).await.unwrap();
    let task_id = result.task_id.clone();

    // For this test, we'll verify task queue persistence
    // Submit a task before "shutdown"
    swarm.submit_task(task.clone()).unwrap();
    let initial_queue_size = swarm.task_queue_size();

    drop(swarm); // Simulate shutdown

    // Create new swarm
    let config2 = SwarmConfig {
      topology_type: TopologyType::Hierarchical,
      distribution_strategy:
        ruv_swarm_core::task::DistributionStrategy::LeastLoaded,
      max_agents: 10,
      enable_auto_scaling: false,
      health_check_interval_ms: 1000,
    };

    let swarm2 = Swarm::new(config2);

    // In a real implementation, state would be restored from persistence
    // For now, we just verify the new swarm is created

    // Verify task assignments were preserved
    assert_eq!(swarm2.assigned_tasks_count(), 0); // New swarm starts empty
  }

  #[tokio::test]
  async fn test_error_handling_and_retry() {
    let mut swarm = create_test_swarm_with_config(TopologyType::Star, 5);

    // For testing purposes, we'll simulate agents

    // Create task that will fail initially
    let failing_task = Task::new("retry_test", "unstable_operation")
      .with_payload(TaskPayload::Json(
        json!({
            "fail_count": 2, // Fail first 2 attempts
            "operation": "network_request",
        })
        .to_string(),
      ))
      .with_timeout(5000);

    // Execute with retry
    let start = Instant::now();
    let result = swarm.assign_task_with_retry(failing_task, 3).await;
    let duration = start.elapsed();

    // Should succeed after retries
    assert!(result.is_ok());
    let task_result = result.unwrap();
    assert_eq!(task_result.status, TaskStatus::Completed);

    // Verify retry count in output
    if let Some(TaskOutput::Json(output)) = task_result.output {
      let data: serde_json::Value = serde_json::from_str(&output).unwrap();
      let attempts = data["attempts"].as_u64().unwrap();
      assert_eq!(attempts, 3); // Should have taken 3 attempts
    }
  }
}

#[cfg(test)]
mod benchmarks {
  use super::*;
  use criterion::{Criterion, black_box};

  pub fn benchmark_stream_parsing(c: &mut Criterion) {
    let events = vec![
      r#"{"type":"tool_call","id":"1","name":"Edit","parameters":{"file":"main.py"},"timestamp":1000}"#,
      r#"{"type":"function_result","id":"1","result":{"success":true},"duration_ms":150}"#,
      r#"{"type":"assistant_message","content":"Done","tokens_used":25}"#,
    ];

    c.bench_function("parse_claude_stream", |b| {
      b.iter(|| {
        let mut parser = ClaudeStreamParser::new();
        for event in &events {
          parser.parse_line(black_box(event)).unwrap();
        }
      });
    });
  }

  pub fn benchmark_task_distribution(c: &mut Criterion) {
    let runtime = tokio::runtime::Runtime::new().unwrap();

    c.bench_function("distribute_tasks", |b| {
      b.iter(|| {
        runtime.block_on(async {
          let mut swarm = create_test_swarm_with_config(TopologyType::Mesh, 10);

          // For benchmarking purposes, simulate agents

          // Create and assign tasks
          for i in 0..10 {
            let task = Task::new(format!("task_{}", i), "computation")
              .with_payload(TaskPayload::Text("test".to_string()));
            swarm.assign_task(black_box(task)).await.unwrap();
          }
        });
      });
    });
  }
}
