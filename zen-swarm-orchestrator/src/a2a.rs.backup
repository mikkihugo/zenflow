//! Agent-to-Agent (A2A) Communication Protocol
//!
//! The A2A protocol enables zen-swarm repository daemons to communicate
//! with zen-orchestrator (running inside THE COLLECTIVE). This is the
//! gateway protocol that connects distributed repository daemons to
//! THE COLLECTIVE's centralized intelligence services.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::{mpsc, RwLock};
use uuid::Uuid;
use chrono::{DateTime, Utc};

/// A2A messages sent from zen-swarm daemons to zen-orchestrator
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum A2AMessage {
    /// Repository daemon registration
    SwarmRegistration {
        swarm_id: String,
        repository_path: String,
        capabilities: Vec<String>,
        daemon_port: u16,
    },
    
    /// LLM intelligence request from repository daemon
    IntelligenceRequest {
        request_id: String,
        swarm_id: String,
        task_description: String,
        preferred_llm: String,
        context: RepositoryContext,
        priority: Priority,
    },
    
    /// Repository intelligence sharing
    RepositoryIntelligence {
        swarm_id: String,
        patterns: Vec<CodePattern>,
        optimizations: Vec<BuildOptimization>,
        domain_knowledge: HashMap<String, String>,
    },
    
    /// Task coordination request
    TaskCoordination {
        task_id: String,
        requesting_swarm: String,
        task_type: TaskType,
        requirements: TaskRequirements,
    },
    
    /// Heartbeat from repository daemon
    SwarmHeartbeat {
        swarm_id: String,
        timestamp: DateTime<Utc>,
        status: SwarmStatus,
        metrics: SwarmMetrics,
    },
    
    /// Service discovery - what capabilities are available
    CapabilityDiscovery {
        request_id: String,
        requesting_swarm: String,
        requested_capabilities: Vec<String>,
    },
}

/// Repository context provided with requests
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RepositoryContext {
    pub working_directory: String,
    pub relevant_files: Vec<String>,
    pub build_system: Option<String>,
    pub test_framework: Option<String>,
    pub domain_area: Option<String>,
}

/// Priority levels for requests
#[derive(Debug, Clone, Serialize, Deserialize, PartialOrd, Ord, PartialEq, Eq)]
pub enum Priority {
    Low,
    Normal,
    High,
    Critical,
}

/// Code patterns discovered by repository daemons
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CodePattern {
    pub pattern_id: String,
    pub pattern_type: String,
    pub description: String,
    pub confidence: f32,
    pub usage_frequency: u32,
}

/// Build optimizations found by repository daemons
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BuildOptimization {
    pub optimization_id: String,
    pub optimization_type: String,
    pub description: String,
    pub performance_gain: f32,
    pub success_rate: f32,
}

/// Task types that can be coordinated
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum TaskType {
    LLMInference,
    NeuralTraining,
    CodeGeneration,
    RepositoryAnalysis,
    CrossRepoPatternSharing,
}

/// Task requirements specification
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskRequirements {
    pub compute_resources: Option<ComputeRequirements>,
    pub llm_models: Vec<String>,
    pub neural_capabilities: Vec<String>,
    pub estimated_duration_ms: Option<u64>,
}

/// Compute resource requirements
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComputeRequirements {
    pub cpu_cores: Option<u32>,
    pub memory_gb: Option<f32>,
    pub gpu_memory_gb: Option<f32>,
    pub storage_gb: Option<f32>,
}

/// Status of a repository daemon
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SwarmStatus {
    Healthy,
    Degraded { issues: Vec<String> },
    Unhealthy { errors: Vec<String> },
    Busy { active_tasks: u32 },
}

/// Performance metrics from repository daemons
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SwarmMetrics {
    pub tasks_processed: u64,
    pub average_response_time_ms: f64,
    pub error_rate: f32,
    pub cpu_usage: f32,
    pub memory_usage: f32,
    pub disk_usage: f32,
}

/// A2A responses sent from zen-orchestrator back to zen-swarm daemons
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum A2AResponse {
    /// Registration acknowledgment
    RegistrationAck {
        swarm_id: String,
        assigned_capabilities: Vec<String>,
        collective_endpoints: HashMap<String, String>,
    },
    
    /// Intelligence response with LLM result
    IntelligenceResponse {
        request_id: String,
        result: IntelligenceResult,
        model_used: String,
        provider: String,
        processing_time_ms: u64,
    },
    
    /// Available capabilities announcement
    CapabilitiesAnnouncement {
        request_id: String,
        available_llms: Vec<AvailableLLM>,
        neural_services: Vec<NeuralService>,
        compute_resources: ComputeAvailability,
    },
    
    /// Cross-repository patterns to apply
    PatternSharing {
        patterns: Vec<SharedPattern>,
        from_repositories: Vec<String>,
        confidence_threshold: f32,
    },
    
    /// Task coordination response
    TaskCoordinationResponse {
        task_id: String,
        assigned_resources: Vec<String>,
        estimated_completion: DateTime<Utc>,
        dependencies: Vec<String>,
    },
    
    /// Error response
    Error {
        request_id: Option<String>,
        error_code: String,
        message: String,
        retry_after_ms: Option<u64>,
    },
}

/// Intelligence processing result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IntelligenceResult {
    pub content: String,
    pub reasoning: Option<String>,
    pub confidence: f32,
    pub sources: Vec<String>,
    pub token_usage: TokenUsage,
}

/// Token usage tracking
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TokenUsage {
    pub prompt_tokens: u32,
    pub completion_tokens: u32,
    pub total_tokens: u32,
}

/// Available LLM information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AvailableLLM {
    pub model_id: String,
    pub provider: String,
    pub capabilities: Vec<String>,
    pub cost_per_token: Option<f64>,
    pub rate_limit: Option<u32>,
    pub current_load: f32,
}

/// Neural service information
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NeuralService {
    pub service_id: String,
    pub service_type: String,
    pub capabilities: Vec<String>,
    pub availability: f32,
}

/// Compute resource availability
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComputeAvailability {
    pub cpu_cores_available: u32,
    pub memory_gb_available: f32,
    pub gpu_memory_gb_available: f32,
    pub storage_gb_available: f32,
}

/// Patterns shared from other repositories
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SharedPattern {
    pub pattern_id: String,
    pub pattern_type: String,
    pub description: String,
    pub source_repository: String,
    pub success_rate: f32,
    pub adaptation_notes: String,
}

/// A2A server that runs inside zen-orchestrator (THE COLLECTIVE)
#[derive(Debug)]
pub struct A2AServer {
    /// Server configuration
    config: A2AServerConfig,
    
    /// Registered repository daemons
    registered_swarms: Arc<RwLock<HashMap<String, RegisteredSwarm>>>,
    
    /// THE COLLECTIVE service endpoints
    collective_services: Arc<CollectiveServices>,
    
    /// Message processing channels
    message_sender: mpsc::UnboundedSender<A2AMessage>,
    response_channels: Arc<RwLock<HashMap<String, mpsc::UnboundedSender<A2AResponse>>>>,
}

/// Configuration for A2A server
#[derive(Debug, Clone)]
pub struct A2AServerConfig {
    pub server_id: String,
    pub listen_port: u16,
    pub max_connections: u32,
    pub heartbeat_timeout_sec: u64,
    pub message_timeout_ms: u64,
}

/// Information about registered repository daemon
#[derive(Debug, Clone)]
pub struct RegisteredSwarm {
    pub swarm_id: String,
    pub repository_path: String,
    pub capabilities: Vec<String>,
    pub daemon_port: u16,
    pub last_heartbeat: DateTime<Utc>,
    pub status: SwarmStatus,
    pub metrics: SwarmMetrics,
}

/// THE COLLECTIVE services interface
#[derive(Debug)]
pub struct CollectiveServices {
    /// LLM routing and management
    llm_coordinator: Arc<dyn LLMCoordinator>,
    
    /// Neural network services (zen-neural, zen-forecasting, zen-compute)
    neural_services: Arc<dyn NeuralServices>,
    
    /// Cross-repository intelligence
    intelligence_hub: Arc<dyn IntelligenceHub>,
}

/// LLM coordination interface
pub trait LLMCoordinator: Send + Sync {
    async fn route_llm_request(&self, request: &IntelligenceRequest) -> Result<IntelligenceResult, A2AError>;
    async fn get_available_models(&self) -> Result<Vec<AvailableLLM>, A2AError>;
}

/// Neural services interface
pub trait NeuralServices: Send + Sync {
    async fn execute_neural_task(&self, task: &TaskCoordination) -> Result<TaskResult, A2AError>;
    async fn get_neural_capabilities(&self) -> Result<Vec<NeuralService>, A2AError>;
}

/// Intelligence hub interface
pub trait IntelligenceHub: Send + Sync {
    async fn share_repository_intelligence(&self, intelligence: &RepositoryIntelligence) -> Result<(), A2AError>;
    async fn get_cross_repo_patterns(&self, domain: &str) -> Result<Vec<SharedPattern>, A2AError>;
}

/// Task execution result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TaskResult {
    pub task_id: String,
    pub success: bool,
    pub result_data: serde_json::Value,
    pub processing_time_ms: u64,
}

/// A2A protocol errors
#[derive(Debug, thiserror::Error)]
pub enum A2AError {
    #[error("Swarm registration failed: {0}")]
    RegistrationFailed(String),
    
    #[error("LLM request failed: {0}")]
    LLMRequestFailed(String),
    
    #[error("Neural service unavailable: {0}")]
    NeuralServiceUnavailable(String),
    
    #[error("Invalid message format: {0}")]
    InvalidMessageFormat(String),
    
    #[error("Timeout occurred: {0}")]
    Timeout(String),
    
    #[error("Internal server error: {0}")]
    InternalError(String),
}

pub type A2AResult<T> = Result<T, A2AError>;

impl A2AServer {
    /// Create new A2A server
    pub fn new(config: A2AServerConfig, collective_services: CollectiveServices) -> Self {
        let (message_sender, _message_receiver) = mpsc::unbounded_channel();
        
        Self {
            config,
            registered_swarms: Arc::new(RwLock::new(HashMap::new())),
            collective_services: Arc::new(collective_services),
            message_sender,
            response_channels: Arc::new(RwLock::new(HashMap::new())),
        }
    }
    
    /// Start the A2A server
    pub async fn start(&self) -> A2AResult<()> {
        tracing::info!("ðŸŒ Starting A2A server on port {}", self.config.listen_port);
        
        // Start message processing loop
        self.start_message_processor().await;
        
        // Start heartbeat monitoring
        self.start_heartbeat_monitor().await;
        
        tracing::info!("âœ… A2A server started successfully");
        Ok(())
    }
    
    /// Handle incoming A2A message from repository daemon
    pub async fn handle_message(&self, message: A2AMessage) -> A2AResult<A2AResponse> {
        tracing::debug!("ðŸ“¥ Received A2A message: {:?}", message);
        
        match message {
            A2AMessage::SwarmRegistration { swarm_id, repository_path, capabilities, daemon_port } => {
                self.handle_swarm_registration(swarm_id, repository_path, capabilities, daemon_port).await
            }
            
            A2AMessage::IntelligenceRequest { request_id, swarm_id: _, task_description: _, preferred_llm: _, context: _, priority: _ } => {
                // Route to LLM coordinator
                let result = self.collective_services.llm_coordinator
                    .route_llm_request(&message.clone().into())
                    .await?;
                
                Ok(A2AResponse::IntelligenceResponse {
                    request_id,
                    result,
                    model_used: "routed-model".to_string(),
                    provider: "collective".to_string(),
                    processing_time_ms: 100,
                })
            }
            
            A2AMessage::CapabilityDiscovery { request_id, requesting_swarm: _, requested_capabilities: _ } => {
                // Get available capabilities from THE COLLECTIVE
                let available_llms = self.collective_services.llm_coordinator.get_available_models().await?;
                let neural_services = self.collective_services.neural_services.get_neural_capabilities().await?;
                
                Ok(A2AResponse::CapabilitiesAnnouncement {
                    request_id,
                    available_llms,
                    neural_services,
                    compute_resources: ComputeAvailability {
                        cpu_cores_available: 16,
                        memory_gb_available: 64.0,
                        gpu_memory_gb_available: 24.0,
                        storage_gb_available: 1000.0,
                    },
                })
            }
            
            A2AMessage::SwarmHeartbeat { swarm_id, timestamp: _, status, metrics } => {
                // Update registered swarm status
                self.update_swarm_status(&swarm_id, status, metrics).await?;
                
                Ok(A2AResponse::RegistrationAck {
                    swarm_id,
                    assigned_capabilities: vec!["llm_routing".to_string()],
                    collective_endpoints: HashMap::new(),
                })
            }
            
            _ => {
                Ok(A2AResponse::Error {
                    request_id: None,
                    error_code: "UNSUPPORTED_MESSAGE".to_string(),
                    message: "Message type not supported".to_string(),
                    retry_after_ms: None,
                })
            }
        }
    }
    
    /// Handle swarm registration
    async fn handle_swarm_registration(
        &self,
        swarm_id: String,
        repository_path: String,
        capabilities: Vec<String>,
        daemon_port: u16,
    ) -> A2AResult<A2AResponse> {
        tracing::info!("ðŸ“ Registering swarm: {} at {}", swarm_id, repository_path);
        
        let registered_swarm = RegisteredSwarm {
            swarm_id: swarm_id.clone(),
            repository_path: repository_path.clone(),
            capabilities: capabilities.clone(),
            daemon_port,
            last_heartbeat: Utc::now(),
            status: SwarmStatus::Healthy,
            metrics: SwarmMetrics {
                tasks_processed: 0,
                average_response_time_ms: 0.0,
                error_rate: 0.0,
                cpu_usage: 0.0,
                memory_usage: 0.0,
                disk_usage: 0.0,
            },
        };
        
        self.registered_swarms.write().await.insert(swarm_id.clone(), registered_swarm);
        
        Ok(A2AResponse::RegistrationAck {
            swarm_id,
            assigned_capabilities: capabilities,
            collective_endpoints: self.get_collective_endpoints().await,
        })
    }
    
    /// Update swarm status from heartbeat
    async fn update_swarm_status(&self, swarm_id: &str, status: SwarmStatus, metrics: SwarmMetrics) -> A2AResult<()> {
        if let Some(swarm) = self.registered_swarms.write().await.get_mut(swarm_id) {
            swarm.last_heartbeat = Utc::now();
            swarm.status = status;
            swarm.metrics = metrics;
        }
        Ok(())
    }
    
    /// Get THE COLLECTIVE service endpoints
    async fn get_collective_endpoints(&self) -> HashMap<String, String> {
        let mut endpoints = HashMap::new();
        endpoints.insert("llm_coordinator".to_string(), "http://localhost:8080/llm".to_string());
        endpoints.insert("neural_services".to_string(), "http://localhost:8080/neural".to_string());
        endpoints.insert("intelligence_hub".to_string(), "http://localhost:8080/intelligence".to_string());
        endpoints
    }
    
    /// Start message processing background task
    async fn start_message_processor(&self) {
        tracing::debug!("ðŸ”„ Starting A2A message processor");
        // Message processing implementation would go here
    }
    
    /// Start heartbeat monitoring background task
    async fn start_heartbeat_monitor(&self) {
        let registered_swarms = self.registered_swarms.clone();
        let timeout_sec = self.config.heartbeat_timeout_sec;
        
        tokio::spawn(async move {
            let mut interval = tokio::time::interval(std::time::Duration::from_secs(30));
            
            loop {
                interval.tick().await;
                
                let mut swarms = registered_swarms.write().await;
                let now = Utc::now();
                
                swarms.retain(|swarm_id, swarm| {
                    let elapsed = now.signed_duration_since(swarm.last_heartbeat);
                    if elapsed.num_seconds() > timeout_sec as i64 {
                        tracing::warn!("ðŸ’” Swarm {} heartbeat timeout", swarm_id);
                        false
                    } else {
                        true
                    }
                });
            }
        });
    }
}