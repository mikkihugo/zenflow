/**
 * @file Smart Neural Integration Tests
 * Comprehensive integration tests for the SmartNeuralCoordinator system.
 * Tests the complete neural backend integration with BrainCoordinator.
 */

import {
  describe,
  it,
  expect,
  beforeAll,
  afterAll,
  beforeEach,
  vi,
} from 'vitest';
import { BrainCoordinator } from '../../brain-coordinator';
import { SmartNeuralCoordinator } from '../../smart-neural-coordinator';
import { NeuralBridge } from '../../neural-bridge';
import type { BrainConfig } from '../../brain-coordinator';

// Mock external dependencies for testing
vi.mock('@xenova/transformers', () => ({
  pipeline: vi.fn().mockResolvedValue({
    generate: vi.fn().mockResolvedValue([0.1, 0.2, 0.3, 0.4, 0.5]),
  }),
  env: {
    allowRemoteModels: true,
    allowLocalModels: true,
  },
}));

vi.mock('onnxruntime-node', () => ({
  InferenceSession: {
    create: vi.fn().mockResolvedValue({
      run: vi.fn().mockResolvedValue({
        output: { data: new Float32Array([0.1, 0.2, 0.3, 0.4, 0.5]) },
      }),
    }),
  },
}));

describe('Smart Neural Integration Tests', () => {
  let brainCoordinator: BrainCoordinator;
  let smartNeuralCoordinator: SmartNeuralCoordinator;
  let neuralBridge: NeuralBridge;

  const testConfig: BrainConfig = {
    neural: {
      wasmPath: './test-wasm',
      gpuAcceleration: false,
      enableTraining: true,
      smartBackend: {
        primaryModel: 'all-mpnet-base-v2',
        enableFallbacks: true,
        enableCaching: true,
        maxCacheSize: 100,
        performanceThresholds: {
          maxLatency: 5000,
          minAccuracy: 0.8,
        },
      },
    },
    behavioral: {
      learningRate: 0.1,
      adaptationThreshold: 0.75,
      memoryRetention: 0.9,
      patterns: {
        coordination: { weight: 1.0, enabled: true },
        optimization: { weight: 0.8, enabled: true },
        prediction: { weight: 0.6, enabled: false },
      },
    },
    autonomous: {
      enabled: true,
      learningRate: 0.1,
      adaptationThreshold: 0.7,
      decisionBoundary: 0.6,
    },
  };

  beforeAll(async () => {
    // Initialize the brain coordinator with smart neural backend
    brainCoordinator = new BrainCoordinator(testConfig);
    await brainCoordinator.initialize();

    // Get references to internal components
    smartNeuralCoordinator = (brainCoordinator as any).smartNeuralCoordinator;
    neuralBridge = (brainCoordinator as any).neuralBridge;
  });

  afterAll(async () => {
    if (brainCoordinator) {
      await brainCoordinator.shutdown();
    }
  });

  beforeEach(() => {
    vi.clearAllMocks();
  });

  describe('Initialization Integration', () => {
    it('should initialize BrainCoordinator with SmartNeuralCoordinator', async () => {
      expect(brainCoordinator).toBeDefined();
      expect(smartNeuralCoordinator).toBeDefined();
      expect(smartNeuralCoordinator).toBeInstanceOf(SmartNeuralCoordinator);
    });

    it('should initialize NeuralBridge with smart neural backend', async () => {
      expect(neuralBridge).toBeDefined();
      expect(neuralBridge).toBeInstanceOf(NeuralBridge);

      const smartNeuralStats = neuralBridge.getSmartNeuralStats();
      expect(smartNeuralStats.available).toBe(true);
      expect(smartNeuralStats.stats).toBeDefined();
    });

    it('should have correct configuration propagated', () => {
      const stats = smartNeuralCoordinator.getCoordinatorStats();
      expect(stats.configuration.primaryModel).toBe('all-mpnet-base-v2');
      expect(stats.configuration.enableFallbacks).toBe(true);
      expect(stats.configuration.enableCaching).toBe(true);
    });
  });

  describe('Neural Embedding Generation', () => {
    it('should generate embeddings through BrainCoordinator API', async () => {
      const text = 'This is a test sentence for neural embedding generation';

      const result = await brainCoordinator.generateEmbedding(text, {
        context: 'integration-test',
        priority: 'medium',
        qualityLevel: 'standard',
      });

      expect(result).toBeDefined();
      expect(result.success).toBe(true);
      expect(result.embedding).toBeDefined();
      expect(Array.isArray(result.embedding)).toBe(true);
      expect(result.embedding.length).toBeGreaterThan(0);
      expect(result.metadata).toBeDefined();
      expect(result.metadata.model).toBeDefined();
      expect(result.metadata.processingTime).toBeGreaterThan(0);
    });

    it('should generate embeddings through NeuralBridge API', async () => {
      const text = 'Neural bridge integration test sentence';

      const result = await neuralBridge.generateEmbedding(text, {
        context: 'neural-bridge-test',
        priority: 'high',
        qualityLevel: 'premium',
      });

      expect(result).toBeDefined();
      expect(result.success).toBe(true);
      expect(result.embedding).toBeDefined();
      expect(result.metadata.qualityLevel).toBe('premium');
    });

    it('should handle different priority levels correctly', async () => {
      const testCases: Array<{
        text: string;
        priority: 'low'' | ''medium'' | ''high';
      }> = [
        { text: 'Low priority test', priority: 'low' },
        { text: 'Medium priority test', priority: 'medium' },
        { text: 'High priority test', priority: 'high' },
      ];

      const results = await Promise.all(
        testCases.map(async ({ text, priority }) => {
          const result = await brainCoordinator.generateEmbedding(text, {
            priority,
          });
          return { priority, result };
        })
      );

      results.forEach(({ priority, result }) => {
        expect(result.success).toBe(true);
        expect(result.metadata.priority).toBe(priority);
      });
    });
  });

  describe('Caching Integration', () => {
    it('should cache embeddings and retrieve from cache', async () => {
      const text = 'This sentence should be cached';

      // First call - should generate and cache
      const result1 = await brainCoordinator.generateEmbedding(text);
      expect(result1.success).toBe(true);
      expect(result1.metadata.fromCache).toBe(false);

      // Second call - should retrieve from cache
      const result2 = await brainCoordinator.generateEmbedding(text);
      expect(result2.success).toBe(true);
      expect(result2.metadata.fromCache).toBe(true);

      // Results should be identical
      expect(result1.embedding).toEqual(result2.embedding);
    });

    it('should clear cache when requested', async () => {
      const text = 'Cache clearing test sentence';

      // Generate and cache
      await brainCoordinator.generateEmbedding(text);

      // Verify cache has content
      const stats1 = brainCoordinator.getSmartNeuralStats();
      expect(stats1.stats.cache.size).toBeGreaterThan(0);

      // Clear cache
      await brainCoordinator.clearSmartNeuralCache();

      // Verify cache is cleared
      const stats2 = brainCoordinator.getSmartNeuralStats();
      expect(stats2.stats.cache.size).toBe(0);
    });
  });

  describe('Error Handling Integration', () => {
    it('should handle embedding generation errors gracefully', async () => {
      // Mock a failure in the transformers pipeline
      const mockPipeline = vi
        .fn()
        .mockRejectedValue(new Error('Model loading failed'));
      vi.doMock('@xenova/transformers', () => ({
        pipeline: mockPipeline,
      }));

      try {
        const result = await brainCoordinator.generateEmbedding('test text');

        // Should still return a result with fallback
        expect(result).toBeDefined();
        // In a real scenario, this might fall back to brain.js or basic features
      } catch (error) {
        // Error handling should be graceful
        expect(error).toBeInstanceOf(Error);
      }
    });

    it('should handle invalid input gracefully', async () => {
      try {
        // Test with empty string
        const result1 = await brainCoordinator.generateEmbedding('');
        expect(result1.success).toBe(false);

        // Test with very long string
        const longText = 'a'.repeat(10000);
        const result2 = await brainCoordinator.generateEmbedding(longText);
        expect(result2).toBeDefined(); // Should handle but may truncate
      } catch (error) {
        expect(error).toBeInstanceOf(Error);
      }
    });
  });

  describe('Performance Integration', () => {
    it('should track performance metrics correctly', async () => {
      const text = 'Performance testing sentence';

      const result = await brainCoordinator.generateEmbedding(text);

      expect(result.metadata.processingTime).toBeGreaterThan(0);
      expect(result.metadata.processingTime).toBeLessThan(10000); // Should be under 10s

      const stats = brainCoordinator.getSmartNeuralStats();
      expect(stats.stats.performance).toBeDefined();
      expect(stats.stats.performance.totalRequests).toBeGreaterThan(0);
      expect(stats.stats.performance.averageLatency).toBeGreaterThan(0);
    });

    it('should handle concurrent requests efficiently', async () => {
      const texts = [
        'Concurrent request 1',
        'Concurrent request 2',
        'Concurrent request 3',
        'Concurrent request 4',
        'Concurrent request 5',
      ];

      const startTime = Date.now();

      const results = await Promise.all(
        texts.map((text) => brainCoordinator.generateEmbedding(text))
      );

      const totalTime = Date.now() - startTime;

      // All requests should succeed
      results.forEach((result) => {
        expect(result.success).toBe(true);
      });

      // Concurrent processing should be faster than sequential
      expect(totalTime).toBeLessThan(15000); // Should complete in under 15s

      const stats = brainCoordinator.getSmartNeuralStats();
      expect(stats.stats.performance.totalRequests).toBeGreaterThanOrEqual(5);
    });
  });

  describe('Fallback System Integration', () => {
    it('should use fallback systems when primary model fails', async () => {
      // This test would require more complex mocking to simulate model failures
      // For now, we verify the fallback configuration is in place
      const stats = smartNeuralCoordinator.getCoordinatorStats();
      expect(stats.configuration.enableFallbacks).toBe(true);
      expect(stats.fallbackChain).toBeDefined();
      expect(stats.fallbackChain.length).toBeGreaterThan(0);
    });
  });

  describe('System Statistics Integration', () => {
    it('should provide comprehensive system statistics', async () => {
      // Generate some activity
      await brainCoordinator.generateEmbedding('Statistics test 1');
      await brainCoordinator.generateEmbedding('Statistics test 2');

      const brainStats = brainCoordinator.getSmartNeuralStats();
      const bridgeStats = neuralBridge.getSmartNeuralStats();
      const coordinatorStats = smartNeuralCoordinator.getCoordinatorStats();

      // Verify all stats are available
      expect(brainStats.available).toBe(true);
      expect(brainStats.stats).toBeDefined();

      expect(bridgeStats.available).toBe(true);
      expect(bridgeStats.stats).toBeDefined();

      expect(coordinatorStats).toBeDefined();
      expect(coordinatorStats.performance.totalRequests).toBeGreaterThan(0);
      expect(coordinatorStats.cache.size).toBeGreaterThanOrEqual(0);
      expect(coordinatorStats.models.primary.status).toBeDefined();
    });
  });

  describe('Lifecycle Integration', () => {
    it('should handle shutdown gracefully', async () => {
      // This test verifies that all components can be shut down cleanly
      const testBrainCoordinator = new BrainCoordinator(testConfig);
      await testBrainCoordinator.initialize();

      // Verify initialization
      expect(testBrainCoordinator).toBeDefined();

      // Generate some activity
      await testBrainCoordinator.generateEmbedding('Lifecycle test');

      // Shutdown should complete without errors
      await expect(testBrainCoordinator.shutdown()).resolves.not.toThrow();
    });
  });
});
