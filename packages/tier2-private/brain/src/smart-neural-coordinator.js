/**
 * @fileoverview Smart Neural Coordinator - Intelligent Neural Backend System
 *
 * Advanced neural backend system with intelligent model selection, fallback chains,
 * and performance optimization. Implements single primary model strategy with
 * smart caching and graceful degradation.
 *
 * Key Features:
 * - Single primary model (all-mpnet-base-v2) for optimal quality/performance balance
 * - Intelligent fallback chain (transformers → brain.js → basic features)
 * - Smart caching system with performance-based eviction
 * - Optional OpenAI upgrade for premium quality
 * - Graceful degradation if models fail to load
 * - Comprehensive performance monitoring and telemetry
 *
 * @author Claude Code Zen Team
 * @since 2.1.0
 */
import { getLogger, ContextError } from '@claude-zen/foundation';
// Telemetry helpers - will be replaced by operations facade calls
const recordMetric = (_name, _value, _metadata) => { };
const recordHistogram = (_name, _value, _metadata) => { };
const recordGauge = (_name, _value, _metadata) => { };
const withTrace = (_name, fn) => fn({});
const withAsyncTrace = (_name, fn) => fn({});
const recordEvent = (_name, _data) => { };
// Neural backend imports with smart loading
let transformersModule = null;
let brainJsModule = null;
let onnxModule = null;
let openaiModule = null;
/**
 * Smart Neural Coordinator - Intelligent neural backend system
 *
 * Provides high-quality neural embeddings through intelligent model selection,
 * smart caching, and graceful fallback chains. Optimized for production use
 * with comprehensive monitoring and telemetry integration.
 *
 * @example
 * ```typescript
 * const coordinator = new SmartNeuralCoordinator({
 *   primaryModel: 'all-mpnet-base-v2',
 *   enableFallbacks: true,
 *   cache: { maxSize: 10000, ttlMs: 3600000, performanceBasedEviction: true }
 * });
 *
 * await coordinator.initialize();
 *
 * const result = await coordinator.generateEmbedding({
 *   text: "Machine learning is transforming software development",
 *   qualityLevel: 'standard',
 *   priority: 'high'
 * });
 *
 * logger.info(`Embedding: ${result.embedding.length}D, Quality: ${result.qualityScore}`);
 * ```
 */
export class SmartNeuralCoordinator {
    logger;
    config;
    initialized = false;
    // Model instances and status tracking
    transformerModel = null;
    brainJsNetwork = null;
    onnxSession = null;
    openaiClient = null;
    modelStatus = new Map();
    // Smart caching system
    embeddingCache = new Map();
    cacheStats = {
        hits: 0,
        misses: 0,
        evictions: 0,
        totalRequests: 0
    };
    // Performance monitoring
    performanceMetrics = {
        totalEmbeddings: 0,
        averageLatency: 0,
        failedEmbeddings: 0,
        minLatency: Number.MAX_VALUE,
        maxLatency: 0,
        modelUsageCount: new Map(),
        fallbackUsageCount: new Map(),
        qualityDistribution: new Map()
    };
    constructor(config = {}) {
        this.logger = getLogger('smart-neural-coordinator');
        // Default configuration with intelligent defaults
        this.config = {
            primaryModel: 'all-mpnet-base-v2',
            enableFallbacks: true,
            cache: {
                maxSize: 10000,
                ttlMs: 3600000, // 1 hour
                performanceBasedEviction: true
            },
            loading: {
                timeoutMs: 30000, // 30 seconds
                retryAttempts: 3,
                lazyLoading: true
            },
            performance: {
                batchSize: 32,
                maxConcurrency: 4,
                enableProfiling: true
            },
            ...config
        };
        this.logger.info('SmartNeuralCoordinator created with intelligent backend configuration');
        // Record initialization metric
        recordMetric('smart_neural_coordinator_created', 1, {
            primary_model: this.config.primaryModel,
            fallbacks_enabled: String(this.config.enableFallbacks),
            cache_enabled: String(this.config.cache.maxSize > 0)
        });
    }
    /**
     * Initialize the Smart Neural Coordinator with intelligent model loading
     */
    async initialize() {
        if (this.initialized) {
            this.logger.debug('SmartNeuralCoordinator already initialized');
            return;
        }
        return withAsyncTrace('smart-neural-coordinator-initialize', async (span) => {
            const initTimer = Date.now();
            try {
                this.logger.info('🧠 Initializing SmartNeuralCoordinator with intelligent backend loading...');
                span.setAttributes({
                    'neural.coordinator.version': '2.1.0',
                    'neural.config.primary_model': this.config.primaryModel,
                    'neural.config.enable_fallbacks': this.config.enableFallbacks,
                    'neural.config.cache_max_size': this.config.cache.maxSize,
                    'neural.config.lazy_loading': this.config.loading.lazyLoading
                });
                // Initialize model status tracking
                this.initializeModelStatus();
                // Load models based on configuration
                await (this.config.loading.lazyLoading ? this.initializeLazyLoading() : this.initializeEagerLoading());
                // Initialize premium features if available
                if (this.config.premium?.enableOpenaiUpgrade) {
                    await this.initializeOpenAI();
                }
                // Start performance monitoring
                if (this.config.performance.enableProfiling) {
                    this.startPerformanceMonitoring();
                }
                this.initialized = true;
                const initTime = Date.now() - initTimer;
                // Record comprehensive initialization metrics
                recordMetric('smart_neural_coordinator_initialized', 1, {
                    status: 'success',
                    duration_ms: String(initTime),
                    models_loaded: String(this.getLoadedModelsCount()),
                    primary_model_ready: String(this.isPrimaryModelReady())
                });
                recordHistogram('smart_neural_initialization_duration_ms', initTime, {
                    lazy_loading: String(this.config.loading.lazyLoading)
                });
                span.setAttributes({
                    'neural.initialization.success': true,
                    'neural.initialization.duration_ms': initTime,
                    'neural.initialization.models_loaded': this.getLoadedModelsCount(),
                    'neural.initialization.primary_model_ready': this.isPrimaryModelReady()
                });
                this.logger.info(`✅ SmartNeuralCoordinator initialized successfully in ${initTime}ms`);
                recordEvent('smart-neural-coordinator-initialized', {
                    duration_ms: String(initTime),
                    models_loaded: String(this.getLoadedModelsCount())
                });
            }
            catch (error) {
                const initTime = Date.now() - initTimer;
                recordMetric('smart_neural_coordinator_initialized', 1, {
                    status: 'error',
                    duration_ms: String(initTime),
                    error_type: error instanceof Error ? error.constructor.name : 'unknown'
                });
                span.setAttributes({
                    'neural.initialization.success': false,
                    'neural.initialization.error': error instanceof Error ? error.message : String(error)
                });
                this.logger.error('❌ Failed to initialize SmartNeuralCoordinator:', error);
                throw new ContextError(`SmartNeuralCoordinator initialization failed: ${error}`, {
                    code: 'NEURAL_INIT_ERROR'
                });
            }
        });
    }
    // =============================================================================
    // PHASE 1: EMBEDDING GENERATION
    // =============================================================================
    /**
     * Generate neural embeddings with intelligent model selection and fallbacks
     */
    async generateEmbedding(request) {
        if (!this.initialized) {
            await this.initialize();
        }
        return withAsyncTrace('smart-neural-generate-embedding', async (span) => {
            const startTime = Date.now();
            // Input validation
            if (!request.text || request.text.trim().length === 0) {
                return {
                    success: false,
                    embedding: [],
                    confidence: 0,
                    model: 'basic',
                    processingTime: 0,
                    fromCache: false,
                    qualityScore: 0,
                    error: 'Input text cannot be empty',
                    metadata: {
                        model: 'basic',
                        processingTime: 0,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        confidence: 0,
                        qualityScore: 0
                    }
                };
            }
            if (request.text.length > 10000) {
                return {
                    success: false,
                    embedding: [],
                    confidence: 0,
                    model: 'basic',
                    processingTime: 0,
                    fromCache: false,
                    qualityScore: 0,
                    error: 'Input text is too long (max 10,000 characters)',
                    metadata: {
                        model: 'basic',
                        processingTime: 0,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        confidence: 0,
                        qualityScore: 0
                    }
                };
            }
            const cacheKey = request.cacheKey || this.generateCacheKey(request);
            // Set comprehensive telemetry attributes
            span.setAttributes({
                'neural.request.text_length': request.text.length,
                'neural.request.priority': request.priority || 'medium',
                'neural.request.quality_level': request.qualityLevel || 'standard',
                'neural.request.has_context': Boolean(request.context),
                'neural.request.cache_key': cacheKey
            });
            // Update request statistics
            this.cacheStats.totalRequests++;
            this.performanceMetrics.totalEmbeddings++;
            try {
                // Check cache first with intelligent cache strategy
                const cachedResult = this.getCachedEmbedding(cacheKey);
                if (cachedResult) {
                    const processingTime = Date.now() - startTime;
                    this.logger.debug(`📦 Using cached embedding for request (${processingTime}ms)`);
                    recordMetric('smart_neural_embedding_generated', 1, {
                        cache_hit: 'true',
                        model: cachedResult.model,
                        quality_level: request.qualityLevel || 'standard',
                        status: 'success'
                    });
                    span.setAttributes({
                        'neural.embedding.cache_hit': true,
                        'neural.embedding.model': cachedResult.model,
                        'neural.embedding.confidence': cachedResult.confidence,
                        'neural.embedding.quality_score': cachedResult.performanceScore
                    });
                    return {
                        success: true,
                        embedding: cachedResult.embedding,
                        confidence: cachedResult.confidence,
                        model: cachedResult.model,
                        processingTime,
                        fromCache: true,
                        qualityScore: cachedResult.performanceScore,
                        metadata: {
                            model: cachedResult.model,
                            processingTime,
                            fromCache: true,
                            priority: request.priority,
                            qualityLevel: request.qualityLevel,
                            context: request.context,
                            confidence: cachedResult.confidence,
                            qualityScore: cachedResult.performanceScore
                        }
                    };
                }
                // Generate new embedding with intelligent model selection
                const result = await this.generateNewEmbedding(request, span);
                const processingTime = Date.now() - startTime;
                // Cache the result with performance-based scoring
                this.cacheEmbedding(cacheKey, {
                    embedding: result.embedding,
                    confidence: result.confidence,
                    model: result.model,
                    timestamp: Date.now(),
                    accessCount: 1,
                    lastAccessTime: Date.now(),
                    performanceScore: this.calculatePerformanceScore(result, processingTime)
                });
                // Update performance metrics
                this.updatePerformanceMetrics(result.model, processingTime, result.qualityScore);
                recordMetric('smart_neural_embedding_generated', 1, {
                    cache_hit: 'false',
                    model: result.model,
                    quality_level: request.qualityLevel || 'standard',
                    status: 'success',
                    fallbacks_used: String(result.fallbacksUsed?.length || 0)
                });
                recordHistogram('smart_neural_embedding_duration_ms', processingTime, {
                    model: result.model,
                    cache_hit: 'false'
                });
                recordGauge('smart_neural_embedding_quality_score', result.qualityScore, {
                    model: result.model,
                    quality_level: request.qualityLevel || 'standard'
                });
                span.setAttributes({
                    'neural.embedding.cache_hit': false,
                    'neural.embedding.model': result.model,
                    'neural.embedding.confidence': result.confidence,
                    'neural.embedding.quality_score': result.qualityScore,
                    'neural.embedding.processing_time_ms': processingTime,
                    'neural.embedding.fallbacks_used': result.fallbacksUsed?.length || 0
                });
                this.logger.info(`🧠 Generated embedding using ${result.model} (${processingTime}ms, quality: ${result.qualityScore.toFixed(2)})`);
                recordEvent('smart-neural-embedding-generated', {
                    model: result.model,
                    processing_time_ms: String(processingTime),
                    quality_score: String(result.qualityScore),
                    fallbacks_used: String(result.fallbacksUsed?.length || 0)
                });
                return {
                    success: true,
                    embedding: result.embedding,
                    confidence: result.confidence,
                    model: result.model,
                    processingTime,
                    fromCache: false,
                    qualityScore: result.qualityScore,
                    fallbacksUsed: result.fallbacksUsed,
                    metadata: {
                        model: result.model,
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        confidence: result.confidence,
                        qualityScore: result.qualityScore
                    }
                };
            }
            catch (error) {
                const processingTime = Date.now() - startTime;
                recordMetric('smart_neural_embedding_generated', 1, {
                    cache_hit: 'false',
                    model: 'error',
                    status: 'error',
                    error_type: error instanceof Error ? error.constructor.name : 'unknown'
                });
                span.setAttributes({
                    'neural.embedding.error': true,
                    'neural.embedding.error_message': error instanceof Error ? error.message : String(error),
                    'neural.embedding.processing_time_ms': processingTime
                });
                this.logger.error('❌ Failed to generate neural embedding:', error);
                // Return a basic fallback result
                return {
                    success: false,
                    embedding: this.generateBasicEmbedding(request.text),
                    confidence: 0.3,
                    model: 'basic',
                    processingTime,
                    fromCache: false,
                    qualityScore: 0.3,
                    fallbacksUsed: ['basic-fallback'],
                    error: error instanceof Error ? error.message : String(error),
                    metadata: {
                        model: 'basic',
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        confidence: 0.3,
                        qualityScore: 0.3
                    }
                };
            }
        });
    }
    // =============================================================================
    // PHASE 2: CLASSIFICATION PHASE
    // =============================================================================
    /**
     * Classify text using intelligent model selection and fallback chains
     */
    async classifyText(request) {
        if (!this.initialized) {
            await this.initialize();
        }
        return withAsyncTrace('smart-neural-classify-text', async (span) => {
            const startTime = Date.now();
            // Input validation
            if (!request.text || request.text.trim().length === 0) {
                return this.createClassificationErrorResult('Input text cannot be empty', startTime, request);
            }
            if (request.text.length > 10000) {
                return this.createClassificationErrorResult('Input text is too long (max 10,000 characters)', startTime, request);
            }
            const cacheKey = request.cacheKey || this.generateClassificationCacheKey(request);
            span.setAttributes({
                'neural.classification.text_length': request.text.length,
                'neural.classification.task_type': request.taskType,
                'neural.classification.priority': request.priority || 'medium',
                'neural.classification.quality_level': request.qualityLevel || 'standard',
                'neural.classification.cache_key': cacheKey
            });
            this.cacheStats.totalRequests++;
            try {
                // Check cache first
                const cachedResult = this.getCachedClassification(cacheKey);
                if (cachedResult) {
                    return this.createClassificationCacheResult(cachedResult, startTime, request, span);
                }
                // Generate new classification
                const result = await this.generateNewClassification(request, span);
                const processingTime = Date.now() - startTime;
                // Cache the result
                this.cacheClassification(cacheKey, result, processingTime);
                // Update performance metrics
                this.updatePerformanceMetrics(result.model, processingTime, result.qualityScore);
                this.recordClassificationMetrics(result, processingTime, request, false);
                return {
                    success: true,
                    classification: result.classification,
                    model: result.model,
                    processingTime,
                    fromCache: false,
                    qualityScore: result.qualityScore,
                    fallbacksUsed: result.fallbacksUsed,
                    metadata: {
                        taskType: request.taskType,
                        model: result.model,
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        confidenceThreshold: request.confidenceThreshold,
                        qualityScore: result.qualityScore
                    }
                };
            }
            catch (error) {
                const processingTime = Date.now() - startTime;
                this.recordClassificationError(error, processingTime, request, span);
                return this.createClassificationFallbackResult(request.text, processingTime, request, error);
            }
        });
    }
    // =============================================================================
    // PHASE 3: GENERATION PHASE  
    // =============================================================================
    /**
     * Generate text using intelligent model selection and fallback chains
     */
    async generateText(request) {
        if (!this.initialized) {
            await this.initialize();
        }
        return withAsyncTrace('smart-neural-generate-text', async (span) => {
            const startTime = Date.now();
            // Input validation
            if (!request.prompt || request.prompt.trim().length === 0) {
                return this.createGenerationErrorResult('Input prompt cannot be empty', startTime, request);
            }
            if (request.prompt.length > 20000) {
                return this.createGenerationErrorResult('Input prompt is too long (max 20,000 characters)', startTime, request);
            }
            const cacheKey = request.cacheKey || this.generateGenerationCacheKey(request);
            span.setAttributes({
                'neural.generation.prompt_length': request.prompt.length,
                'neural.generation.task_type': request.taskType,
                'neural.generation.priority': request.priority || 'medium',
                'neural.generation.quality_level': request.qualityLevel || 'standard',
                'neural.generation.max_length': request.maxLength || 1000,
                'neural.generation.temperature': request.temperature || 0.7,
                'neural.generation.cache_key': cacheKey
            });
            this.cacheStats.totalRequests++;
            try {
                // Check cache first
                const cachedResult = this.getCachedGeneration(cacheKey);
                if (cachedResult) {
                    return this.createGenerationCacheResult(cachedResult, startTime, request, span);
                }
                // Generate new text
                const result = await this.generateNewText(request, span);
                const processingTime = Date.now() - startTime;
                // Cache the result
                this.cacheGeneration(cacheKey, result, processingTime);
                // Update performance metrics
                this.updatePerformanceMetrics(result.model, processingTime, result.qualityScore);
                this.recordGenerationMetrics(result, processingTime, request, false);
                return {
                    success: true,
                    generated: result.generated,
                    model: result.model,
                    processingTime,
                    fromCache: false,
                    qualityScore: result.qualityScore,
                    fallbacksUsed: result.fallbacksUsed,
                    metadata: {
                        taskType: request.taskType,
                        model: result.model,
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        parameters: {
                            maxLength: request.maxLength,
                            temperature: request.temperature,
                            topP: request.topP,
                            topK: request.topK
                        },
                        qualityScore: result.qualityScore
                    }
                };
            }
            catch (error) {
                const processingTime = Date.now() - startTime;
                this.recordGenerationError(error, processingTime, request, span);
                return this.createGenerationFallbackResult(request.prompt, processingTime, request, error);
            }
        });
    }
    // =============================================================================
    // PHASE 4: VISION PHASE
    // =============================================================================
    /**
     * Process images using intelligent model selection and fallback chains
     */
    async processImage(request) {
        if (!this.initialized) {
            await this.initialize();
        }
        return withAsyncTrace('smart-neural-process-image', async (span) => {
            const startTime = Date.now();
            // Input validation
            if (!request.image) {
                return this.createVisionErrorResult('Input image cannot be empty', startTime, request);
            }
            const imageInfo = this.analyzeImageInput(request.image);
            if (!imageInfo.valid) {
                return this.createVisionErrorResult('Invalid image format or data', startTime, request);
            }
            const cacheKey = request.cacheKey || this.generateVisionCacheKey(request, imageInfo);
            span.setAttributes({
                'neural.vision.task_type': request.taskType,
                'neural.vision.image_format': imageInfo.format,
                'neural.vision.image_size': imageInfo.size,
                'neural.vision.priority': request.priority || 'medium',
                'neural.vision.quality_level': request.qualityLevel || 'standard',
                'neural.vision.cache_key': cacheKey
            });
            this.cacheStats.totalRequests++;
            try {
                // Check cache first
                const cachedResult = this.getCachedVision(cacheKey);
                if (cachedResult) {
                    return this.createVisionCacheResult(cachedResult, startTime, request, span, imageInfo);
                }
                // Process image
                const result = await this.processNewImage(request, span, imageInfo);
                const processingTime = Date.now() - startTime;
                // Cache the result
                this.cacheVision(cacheKey, result, processingTime);
                // Update performance metrics
                this.updatePerformanceMetrics(result.model, processingTime, result.qualityScore);
                this.recordVisionMetrics(result, processingTime, request, false);
                return {
                    success: true,
                    vision: result.vision,
                    model: result.model,
                    processingTime,
                    fromCache: false,
                    qualityScore: result.qualityScore,
                    fallbacksUsed: result.fallbacksUsed,
                    metadata: {
                        taskType: request.taskType,
                        model: result.model,
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        context: request.context,
                        imageInfo,
                        qualityScore: result.qualityScore
                    }
                };
            }
            catch (error) {
                const processingTime = Date.now() - startTime;
                this.recordVisionError(error, processingTime, request, span);
                return this.createVisionFallbackResult(request, processingTime, error, imageInfo);
            }
        });
    }
    // =============================================================================
    // PHASE 5: OTHER NEURAL TASKS
    // =============================================================================
    /**
     * Perform various neural tasks using intelligent model selection and fallback chains
     */
    async performNeuralTask(request) {
        if (!this.initialized) {
            await this.initialize();
        }
        return withAsyncTrace('smart-neural-perform-task', async (span) => {
            const startTime = Date.now();
            // Input validation
            const validationError = this.validateNeuralTaskRequest(request);
            if (validationError) {
                return this.createNeuralTaskErrorResult(validationError, startTime, request);
            }
            const cacheKey = request.cacheKey || this.generateNeuralTaskCacheKey(request);
            span.setAttributes({
                'neural.task.type': request.taskType,
                'neural.task.priority': request.priority || 'medium',
                'neural.task.quality_level': request.qualityLevel || 'standard',
                'neural.task.cache_key': cacheKey
            });
            this.cacheStats.totalRequests++;
            try {
                // Check cache first
                const cachedResult = this.getCachedNeuralTask(cacheKey);
                if (cachedResult) {
                    return this.createNeuralTaskCacheResult(cachedResult, startTime, request, span);
                }
                // Perform neural task
                const result = await this.executeNewNeuralTask(request, span);
                const processingTime = Date.now() - startTime;
                // Cache the result
                this.cacheNeuralTask(cacheKey, result, processingTime);
                // Update performance metrics
                this.updatePerformanceMetrics(result.model, processingTime, result.qualityScore);
                this.recordNeuralTaskMetrics(result, processingTime, request, false);
                return {
                    success: true,
                    result: result.result,
                    model: result.model,
                    processingTime,
                    fromCache: false,
                    qualityScore: result.qualityScore,
                    fallbacksUsed: result.fallbacksUsed,
                    metadata: {
                        taskType: request.taskType,
                        model: result.model,
                        processingTime,
                        fromCache: false,
                        priority: request.priority,
                        qualityLevel: request.qualityLevel,
                        parameters: request.parameters,
                        qualityScore: result.qualityScore
                    }
                };
            }
            catch (error) {
                const processingTime = Date.now() - startTime;
                this.recordNeuralTaskError(error, processingTime, request, span);
                return this.createNeuralTaskFallbackResult(request, processingTime, error);
            }
        });
    }
    /**
     * Get comprehensive neural coordinator statistics and insights
     */
    getCoordinatorStats() {
        return withTrace('smart-neural-get-stats', (span) => {
            const primaryModelReady = this.isPrimaryModelReady();
            const fallbacksAvailable = this.getAvailableFallbacksCount();
            const cacheEfficiency = this.calculateCacheEfficiency();
            const averageQuality = this.calculateAverageQuality();
            span.setAttributes({
                'neural.stats.initialized': this.initialized,
                'neural.stats.primary_model_ready': primaryModelReady,
                'neural.stats.fallbacks_available': fallbacksAvailable,
                'neural.stats.cache_efficiency': cacheEfficiency,
                'neural.stats.average_quality': averageQuality
            });
            recordEvent('smart-neural-stats-retrieved', {
                primary_model_ready: String(primaryModelReady),
                fallbacks_available: String(fallbacksAvailable),
                cache_efficiency: String(cacheEfficiency)
            });
            const primaryModelStatus = this.modelStatus.get('transformers');
            return {
                initialized: this.initialized,
                configuration: {
                    primaryModel: this.config.primaryModel,
                    enableFallbacks: this.config.enableFallbacks,
                    enableCaching: this.config.cache.maxSize > 0,
                    maxCacheSize: this.config.cache.maxSize,
                    performanceThresholds: this.config.performance.thresholds ? {
                        maxLatency: this.config.performance.thresholds.maxLatency,
                        minAccuracy: this.config.performance.thresholds.minConfidence // Map minConfidence to minAccuracy
                    } : undefined
                },
                models: {
                    primary: {
                        status: primaryModelStatus?.loaded ? 'ready' :
                            primaryModelStatus?.loading ? 'loading' :
                                primaryModelStatus?.error ? 'error' : 'not_loaded',
                        model: this.config.primaryModel,
                        lastUsed: primaryModelStatus?.lastUsed
                    },
                    fallbacks: Array.from(this.modelStatus.values()).filter(status => status.name !== 'transformers')
                },
                performance: {
                    totalRequests: this.performanceMetrics.totalEmbeddings,
                    successfulRequests: this.performanceMetrics.totalEmbeddings - this.performanceMetrics.failedEmbeddings,
                    failedRequests: this.performanceMetrics.failedEmbeddings,
                    averageLatency: this.performanceMetrics.averageLatency,
                    minLatency: this.performanceMetrics.minLatency,
                    maxLatency: this.performanceMetrics.maxLatency
                },
                cache: {
                    size: this.embeddingCache.size,
                    hits: this.cacheStats.hits,
                    misses: this.cacheStats.misses,
                    evictions: this.cacheStats.evictions
                },
                fallbackChain: ['transformers', 'brain.js', 'basic-features'],
                systemHealth: {
                    primaryModelReady,
                    fallbacksAvailable,
                    cacheEfficiency,
                    averageQuality
                }
            };
        });
    }
    /**
     * Clear caches and reset coordinator state
     */
    async clearCache() {
        return withAsyncTrace('smart-neural-clear-cache', async (span) => {
            const cacheSize = this.embeddingCache.size;
            this.embeddingCache.clear();
            this.cacheStats = {
                hits: 0,
                misses: 0,
                evictions: 0,
                totalRequests: 0
            };
            recordMetric('smart_neural_cache_cleared', 1, {
                previous_size: String(cacheSize),
                status: 'success'
            });
            span.setAttributes({
                'neural.cache.previous_size': cacheSize,
                'neural.cache.cleared': true
            });
            this.logger.info('🗑️ SmartNeuralCoordinator cache cleared', { previousSize: cacheSize });
            recordEvent('smart-neural-cache-cleared', {
                previous_size: String(cacheSize)
            });
        });
    }
    /**
     * Shutdown coordinator and cleanup resources
     */
    async shutdown() {
        return withAsyncTrace('smart-neural-coordinator-shutdown', async (span) => {
            try {
                this.logger.info('🛑 Shutting down SmartNeuralCoordinator...');
                // Clear caches
                await this.clearCache();
                // Cleanup model instances
                this.transformerModel = null;
                this.brainJsNetwork = null;
                this.onnxSession = null;
                this.openaiClient = null;
                // Reset state
                this.modelStatus.clear();
                this.initialized = false;
                recordMetric('smart_neural_coordinator_shutdown', 1, {
                    status: 'success'
                });
                span.setAttributes({
                    'neural.shutdown.success': true,
                    'neural.shutdown.cache_cleared': true
                });
                this.logger.info('✅ SmartNeuralCoordinator shutdown completed');
                recordEvent('smart-neural-coordinator-shutdown-complete', {
                    status: 'success'
                });
            }
            catch (error) {
                recordMetric('smart_neural_coordinator_shutdown', 1, {
                    status: 'error',
                    error_type: error instanceof Error ? error.constructor.name : 'unknown'
                });
                span.setAttributes({
                    'neural.shutdown.success': false,
                    'neural.shutdown.error': error instanceof Error ? error.message : String(error)
                });
                this.logger.error('❌ Failed to shutdown SmartNeuralCoordinator:', error);
                throw error;
            }
        });
    }
    // Private implementation methods
    initializeModelStatus() {
        const models = ['transformers', 'brain-js', 'onnx', 'openai', 'basic'];
        for (const model of models) {
            this.modelStatus.set(model, {
                name: model,
                loaded: false,
                loading: false,
                error: null,
                successRate: 0,
                averageLatency: 0
            });
        }
    }
    async initializeLazyLoading() {
        this.logger.info('🔄 Initialized lazy loading mode - models will load on demand');
        // Mark basic fallback as always available
        const basicStatus = this.modelStatus.get('basic');
        if (basicStatus) {
            basicStatus.loaded = true;
            basicStatus.loadingTime = 0;
        }
    }
    async initializeEagerLoading() {
        this.logger.info('⚡ Eager loading mode - attempting to load all models...');
        // Load primary model first
        await this.loadTransformersModel();
        // Load fallback models
        if (this.config.enableFallbacks) {
            await Promise.allSettled([
                this.loadBrainJsModel(),
                this.loadOnnxModel()
            ]);
        }
    }
    async initializeOpenAI() {
        if (!this.config.premium?.openaiApiKey) {
            this.logger.warn('OpenAI API key not provided, premium features disabled');
            return;
        }
        try {
            const startTime = Date.now();
            if (!openaiModule) {
                openaiModule = await import('openai');
            }
            this.openaiClient = new openaiModule.default({
                apiKey: this.config.premium.openaiApiKey
            });
            const loadingTime = Date.now() - startTime;
            const openaiStatus = this.modelStatus.get('openai');
            if (openaiStatus) {
                openaiStatus.loaded = true;
                openaiStatus.loadingTime = loadingTime;
            }
            this.logger.info(`✨ OpenAI client initialized for premium features (${loadingTime}ms)`);
        }
        catch (error) {
            const openaiStatus = this.modelStatus.get('openai');
            if (openaiStatus) {
                openaiStatus.loaded = false;
                openaiStatus.errorMessage = error instanceof Error ? error.message : String(error);
            }
            this.logger.warn('Failed to initialize OpenAI client:', error);
        }
    }
    async loadTransformersModel() {
        try {
            const startTime = Date.now();
            if (!transformersModule) {
                transformersModule = await import('@xenova/transformers');
            }
            this.transformerModel = await transformersModule.pipeline('feature-extraction', this.config.primaryModel, {
                device: 'cpu',
                dtype: 'fp32'
            });
            const loadingTime = Date.now() - startTime;
            const transformersStatus = this.modelStatus.get('transformers');
            if (transformersStatus) {
                transformersStatus.loaded = true;
                transformersStatus.loadingTime = loadingTime;
            }
            this.logger.info(`✅ Transformers model loaded: ${this.config.primaryModel} (${loadingTime}ms)`);
        }
        catch (error) {
            const transformersStatus = this.modelStatus.get('transformers');
            if (transformersStatus) {
                transformersStatus.loaded = false;
                transformersStatus.errorMessage = error instanceof Error ? error.message : String(error);
            }
            this.logger.warn('Failed to load transformers model:', error);
        }
    }
    async loadBrainJsModel() {
        try {
            const startTime = Date.now();
            if (!brainJsModule) {
                brainJsModule = await import('brain.js');
            }
            // Create a simple neural network for text embedding approximation
            this.brainJsNetwork = new brainJsModule.NeuralNetwork({
                hiddenLayers: [256, 128, 64],
                activation: 'sigmoid'
            });
            const loadingTime = Date.now() - startTime;
            const brainJsStatus = this.modelStatus.get('brain-js');
            if (brainJsStatus) {
                brainJsStatus.loaded = true;
                brainJsStatus.loadingTime = loadingTime;
            }
            this.logger.info(`✅ Brain.js model loaded (${loadingTime}ms)`);
        }
        catch (error) {
            const brainJsStatus = this.modelStatus.get('brain-js');
            if (brainJsStatus) {
                brainJsStatus.loaded = false;
                brainJsStatus.errorMessage = error instanceof Error ? error.message : String(error);
            }
            this.logger.warn('Failed to load Brain.js model:', error);
        }
    }
    async loadOnnxModel() {
        try {
            const startTime = Date.now();
            if (!onnxModule) {
                onnxModule = await import('onnxruntime-node');
            }
            // ONNX model loading would happen here
            // For now, just mark as loaded for fallback capability
            const loadingTime = Date.now() - startTime;
            const onnxStatus = this.modelStatus.get('onnx');
            if (onnxStatus) {
                onnxStatus.loaded = true;
                onnxStatus.loadingTime = loadingTime;
            }
            this.logger.info(`✅ ONNX runtime loaded (${loadingTime}ms)`);
        }
        catch (error) {
            const onnxStatus = this.modelStatus.get('onnx');
            if (onnxStatus) {
                onnxStatus.loaded = false;
                onnxStatus.errorMessage = error instanceof Error ? error.message : String(error);
            }
            this.logger.warn('Failed to load ONNX runtime:', error);
        }
    }
    startPerformanceMonitoring() {
        // Start periodic performance monitoring
        setInterval(() => {
            this.performPerformanceOptimization();
        }, 60000); // Every minute
        this.logger.info('📊 Performance monitoring started');
    }
    async generateNewEmbedding(request, span) {
        const fallbacksUsed = [];
        // Try premium OpenAI first if requested and available
        if (request.qualityLevel === 'premium' && this.openaiClient) {
            try {
                const result = await this.generateOpenAIEmbedding(request.text);
                span.setAttributes({ 'neural.embedding.primary_method': 'openai' });
                return { ...result, fallbacksUsed };
            }
            catch (error) {
                fallbacksUsed.push('openai-failed');
                this.logger.debug('OpenAI embedding failed, falling back:', error);
            }
        }
        // Try primary transformers model
        if (this.transformerModel || this.config.loading.lazyLoading) {
            try {
                if (!this.transformerModel && this.config.loading.lazyLoading) {
                    await this.loadTransformersModel();
                }
                if (this.transformerModel) {
                    const result = await this.generateTransformersEmbedding(request.text);
                    span.setAttributes({ 'neural.embedding.primary_method': 'transformers' });
                    return { ...result, fallbacksUsed };
                }
            }
            catch (error) {
                fallbacksUsed.push('transformers-failed');
                this.logger.debug('Transformers embedding failed, falling back:', error);
            }
        }
        // Try Brain.js fallback
        if (this.config.enableFallbacks && (this.brainJsNetwork || this.config.loading.lazyLoading)) {
            try {
                if (!this.brainJsNetwork && this.config.loading.lazyLoading) {
                    await this.loadBrainJsModel();
                }
                if (this.brainJsNetwork) {
                    const result = await this.generateBrainJsEmbedding(request.text);
                    fallbacksUsed.push('brain-js');
                    span.setAttributes({ 'neural.embedding.primary_method': 'brain-js' });
                    return { ...result, fallbacksUsed };
                }
            }
            catch (error) {
                fallbacksUsed.push('brain-js-failed');
                this.logger.debug('Brain.js embedding failed, falling back:', error);
            }
        }
        // Final fallback: basic text features
        fallbacksUsed.push('basic');
        span.setAttributes({ 'neural.embedding.primary_method': 'basic' });
        return {
            success: true,
            embedding: this.generateBasicEmbedding(request.text),
            confidence: 0.4,
            model: 'basic',
            processingTime: 0,
            fromCache: false,
            qualityScore: 0.4,
            fallbacksUsed,
            metadata: {
                model: 'basic',
                processingTime: 0,
                fromCache: false,
                confidence: 0.5,
                qualityScore: 0.6
            }
        };
    }
    async generateTransformersEmbedding(text) {
        const startTime = Date.now();
        const output = await this.transformerModel(text, { pooling: 'mean', normalize: true });
        const embedding = Array.from(output.data);
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('transformers', processingTime, true);
        return {
            success: true,
            embedding,
            confidence: 0.9,
            model: 'transformers',
            processingTime,
            fromCache: false,
            qualityScore: 0.9,
            metadata: {
                model: 'transformers',
                processingTime,
                fromCache: false,
                confidence: 0.9,
                qualityScore: 0.9
            }
        };
    }
    async generateBrainJsEmbedding(text) {
        const startTime = Date.now();
        // Convert text to simple numerical features
        const features = this.textToFeatures(text);
        // Use brain.js network to generate embedding-like output
        const output = this.brainJsNetwork.run(features);
        const embedding = Object.values(output);
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('brain-js', processingTime, true);
        return {
            success: true,
            embedding,
            confidence: 0.7,
            model: 'brain-js',
            processingTime,
            fromCache: false,
            qualityScore: 0.7,
            metadata: {
                model: 'brain-js',
                processingTime,
                fromCache: false,
                confidence: 0.7,
                qualityScore: 0.7
            }
        };
    }
    async generateOpenAIEmbedding(text) {
        const startTime = Date.now();
        const response = await this.openaiClient.embeddings.create({
            model: 'text-embedding-3-small',
            input: text,
            encoding_format: 'float'
        });
        const embedding = response.data[0].embedding;
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('openai', processingTime, true);
        return {
            success: true,
            embedding,
            confidence: 0.95,
            model: 'openai',
            processingTime,
            fromCache: false,
            qualityScore: 0.95,
            metadata: {
                model: 'openai',
                processingTime,
                fromCache: false,
                confidence: 0.95,
                qualityScore: 0.95
            }
        };
    }
    generateBasicEmbedding(text) {
        // Simple text-based features as embedding fallback
        const features = this.textToFeatures(text);
        // Pad or truncate to standard embedding size (384 dimensions)
        const targetSize = 384;
        const embedding = new Array(targetSize).fill(0);
        for (let i = 0; i < Math.min(features.length, targetSize); i++) {
            embedding[i] = features[i];
        }
        return embedding;
    }
    textToFeatures(text) {
        const features = [];
        // Basic text statistics
        features.push(text.length / 1000); // Normalized length
        features.push(text.split(' ').length / 100); // Normalized word count
        features.push(text.split('.').length / 10); // Normalized sentence count
        // Character frequency features (first 26 letters)
        const charCounts = new Array(26).fill(0);
        const normalizedText = text.toLowerCase();
        for (const char of normalizedText) {
            const charCode = char.charCodeAt(0);
            if (charCode >= 97 && charCode <= 122) {
                charCounts[charCode - 97]++;
            }
        }
        // Normalize character counts
        const totalChars = text.length;
        for (let i = 0; i < 26; i++) {
            features.push(charCounts[i] / totalChars);
        }
        return features;
    }
    generateCacheKey(request) {
        const content = `${request.text}${request.context || ''}`;
        return `${request.qualityLevel || 'standard'}:${this.hashString(content)}`;
    }
    getCachedEmbedding(cacheKey) {
        const entry = this.embeddingCache.get(cacheKey);
        if (!entry) {
            this.cacheStats.misses++;
            return null;
        }
        // Check TTL
        if (Date.now() - entry.timestamp > this.config.cache.ttlMs) {
            this.embeddingCache.delete(cacheKey);
            this.cacheStats.evictions++;
            this.cacheStats.misses++;
            return null;
        }
        // Update access statistics
        entry.accessCount++;
        entry.lastAccessTime = Date.now();
        this.cacheStats.hits++;
        return entry;
    }
    cacheEmbedding(cacheKey, entry) {
        // Check cache size and evict if necessary
        if (this.embeddingCache.size >= this.config.cache.maxSize) {
            this.performCacheEviction();
        }
        this.embeddingCache.set(cacheKey, entry);
    }
    performCacheEviction() {
        if (!this.config.cache.performanceBasedEviction) {
            // Simple LRU eviction
            const oldestKey = Array.from(this.embeddingCache.entries())
                .sort(([, a], [, b]) => a.lastAccessTime - b.lastAccessTime)[0]?.[0];
            if (oldestKey) {
                this.embeddingCache.delete(oldestKey);
                this.cacheStats.evictions++;
            }
            return;
        }
        // Performance-based eviction: remove entries with lowest performance scores
        const entries = Array.from(this.embeddingCache.entries());
        entries.sort(([, a], [, b]) => a.performanceScore - b.performanceScore);
        const toEvict = Math.floor(this.config.cache.maxSize * 0.1); // Evict 10%
        for (let i = 0; i < toEvict && i < entries.length; i++) {
            this.embeddingCache.delete(entries[i][0]);
            this.cacheStats.evictions++;
        }
    }
    calculatePerformanceScore(result, processingTime) {
        // Combine quality score, confidence, and speed for overall performance score
        const speedScore = Math.max(0, 1 - processingTime / 5000); // Normalize to 5 seconds max
        const qualityScore = result.qualityScore;
        const confidenceScore = result.confidence;
        return (speedScore * 0.3 + qualityScore * 0.5 + confidenceScore * 0.2);
    }
    updateModelMetrics(model, processingTime, success) {
        const status = this.modelStatus.get(model);
        if (!status)
            return;
        // Update success rate
        const totalRequests = this.performanceMetrics.modelUsageCount.get(model) || 0;
        const previousSuccesses = totalRequests * status.successRate;
        const newSuccesses = previousSuccesses + (success ? 1 : 0);
        const newTotal = totalRequests + 1;
        status.successRate = newSuccesses / newTotal;
        status.averageLatency = (status.averageLatency * totalRequests + processingTime) / newTotal;
        status.lastUsed = Date.now();
        this.performanceMetrics.modelUsageCount.set(model, newTotal);
    }
    updatePerformanceMetrics(model, processingTime, qualityScore) {
        // Update average latency
        const totalEmbeddings = this.performanceMetrics.totalEmbeddings;
        this.performanceMetrics.averageLatency =
            (this.performanceMetrics.averageLatency * (totalEmbeddings - 1) + processingTime) / totalEmbeddings;
        // Update quality distribution
        const qualityBucket = Math.floor(qualityScore * 10) / 10; // Round to nearest 0.1
        const currentCount = this.performanceMetrics.qualityDistribution.get(String(qualityBucket)) || 0;
        this.performanceMetrics.qualityDistribution.set(String(qualityBucket), currentCount + 1);
    }
    performPerformanceOptimization() {
        // Optimize cache based on performance data
        if (this.config.cache.performanceBasedEviction) {
            const cacheEfficiency = this.calculateCacheEfficiency();
            if (cacheEfficiency < 0.5) {
                this.logger.info('📊 Cache efficiency low, performing optimization...');
                this.performCacheEviction();
            }
        }
        // Log performance metrics
        const stats = this.getCoordinatorStats();
        this.logger.debug('📊 Performance metrics:', {
            averageLatency: stats.performance.averageLatency,
            cacheEfficiency: stats.systemHealth.cacheEfficiency,
            averageQuality: stats.systemHealth.averageQuality
        });
    }
    isPrimaryModelReady() {
        const transformersStatus = this.modelStatus.get('transformers');
        return transformersStatus?.loaded === true;
    }
    getLoadedModelsCount() {
        return Array.from(this.modelStatus.values()).filter(status => status.loaded).length;
    }
    getAvailableFallbacksCount() {
        return Array.from(this.modelStatus.values()).filter(status => status.loaded && status.name !== 'transformers').length;
    }
    calculateCacheEfficiency() {
        if (this.cacheStats.totalRequests === 0)
            return 0;
        return this.cacheStats.hits / this.cacheStats.totalRequests;
    }
    calculateAverageQuality() {
        if (this.performanceMetrics.qualityDistribution.size === 0)
            return 0;
        let totalWeightedQuality = 0;
        let totalCount = 0;
        for (const [qualityStr, count] of this.performanceMetrics.qualityDistribution) {
            const quality = parseFloat(qualityStr);
            totalWeightedQuality += quality * count;
            totalCount += count;
        }
        return totalCount > 0 ? totalWeightedQuality / totalCount : 0;
    }
    hashString(str) {
        let hash = 0;
        for (let i = 0; i < str.length; i++) {
            const char = str.charCodeAt(i);
            hash = ((hash << 5) - hash) + char;
            hash = hash & hash; // Convert to 32-bit integer
        }
        return hash.toString(36);
    }
    // =============================================================================
    // CLASSIFICATION PHASE IMPLEMENTATION METHODS
    // =============================================================================
    generateClassificationCacheKey(request) {
        const content = `${request.text}${request.taskType}${request.categories?.join(',') || ''}${request.context || ''}`;
        return `classify:${request.qualityLevel || 'standard'}:${this.hashString(content)}`;
    }
    getCachedClassification(cacheKey) {
        return this.getCachedEmbedding(cacheKey); // Reuse same cache structure
    }
    cacheClassification(cacheKey, result, processingTime) {
        const entry = {
            embedding: [], // Not used for classification
            confidence: result.classification.confidence,
            model: result.model,
            timestamp: Date.now(),
            accessCount: 1,
            lastAccessTime: Date.now(),
            performanceScore: this.calculatePerformanceScore(result, processingTime),
            classificationData: result.classification // Store classification-specific data
        };
        this.cacheEmbedding(cacheKey, entry); // Reuse same cache mechanism
    }
    async generateNewClassification(request, span) {
        const fallbacksUsed = [];
        // Try premium OpenAI first if requested and available
        if (request.qualityLevel === 'premium' && this.openaiClient) {
            try {
                const result = await this.generateOpenAIClassification(request);
                span.setAttributes({ 'neural.classification.primary_method': 'openai' });
                return { ...result, fallbacksUsed };
            }
            catch (error) {
                fallbacksUsed.push('openai-failed');
                this.logger.debug('OpenAI classification failed, falling back:', error);
            }
        }
        // Try transformers model
        if (this.transformerModel || this.config.loading.lazyLoading) {
            try {
                if (!this.transformerModel && this.config.loading.lazyLoading) {
                    await this.loadTransformersModel();
                }
                if (this.transformerModel) {
                    const result = await this.generateTransformersClassification(request);
                    span.setAttributes({ 'neural.classification.primary_method': 'transformers' });
                    return { ...result, fallbacksUsed };
                }
            }
            catch (error) {
                fallbacksUsed.push('transformers-failed');
                this.logger.debug('Transformers classification failed, falling back:', error);
            }
        }
        // Try Brain.js fallback
        if (this.config.enableFallbacks && (this.brainJsNetwork || this.config.loading.lazyLoading)) {
            try {
                if (!this.brainJsNetwork && this.config.loading.lazyLoading) {
                    await this.loadBrainJsModel();
                }
                if (this.brainJsNetwork) {
                    const result = await this.generateBrainJsClassification(request);
                    fallbacksUsed.push('brain-js');
                    span.setAttributes({ 'neural.classification.primary_method': 'brain-js' });
                    return { ...result, fallbacksUsed };
                }
            }
            catch (error) {
                fallbacksUsed.push('brain-js-failed');
                this.logger.debug('Brain.js classification failed, falling back:', error);
            }
        }
        // Final fallback: basic classification
        fallbacksUsed.push('basic');
        span.setAttributes({ 'neural.classification.primary_method': 'basic' });
        return {
            classification: this.generateBasicClassification(request),
            model: 'basic',
            qualityScore: 0.4,
            fallbacksUsed
        };
    }
    async generateTransformersClassification(request) {
        // For transformers-based classification, we'll use sentiment analysis as example
        // In a real implementation, you'd use task-specific models
        const startTime = Date.now();
        let classification;
        switch (request.taskType) {
            case 'sentiment':
                classification = await this.classifySentiment(request.text);
                break;
            case 'intent':
                classification = await this.classifyIntent(request.text, request.categories);
                break;
            case 'category':
                classification = await this.classifyCategory(request.text, request.categories);
                break;
            case 'toxicity':
                classification = await this.classifyToxicity(request.text);
                break;
            case 'language':
                classification = await this.classifyLanguage(request.text);
                break;
            default:
                classification = await this.classifyCustom(request.text, request.categories);
        }
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('transformers', processingTime, true);
        return {
            classification,
            model: 'transformers',
            qualityScore: 0.9
        };
    }
    async generateBrainJsClassification(request) {
        const startTime = Date.now();
        // Convert text to features for brain.js processing
        const features = this.textToFeatures(request.text);
        const output = this.brainJsNetwork.run(features);
        // Convert brain.js output to classification result
        const classification = this.convertToClassification(output, request);
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('brain-js', processingTime, true);
        return {
            classification,
            model: 'brain-js',
            qualityScore: 0.7
        };
    }
    async generateOpenAIClassification(request) {
        const startTime = Date.now();
        // Use OpenAI for high-quality classification
        const response = await this.openaiClient.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [{
                    role: 'system',
                    content: this.buildClassificationSystemPrompt(request)
                }, {
                    role: 'user',
                    content: request.text
                }],
            temperature: 0.1,
            max_tokens: 200
        });
        const classification = this.parseOpenAIClassificationResponse(response.choices[0].message.content, request);
        const processingTime = Date.now() - startTime;
        this.updateModelMetrics('openai', processingTime, true);
        return {
            classification,
            model: 'openai',
            qualityScore: 0.95
        };
    }
    generateBasicClassification(request) {
        // Basic rule-based classification as final fallback
        switch (request.taskType) {
            case 'sentiment':
                return this.basicSentimentAnalysis(request.text);
            case 'language':
                return this.basicLanguageDetection(request.text);
            case 'toxicity':
                return this.basicToxicityDetection(request.text);
            default:
                return {
                    label: 'unknown',
                    confidence: 0.3,
                    scores: { unknown: 0.3, other: 0.7 }
                };
        }
    }
    // =============================================================================
    // GENERATION PHASE IMPLEMENTATION METHODS
    // =============================================================================
    generateGenerationCacheKey(request) {
        const content = `${request.prompt}${request.taskType}${request.temperature || 0.7}${request.maxLength || 1000}${request.context || ''}`;
        return `generate:${request.qualityLevel || 'standard'}:${this.hashString(content)}`;
    }
    getCachedGeneration(cacheKey) {
        return this.getCachedEmbedding(cacheKey); // Reuse same cache structure
    }
    cacheGeneration(cacheKey, result, processingTime) {
        const entry = {
            embedding: [], // Not used for generation
            confidence: 0.8, // Default confidence for generation
            model: result.model,
            timestamp: Date.now(),
            accessCount: 1,
            lastAccessTime: Date.now(),
            performanceScore: this.calculatePerformanceScore(result, processingTime),
            generationData: result.generated // Store generation-specific data
        };
        this.cacheEmbedding(cacheKey, entry); // Reuse same cache mechanism
    }
    async generateNewText(request, span) {
        const fallbacksUsed = [];
        // Try premium OpenAI first if requested and available
        if (request.qualityLevel === 'premium' && this.openaiClient) {
            try {
                const result = await this.generateOpenAIText(request);
                span.setAttributes({ 'neural.generation.primary_method': 'openai' });
                return { ...result, fallbacksUsed };
            }
            catch (error) {
                fallbacksUsed.push('openai-failed');
                this.logger.debug('OpenAI generation failed, falling back:', error);
            }
        }
        // Try transformers model
        if (this.transformerModel || this.config.loading.lazyLoading) {
            try {
                const result = await this.generateTransformersText(request);
                span.setAttributes({ 'neural.generation.primary_method': 'transformers' });
                return { ...result, fallbacksUsed };
            }
            catch (error) {
                fallbacksUsed.push('transformers-failed');
                this.logger.debug('Transformers generation failed, falling back:', error);
            }
        }
        // Try Brain.js fallback
        if (this.config.enableFallbacks) {
            try {
                const result = await this.generateBrainJsText(request);
                fallbacksUsed.push('brain-js');
                span.setAttributes({ 'neural.generation.primary_method': 'brain-js' });
                return { ...result, fallbacksUsed };
            }
            catch (error) {
                fallbacksUsed.push('brain-js-failed');
                this.logger.debug('Brain.js generation failed, falling back:', error);
            }
        }
        // Final fallback: basic generation
        fallbacksUsed.push('basic');
        span.setAttributes({ 'neural.generation.primary_method': 'basic' });
        return {
            generated: this.generateBasicText(request),
            model: 'basic',
            qualityScore: 0.3,
            fallbacksUsed
        };
    }
    // =============================================================================
    // VISION PHASE IMPLEMENTATION METHODS
    // =============================================================================
    generateVisionCacheKey(request, imageInfo) {
        const imageHash = this.hashImageData(request.image);
        // Use imageInfo for enhanced cache key generation
        const imageMetadata = imageInfo ? {
            format: imageInfo.format || 'unknown',
            size: imageInfo.size || 0,
            valid: imageInfo.valid || false
        } : { format: 'unknown', size: 0, valid: true };
        // Include image metadata in cache key for better cache differentiation
        const content = `${imageHash}${request.taskType}${request.prompt || ''}${request.context || ''}${imageMetadata.format}_${imageMetadata.size}_${imageMetadata.valid ? 'valid' : 'invalid'}`;
        const cacheKey = `vision:${request.qualityLevel || 'standard'}:${this.hashString(content)}`;
        // Log cache key generation with image metadata
        this.logger.debug('Vision cache key generated with image info', {
            imageFormat: imageMetadata.format,
            imageSize: imageMetadata.size,
            imageValid: imageMetadata.valid,
            taskType: request.taskType,
            qualityLevel: request.qualityLevel || 'standard',
            cacheKeyLength: cacheKey.length
        });
        return cacheKey;
    }
    analyzeImageInput(image) {
        try {
            let format = 'unknown';
            let size = 0;
            let valid = false;
            if (typeof image === 'string') {
                // Base64 string
                if (image.startsWith('data:image/')) {
                    format = image.split(';')[0].split('/')[1];
                    size = Buffer.from(image.split(',')[1], 'base64').length;
                    valid = true;
                }
                else if (image.length > 0) {
                    size = Buffer.from(image, 'base64').length;
                    valid = true;
                }
            }
            else if (Buffer.isBuffer(image)) {
                size = image.length;
                valid = true;
                // Detect format from buffer header
                if (image.length > 4) {
                    const header = image.subarray(0, 4);
                    if (header.toString('hex').startsWith('ffd8ff'))
                        format = 'jpeg';
                    else if (header.toString('hex').startsWith('89504e47'))
                        format = 'png';
                    else if (header.toString('hex').startsWith('47494638'))
                        format = 'gif';
                }
            }
            else if (image instanceof ArrayBuffer) {
                size = image.byteLength;
                valid = image.byteLength > 0;
            }
            return { valid, format, size };
        }
        catch (error) {
            // Use error information for better image validation debugging
            const errorMessage = error instanceof Error ? error.message : String(error);
            this.logger.debug('Image input analysis failed', {
                errorType: error instanceof Error ? error.constructor.name : 'UnknownError',
                errorMessage,
                fallbackResponse: { valid: false, format: 'unknown', size: 0 }
            });
            return { valid: false, format: 'unknown', size: 0 };
        }
    }
    hashImageData(image) {
        try {
            let data;
            if (typeof image === 'string') {
                data = image.length > 1000 ? image.substring(0, 1000) : image;
            }
            else if (Buffer.isBuffer(image)) {
                data = image.subarray(0, 1000).toString('hex');
            }
            else {
                data = Buffer.from(image).subarray(0, 1000).toString('hex');
            }
            return this.hashString(data);
        }
        catch (error) {
            // Use error information for better error handling and logging
            const errorMessage = error instanceof Error ? error.message : String(error);
            this.logger.warn('Image hash generation failed', {
                errorType: error instanceof Error ? error.constructor.name : 'UnknownError',
                errorMessage,
                imageType: typeof error === 'object' && error ? 'complex_object' : typeof error
            });
            // Return error-specific hash for better debugging
            return `invalid_image_${errorMessage.replace(/[^\dA-Za-z]/g, '_').substring(0, 20)}`;
        }
    }
    // =============================================================================
    // NEURAL TASK PHASE IMPLEMENTATION METHODS
    // =============================================================================
    validateNeuralTaskRequest(request) {
        if (!request.input) {
            return 'Input data is required for neural tasks';
        }
        switch (request.taskType) {
            case 'question_answering':
                if (!request.input.question || !request.input.context) {
                    return 'Question answering requires both question and context';
                }
                break;
            case 'similarity':
                if (!request.input.texts || request.input.texts.length < 2) {
                    return 'Similarity task requires at least 2 texts';
                }
                break;
            case 'clustering':
                if (!request.input.data || !Array.isArray(request.input.data) || request.input.data.length < 2) {
                    return 'Clustering task requires an array of data with at least 2 items';
                }
                break;
        }
        return null;
    }
    generateNeuralTaskCacheKey(request) {
        const inputStr = JSON.stringify(request.input).substring(0, 1000); // Limit size
        const paramsStr = JSON.stringify(request.parameters || {});
        const content = `${request.taskType}${inputStr}${paramsStr}`;
        return `task:${this.hashString(content)}`;
    }
    // =============================================================================
    // HELPER METHODS FOR ERROR HANDLING AND FALLBACKS
    // =============================================================================
    createClassificationErrorResult(error, startTime, request) {
        return {
            success: false,
            classification: { label: 'error', confidence: 0, scores: {} },
            model: 'basic',
            processingTime: Date.now() - startTime,
            fromCache: false,
            qualityScore: 0,
            error,
            metadata: {
                taskType: request.taskType,
                model: 'basic',
                processingTime: Date.now() - startTime,
                fromCache: false,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                context: request.context,
                qualityScore: 0
            }
        };
    }
    createGenerationErrorResult(error, startTime, request) {
        return {
            success: false,
            generated: {
                text: '',
                finishReason: 'error',
                tokensGenerated: 0
            },
            model: 'basic',
            processingTime: Date.now() - startTime,
            fromCache: false,
            qualityScore: 0,
            error,
            metadata: {
                taskType: request.taskType,
                model: 'basic',
                processingTime: Date.now() - startTime,
                fromCache: false,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                context: request.context,
                parameters: {},
                qualityScore: 0
            }
        };
    }
    createVisionErrorResult(error, startTime, request) {
        return {
            success: false,
            vision: {},
            model: 'basic',
            processingTime: Date.now() - startTime,
            fromCache: false,
            qualityScore: 0,
            error,
            metadata: {
                taskType: request.taskType,
                model: 'basic',
                processingTime: Date.now() - startTime,
                fromCache: false,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                context: request.context,
                qualityScore: 0
            }
        };
    }
    createNeuralTaskErrorResult(error, startTime, request) {
        return {
            success: false,
            result: {},
            model: 'basic',
            processingTime: Date.now() - startTime,
            fromCache: false,
            qualityScore: 0,
            error,
            metadata: {
                taskType: request.taskType,
                model: 'basic',
                processingTime: Date.now() - startTime,
                fromCache: false,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                qualityScore: 0
            }
        };
    }
    // =============================================================================
    // BASIC CLASSIFICATION IMPLEMENTATIONS
    // =============================================================================
    basicSentimentAnalysis(text) {
        const positiveWords = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'awesome', 'love', 'like', 'happy'];
        const negativeWords = ['bad', 'terrible', 'awful', 'horrible', 'hate', 'dislike', 'sad', 'angry', 'disappointed', 'frustrated'];
        const words = text.toLowerCase().split(/\W+/);
        let positiveScore = 0;
        let negativeScore = 0;
        words.forEach(word => {
            if (positiveWords.includes(word))
                positiveScore++;
            if (negativeWords.includes(word))
                negativeScore++;
        });
        const total = positiveScore + negativeScore;
        if (total === 0) {
            return { label: 'neutral', confidence: 0.5, scores: { positive: 0.33, negative: 0.33, neutral: 0.34 } };
        }
        return positiveScore > negativeScore ? { label: 'positive', confidence: positiveScore / total, scores: { positive: positiveScore / total, negative: negativeScore / total, neutral: 0 } } : { label: 'negative', confidence: negativeScore / total, scores: { positive: positiveScore / total, negative: negativeScore / total, neutral: 0 } };
    }
    basicLanguageDetection(text) {
        // Very basic language detection based on character patterns
        const englishPattern = /^[\s!"',.:;?A-Za-z-]+$/;
        if (englishPattern.test(text)) {
            return { label: 'english', confidence: 0.7, scores: { english: 0.7, other: 0.3 } };
        }
        return { label: 'unknown', confidence: 0.3, scores: { unknown: 0.3, other: 0.7 } };
    }
    basicToxicityDetection(text) {
        const toxicWords = ['hate', 'kill', 'die', 'stupid', 'idiot', 'moron', 'dumb'];
        const words = text.toLowerCase().split(/\W+/);
        const toxicCount = words.filter(word => toxicWords.includes(word)).length;
        const toxicityScore = Math.min(toxicCount / words.length * 5, 1); // Scale up small counts
        return {
            label: toxicityScore > 0.5 ? 'toxic' : 'non_toxic',
            confidence: toxicityScore > 0.5 ? toxicityScore : 1 - toxicityScore,
            scores: { toxic: toxicityScore, non_toxic: 1 - toxicityScore }
        };
    }
    // =============================================================================
    // STUB IMPLEMENTATIONS FOR SPECIFIC TASKS
    // =============================================================================
    async classifySentiment(text) {
        return this.basicSentimentAnalysis(text);
    }
    async classifyIntent(text, categories) {
        // Use text analysis for intent classification
        const textLower = text.toLowerCase();
        const textLength = text.length;
        const words = text.split(/\s+/);
        // Define default categories or use provided ones
        const availableCategories = categories && categories.length > 0 ? categories :
            ['question', 'request', 'complaint', 'compliment', 'information', 'action', 'unknown'];
        // Simple intent analysis based on text patterns
        let bestIntent = 'unknown';
        let confidence = 0.3;
        // Question detection
        if (textLower.includes('?') || textLower.startsWith('what') || textLower.startsWith('how') ||
            textLower.startsWith('why') || textLower.startsWith('when') || textLower.startsWith('where')) {
            bestIntent = availableCategories.includes('question') ? 'question' : availableCategories[0];
            confidence = 0.8;
        }
        // Request detection
        else if (textLower.includes('please') || textLower.startsWith('can you') ||
            textLower.startsWith('could you') || textLower.includes('help')) {
            bestIntent = availableCategories.includes('request') ? 'request' : availableCategories[0];
            confidence = 0.7;
        }
        // Complaint detection
        else if (textLower.includes('problem') || textLower.includes('issue') ||
            textLower.includes('wrong') || textLower.includes('broken')) {
            bestIntent = availableCategories.includes('complaint') ? 'complaint' : availableCategories[0];
            confidence = 0.75;
        }
        // Action detection
        else if (textLower.includes('do') || textLower.includes('execute') ||
            textLower.includes('run') || textLower.includes('perform')) {
            bestIntent = availableCategories.includes('action') ? 'action' : availableCategories[0];
            confidence = 0.6;
        }
        // Create scores for all categories
        const scores = {};
        for (const category of availableCategories) {
            scores[category] = category === bestIntent ? confidence : (1 - confidence) / (availableCategories.length - 1);
        }
        this.logger.debug('Intent classified using text analysis', {
            textLength,
            wordCount: words.length,
            providedCategories: categories?.length || 0,
            availableCategories: availableCategories.length,
            bestIntent,
            confidence
        });
        return { label: bestIntent, confidence, scores };
    }
    async classifyCategory(text, categories) {
        // Use text content and provided categories for classification
        const textLower = text.toLowerCase();
        const words = text.split(/\s+/);
        // Define default categories or use provided ones
        const availableCategories = categories && categories.length > 0 ? categories :
            ['technology', 'business', 'science', 'art', 'sports', 'politics', 'general'];
        // Simple category scoring based on keyword matching
        const categoryScores = {};
        for (const category of availableCategories) {
            let score = 0.1; // Base score
            // Define keywords for each category (simplified)
            const keywords = {
                technology: ['computer', 'software', 'AI', 'tech', 'digital', 'code', 'programming'],
                business: ['company', 'revenue', 'profit', 'market', 'sales', 'business', 'corporate'],
                science: ['research', 'experiment', 'data', 'analysis', 'study', 'scientific'],
                art: ['design', 'creative', 'artwork', 'visual', 'aesthetic', 'artistic'],
                sports: ['game', 'team', 'player', 'score', 'match', 'athletic'],
                politics: ['government', 'policy', 'election', 'political', 'vote', 'democracy']
            };
            // Check for keyword matches
            const categoryKeywords = keywords[category.toLowerCase()] || [category.toLowerCase()];
            for (const keyword of categoryKeywords) {
                if (textLower.includes(keyword.toLowerCase())) {
                    score += 0.3;
                }
            }
            categoryScores[category] = Math.min(0.95, score);
        }
        // Find best category
        const bestCategory = Object.keys(categoryScores).reduce((a, b) => categoryScores[a] > categoryScores[b] ? a : b);
        const confidence = categoryScores[bestCategory];
        this.logger.debug('Category classified using text analysis and categories', {
            textLength: text.length,
            wordCount: words.length,
            providedCategories: categories?.length || 0,
            availableCategories: availableCategories.length,
            bestCategory,
            confidence,
            topScores: Object.entries(categoryScores)
                .sort(([, a], [, b]) => b - a)
                .slice(0, 3)
                .map(([cat, score]) => ({ category: cat, score }))
        });
        return { label: bestCategory, confidence, scores: categoryScores };
    }
    async classifyToxicity(text) {
        return this.basicToxicityDetection(text);
    }
    async classifyLanguage(text) {
        return this.basicLanguageDetection(text);
    }
    async classifyCustom(text, categories) {
        // Implement custom classification using text analysis and provided categories
        if (!categories || categories.length === 0) {
            this.logger.warn('Custom classification requires categories', {
                textLength: text.length,
                categoriesProvided: false
            });
            return { label: 'unknown', confidence: 0.2, scores: { unknown: 1.0 } };
        }
        const textLower = text.toLowerCase();
        const words = text.split(/\s+/).map(w => w.toLowerCase());
        // Custom scoring algorithm based on text similarity to categories
        const categoryScores = {};
        for (const category of categories) {
            const categoryWords = category.toLowerCase().split(/\s+/);
            let score = 0;
            // Direct category name matching
            if (textLower.includes(category.toLowerCase())) {
                score += 0.5;
            }
            // Word overlap scoring
            for (const categoryWord of categoryWords) {
                if (words.includes(categoryWord)) {
                    score += 0.3;
                }
                // Partial word matching
                for (const word of words) {
                    if (word.includes(categoryWord) || categoryWord.includes(word)) {
                        score += 0.1;
                    }
                }
            }
            // Text length-based confidence adjustment
            const lengthFactor = Math.min(1, text.length / 100);
            score = score * lengthFactor;
            categoryScores[category] = Math.min(0.95, Math.max(0.05, score));
        }
        // Normalize scores
        const totalScore = Object.values(categoryScores).reduce((sum, score) => sum + score, 0);
        if (totalScore > 0) {
            for (const category of categories) {
                categoryScores[category] = categoryScores[category] / totalScore;
            }
        }
        // Find best match
        const bestCategory = Object.keys(categoryScores).reduce((a, b) => categoryScores[a] > categoryScores[b] ? a : b);
        const confidence = categoryScores[bestCategory];
        this.logger.debug('Custom classification completed using text analysis', {
            textLength: text.length,
            wordCount: words.length,
            categoriesCount: categories.length,
            bestCategory,
            confidence,
            allScores: categoryScores
        });
        return { label: bestCategory, confidence, scores: categoryScores };
    }
    convertToClassification(output, request) {
        // Convert brain.js output to classification format
        const values = Object.values(output);
        const maxIndex = values.indexOf(Math.max(...values));
        const categories = request.categories || ['category1', 'category2', 'category3'];
        return {
            label: categories[maxIndex] || 'unknown',
            confidence: values[maxIndex] || 0.5,
            scores: categories.reduce((acc, cat, idx) => {
                acc[cat] = values[idx] || 0;
                return acc;
            }, {})
        };
    }
    buildClassificationSystemPrompt(request) {
        switch (request.taskType) {
            case 'sentiment':
                return 'Analyze the sentiment of the following text. Respond with: positive, negative, or neutral.';
            case 'intent':
                return 'Classify the intent of the following text.';
            case 'toxicity':
                return 'Determine if the following text is toxic or non-toxic.';
            default:
                return 'Classify the following text according to the specified categories.';
        }
    }
    parseOpenAIClassificationResponse(response, _request) {
        if (!response) {
            return { label: 'unknown', confidence: 0.5, scores: { unknown: 0.5, other: 0.5 } };
        }
        const label = response.toLowerCase().trim();
        return {
            label,
            confidence: 0.9,
            scores: { [label]: 0.9, other: 0.1 }
        };
    }
    // =============================================================================
    // TEXT GENERATION IMPLEMENTATIONS
    // =============================================================================
    async generateTransformersText(request) {
        // Placeholder for transformers text generation
        return {
            generated: {
                text: `Generated response for: ${request.prompt.substring(0, 50)}...`,
                finishReason: 'completed',
                tokensGenerated: 100
            },
            model: 'transformers',
            qualityScore: 0.8
        };
    }
    async generateBrainJsText(request) {
        // Placeholder for brain.js text generation
        return {
            generated: {
                text: `Brain.js generated response for: ${request.prompt.substring(0, 30)}...`,
                finishReason: 'completed',
                tokensGenerated: 50
            },
            model: 'brain-js',
            qualityScore: 0.6
        };
    }
    async generateOpenAIText(request) {
        // Placeholder for OpenAI text generation
        const response = await this.openaiClient.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [{ role: 'user', content: request.prompt }],
            max_tokens: request.maxLength || 1000,
            temperature: request.temperature || 0.7
        });
        return {
            generated: {
                text: response.choices[0].message.content || '',
                finishReason: 'completed',
                tokensGenerated: response.usage?.completion_tokens || 0
            },
            model: 'openai',
            qualityScore: 0.95
        };
    }
    generateBasicText(request) {
        return {
            text: `Basic completion for: ${request.prompt.substring(0, 100)}... [This is a basic fallback response]`,
            finishReason: 'completed',
            tokensGenerated: 20
        };
    }
    // =============================================================================
    // CACHE AND METRIC HELPER IMPLEMENTATIONS
    // =============================================================================
    createClassificationCacheResult(cachedResult, startTime, request, span) {
        const processingTime = Date.now() - startTime;
        this.cacheStats.hits++;
        span.setAttributes({
            'neural.classification.cache_hit': true,
            'neural.classification.model': cachedResult.model
        });
        return {
            success: true,
            classification: cachedResult.classificationData || { label: 'cached', confidence: cachedResult.confidence, scores: {} },
            model: cachedResult.model,
            processingTime,
            fromCache: true,
            qualityScore: cachedResult.performanceScore,
            metadata: {
                taskType: request.taskType,
                model: cachedResult.model,
                processingTime,
                fromCache: true,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                context: request.context,
                qualityScore: cachedResult.performanceScore
            }
        };
    }
    recordClassificationMetrics(result, processingTime, request, fromCache) {
        recordMetric('smart_neural_classification_generated', 1, {
            cache_hit: String(fromCache),
            model: result.model,
            task_type: request.taskType,
            quality_level: request.qualityLevel || 'standard',
            status: 'success'
        });
        recordHistogram('smart_neural_classification_duration_ms', processingTime, {
            model: result.model,
            task_type: request.taskType
        });
    }
    recordClassificationError(error, processingTime, request, span) {
        recordMetric('smart_neural_classification_generated', 1, {
            status: 'error',
            task_type: request.taskType,
            error_type: error instanceof Error ? error.constructor.name : 'unknown'
        });
        span.setAttributes({
            'neural.classification.error': true,
            'neural.classification.error_message': error instanceof Error ? error.message : String(error)
        });
    }
    createClassificationFallbackResult(text, processingTime, request, error) {
        return {
            success: false,
            classification: this.generateBasicClassification(request),
            model: 'basic',
            processingTime,
            fromCache: false,
            qualityScore: 0.3,
            fallbacksUsed: ['basic-fallback'],
            error: error instanceof Error ? error.message : String(error),
            metadata: {
                taskType: request.taskType,
                model: 'basic',
                processingTime,
                fromCache: false,
                priority: request.priority,
                qualityLevel: request.qualityLevel,
                context: request.context,
                qualityScore: 0.3
            }
        };
    }
    // =============================================================================
    // STUB METHODS FOR VISION AND NEURAL TASKS (TO BE FULLY IMPLEMENTED)
    // =============================================================================
    getCachedVision(_cacheKey) { return null; }
    cacheVision(_cacheKey, _result, _processingTime) { }
    async processNewImage(_request, _span, _imageInfo) {
        return { vision: { description: 'Basic image processing' }, model: 'basic', qualityScore: 0.3 };
    }
    createVisionCacheResult(_cached, startTime, request, _span, _imageInfo) {
        return this.createVisionErrorResult('Not implemented', startTime, request);
    }
    recordVisionMetrics(_result, _processingTime, _request, _fromCache) { }
    recordVisionError(_error, _processingTime, _request, _span) { }
    createVisionFallbackResult(request, _processingTime, _error, _imageInfo) {
        return this.createVisionErrorResult('Fallback error', Date.now(), request);
    }
    getCachedNeuralTask(_cacheKey) { return null; }
    cacheNeuralTask(_cacheKey, _result, _processingTime) { }
    async executeNewNeuralTask(_request, _span) {
        return { result: { custom: 'Basic task result' }, model: 'basic', qualityScore: 0.3 };
    }
    createNeuralTaskCacheResult(cached, startTime, request, span) {
        const processingTime = Date.now() - startTime;
        // Mark span as successful cache hit
        span.setAttributes({
            'neural.cache.hit': true,
            'neural.processing_time': processingTime
        });
        return {
            success: true,
            result: cached.result,
            model: (cached.model || 'basic'),
            processingTime,
            fromCache: true,
            qualityScore: cached.qualityScore || 0.8,
            metadata: {
                model: cached.model || 'cached',
                processingTime,
                fromCache: true,
                taskType: request.taskType,
                qualityLevel: request.qualityLevel,
                qualityScore: cached.qualityScore || 0.8
            }
        };
    }
    recordNeuralTaskMetrics(result, processingTime, request, fromCache) {
        this.performanceMetrics.totalEmbeddings++;
        if (!result.success) {
            this.performanceMetrics.failedEmbeddings++;
        }
        // Update latency tracking
        this.performanceMetrics.minLatency = Math.min(this.performanceMetrics.minLatency, processingTime);
        this.performanceMetrics.maxLatency = Math.max(this.performanceMetrics.maxLatency, processingTime);
        // Calculate rolling average latency
        const currentTotal = this.performanceMetrics.averageLatency * (this.performanceMetrics.totalEmbeddings - 1);
        this.performanceMetrics.averageLatency = (currentTotal + processingTime) / this.performanceMetrics.totalEmbeddings;
        // Update cache metrics if cached
        if (fromCache) {
            this.cacheStats.hits++;
        }
        else {
            this.cacheStats.misses++;
        }
        this.cacheStats.totalRequests++;
        this.logger.debug(`Neural task metrics recorded: ${processingTime}ms, fromCache: ${fromCache}`);
    }
    recordNeuralTaskError(error, processingTime, request, span) {
        this.performanceMetrics.failedEmbeddings++;
        this.performanceMetrics.totalEmbeddings++;
        // Record error in telemetry span
        span.recordException(error);
        span.setStatus({ code: 2, message: error.message });
        this.logger.error(`Neural task error: ${error.message} (${processingTime}ms)`, {
            taskType: request.taskType,
            error: error.message
        });
    }
    createNeuralTaskFallbackResult(request, processingTime, _error) {
        // Attempt basic fallback implementation
        try {
            let fallbackResult;
            switch (request.taskType) {
                case 'question_answering':
                    fallbackResult = { answer: 'Unable to process question at this time', confidence: 0.1 };
                    break;
                case 'similarity':
                    fallbackResult = { similarity: 0.5, method: 'basic' };
                    break;
                case 'clustering':
                    fallbackResult = { clusters: [], method: 'basic' };
                    break;
                default:
                    fallbackResult = { result: 'Basic fallback result', method: 'basic' };
            }
            return {
                success: true,
                result: fallbackResult,
                model: 'basic',
                processingTime,
                fromCache: false,
                qualityScore: 0.1,
                metadata: {
                    model: 'fallback-basic',
                    processingTime,
                    fromCache: false,
                    taskType: request.taskType,
                    qualityLevel: 'basic',
                    qualityScore: 0.1
                }
            };
        }
        catch (fallbackError) {
            return this.createNeuralTaskErrorResult(`Fallback failed: ${String(fallbackError)}`, Date.now(), request);
        }
    }
    createGenerationCacheResult(cached, startTime, request, span) {
        const processingTime = Date.now() - startTime;
        // Mark span as successful cache hit
        span.setAttributes({
            'neural.cache.hit': true,
            'neural.processing_time': processingTime
        });
        return {
            success: true,
            generated: cached.generated,
            model: (cached.model || 'basic'),
            processingTime,
            fromCache: true,
            qualityScore: cached.qualityScore || 0.8,
            metadata: {
                model: cached.model || 'cached',
                processingTime,
                fromCache: true,
                taskType: request.taskType || 'generation',
                qualityLevel: request.qualityLevel,
                parameters: {
                    maxLength: 100,
                    temperature: 0.7
                },
                qualityScore: cached.qualityScore || 0.8
            }
        };
    }
    recordGenerationMetrics(result, processingTime, request, fromCache) {
        this.performanceMetrics.totalEmbeddings++;
        if (!result.success) {
            this.performanceMetrics.failedEmbeddings++;
        }
        // Update latency tracking
        this.performanceMetrics.minLatency = Math.min(this.performanceMetrics.minLatency, processingTime);
        this.performanceMetrics.maxLatency = Math.max(this.performanceMetrics.maxLatency, processingTime);
        // Calculate rolling average latency
        const currentTotal = this.performanceMetrics.averageLatency * (this.performanceMetrics.totalEmbeddings - 1);
        this.performanceMetrics.averageLatency = (currentTotal + processingTime) / this.performanceMetrics.totalEmbeddings;
        // Update cache metrics if cached
        if (fromCache) {
            this.cacheStats.hits++;
        }
        else {
            this.cacheStats.misses++;
        }
        this.cacheStats.totalRequests++;
        this.logger.debug(`Generation metrics recorded: ${processingTime}ms, fromCache: ${fromCache}`);
    }
    recordGenerationError(error, processingTime, request, span) {
        this.performanceMetrics.failedEmbeddings++;
        this.performanceMetrics.totalEmbeddings++;
        // Record error in telemetry span
        span.recordException(error);
        span.setStatus({ code: 2, message: error.message });
        this.logger.error(`Generation error: ${error.message} (${processingTime}ms)`, {
            prompt: request.prompt.substring(0, 100),
            error: error.message
        });
    }
    createGenerationFallbackResult(prompt, processingTime, request, _error) {
        // Attempt basic text completion fallback
        try {
            const generatedText = prompt.length > 50 ?
                `${prompt.substring(0, 47)}...` :
                `${prompt} [Generation unavailable]`;
            return {
                success: true,
                generated: {
                    text: generatedText,
                    finishReason: 'error',
                    tokensGenerated: generatedText.split(' ').length
                },
                model: 'basic',
                processingTime,
                fromCache: false,
                qualityScore: 0.1,
                metadata: {
                    model: 'fallback-basic',
                    processingTime,
                    fromCache: false,
                    taskType: 'generation',
                    qualityLevel: 'basic',
                    parameters: {
                        maxLength: 100,
                        temperature: 0.7
                    },
                    qualityScore: 0.1
                }
            };
        }
        catch (fallbackError) {
            return this.createGenerationErrorResult(`Fallback failed: ${String(fallbackError)}`, Date.now(), request);
        }
    }
}
export default SmartNeuralCoordinator;
//# sourceMappingURL=smart-neural-coordinator.js.map