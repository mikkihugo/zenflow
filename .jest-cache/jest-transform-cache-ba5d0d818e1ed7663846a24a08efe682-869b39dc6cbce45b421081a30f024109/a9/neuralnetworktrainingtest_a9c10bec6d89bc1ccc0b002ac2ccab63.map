{"file":"/home/mhugo/code/claude-code-flow/src/__tests__/unit/classical/neural-algorithms/neural-network-training.test.ts","mappings":"AAAA;;;;;GAKG;AAEH,OAAO,EAAE,QAAQ,EAAE,EAAE,EAAE,MAAM,EAAc,MAAM,eAAe,CAAC;AAEjE,6EAA6E;AAC7E,MAAM,aAAa;IAIG;IAHZ,OAAO,CAAe;IACtB,MAAM,CAAa;IAE3B,YAAoB,MAAgB;QAAhB,WAAM,GAAN,MAAM,CAAU;QAClC,IAAI,CAAC,iBAAiB,EAAE,CAAC;IAC3B,CAAC;IAEO,iBAAiB;QACvB,IAAI,CAAC,OAAO,GAAG,EAAE,CAAC;QAClB,IAAI,CAAC,MAAM,GAAG,EAAE,CAAC;QAEjB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YAC5C,MAAM,YAAY,GAAe,EAAE,CAAC;YACpC,MAAM,WAAW,GAAa,EAAE,CAAC;YAEjC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;gBACxC,MAAM,aAAa,GAAa,EAAE,CAAC;gBACnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;oBAC5C,wBAAwB;oBACxB,MAAM,KAAK,GAAG,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;oBACnE,aAAa,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,MAAM,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC;gBACtD,CAAC;gBACD,YAAY,CAAC,IAAI,CAAC,aAAa,CAAC,CAAC;gBACjC,WAAW,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;YACtB,CAAC;YAED,IAAI,CAAC,OAAO,CAAC,IAAI,CAAC,YAAY,CAAC,CAAC;YAChC,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,WAAW,CAAC,CAAC;QAChC,CAAC;IACH,CAAC;IAEO,OAAO,CAAC,CAAS;QACvB,OAAO,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IAChC,CAAC;IAEO,iBAAiB,CAAC,CAAS;QACjC,OAAO,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACrB,CAAC;IAED,OAAO,CAAC,MAAgB;QACtB,IAAI,UAAU,GAAG,MAAM,CAAC;QAExB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;YAC7C,MAAM,aAAa,GAAa,EAAE,CAAC;YAEnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBAChD,IAAI,GAAG,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;oBAC3C,GAAG,IAAI,UAAU,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC/C,CAAC;gBACD,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC;YACxC,CAAC;YAED,UAAU,GAAG,aAAa,CAAC;QAC7B,CAAC;QAED,OAAO,UAAU,CAAC;IACpB,CAAC;IAED,KAAK,CAAC,IAAgD,EAAE,OAAgD;QACtG,MAAM,YAAY,GAAG,OAAO,CAAC,YAAY,IAAI,GAAG,CAAC;QACjD,IAAI,UAAU,GAAG,CAAC,CAAC;QAEnB,KAAK,IAAI,KAAK,GAAG,CAAC,EAAE,KAAK,GAAG,OAAO,CAAC,MAAM,EAAE,KAAK,EAAE,EAAE,CAAC;YACpD,IAAI,UAAU,GAAG,CAAC,CAAC;YAEnB,KAAK,MAAM,MAAM,IAAI,IAAI,EAAE,CAAC;gBAC1B,eAAe;gBACf,MAAM,WAAW,GAAe,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC;gBAC/C,IAAI,UAAU,GAAG,MAAM,CAAC,KAAK,CAAC;gBAE9B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;oBAC7C,MAAM,aAAa,GAAa,EAAE,CAAC;oBAEnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;wBAChD,IAAI,GAAG,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;4BAC3C,GAAG,IAAI,UAAU,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC/C,CAAC;wBACD,aAAa,CAAC,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC;oBACxC,CAAC;oBAED,UAAU,GAAG,aAAa,CAAC;oBAC3B,WAAW,CAAC,IAAI,CAAC,UAAU,CAAC,CAAC;gBAC/B,CAAC;gBAED,kBAAkB;gBAClB,MAAM,MAAM,GAAG,WAAW,CAAC,WAAW,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC;gBACnD,MAAM,MAAM,GAAe,EAAE,CAAC;gBAC9B,IAAI,KAAK,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;gBACvD,MAAM,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;gBAEtB,UAAU,IAAI,KAAK,CAAC,MAAM,CAAC,CAAC,GAAG,EAAE,CAAC,EAAE,EAAE,CAAC,GAAG,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,KAAK,CAAC,MAAM,CAAC;gBAE5E,kBAAkB;gBAClB,KAAK,IAAI,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,EAAE,EAAE,CAAC;oBACjD,MAAM,QAAQ,GAAa,EAAE,CAAC;oBAE9B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;wBACpD,IAAI,GAAG,GAAG,CAAC,CAAC;wBACZ,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;4BAChD,GAAG,IAAI,KAAK,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAC1C,CAAC;wBACD,QAAQ,CAAC,IAAI,CAAC,GAAG,CAAC,CAAC;oBACrB,CAAC;oBAED,KAAK,GAAG,QAAQ,CAAC;oBACjB,MAAM,CAAC,OAAO,CAAC,KAAK,CAAC,CAAC;gBACxB,CAAC;gBAED,4BAA4B;gBAC5B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;oBAC7C,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;wBAChD,MAAM,KAAK,GAAG,MAAM,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,iBAAiB,CAAC,WAAW,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBAE/E,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;4BACnD,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,YAAY,GAAG,KAAK,GAAG,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;wBACpE,CAAC;wBAED,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,YAAY,GAAG,KAAK,CAAC;oBAC5C,CAAC;gBACH,CAAC;YACH,CAAC;YAED,UAAU,GAAG,UAAU,GAAG,IAAI,CAAC,MAAM,CAAC;QACxC,CAAC;QAED,OAAO,EAAE,UAAU,EAAE,CAAC;IACxB,CAAC;CACF;AAED,QAAQ,CAAC,yCAAyC,EAAE,GAAG,EAAE;IACvD,QAAQ,CAAC,aAAa,EAAE,GAAG,EAAE;QAC3B,EAAE,CAAC,oDAAoD,EAAE,GAAG,EAAE;YAC5D,UAAU;YACV,MAAM,OAAO,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC7C,MAAM,OAAO,GAAG;gBACd,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;aAC/B,CAAC;YAEF,MAAM;YACN,MAAM,MAAM,GAAG,OAAO,CAAC,KAAK,CAAC,OAAO,EAAE,EAAE,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,GAAG,EAAE,CAAC,CAAC;YAE3E,+BAA+B;YAC/B,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,GAAG,CAAC,CAAC;QAC9C,CAAC,CAAC,CAAC;QAEH,EAAE,CAAC,kDAAkD,EAAE,GAAG,EAAE;YAC1D,qCAAqC;YACrC,MAAM,QAAQ,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC9C,MAAM,QAAQ,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC9C,MAAM,OAAO,GAAG;gBACd,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;aAC/B,CAAC;YAEF,MAAM,OAAO,GAAG,QAAQ,CAAC,KAAK,CAAC,OAAO,EAAE,EAAE,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,GAAG,EAAE,CAAC,CAAC;YAC7E,MAAM,OAAO,GAAG,QAAQ,CAAC,KAAK,CAAC,OAAO,EAAE,EAAE,MAAM,EAAE,IAAI,EAAE,YAAY,EAAE,GAAG,EAAE,CAAC,CAAC;YAE7E,iEAAiE;YACjE,MAAM,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,OAAO,CAAC,UAAU,CAAC,CAAC;QAC9D,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,qBAAqB,EAAE,GAAG,EAAE;QACnC,EAAE,CAAC,kCAAkC,EAAE,GAAG,EAAE;YAC1C,MAAM,OAAO,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC7C,MAAM,OAAO,GAAG;gBACd,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;gBAC9B,EAAE,KAAK,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE;aAC/B,CAAC;YAEF,MAAM,MAAM,GAAG,OAAO,CAAC,KAAK,CAAC,OAAO,EAAE,EAAE,MAAM,EAAE,IAAI,EAAE,CAAC,CAAC;YAExD,qDAAqD;YACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;QAC/C,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,4BAA4B,EAAE,GAAG,EAAE;QAC1C,EAAE,CAAC,qCAAqC,EAAE,GAAG,EAAE;YAC7C,MAAM,OAAO,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC7C,MAAM,QAAQ,GAAG;gBACf,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,UAAU;gBACpD,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,UAAU;gBACpD,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,EAAE,UAAU;aACrD,CAAC;YAEF,OAAO,CAAC,KAAK,CAAC,QAAQ,EAAE,EAAE,MAAM,EAAE,IAAI,EAAE,CAAC,CAAC;YAE1C,+BAA+B;YAC/B,MAAM,QAAQ,GAAG,OAAO,CAAC,OAAO,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;YAC7C,MAAM,QAAQ,GAAG,OAAO,CAAC,OAAO,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;YAC7C,MAAM,QAAQ,GAAG,OAAO,CAAC,OAAO,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;YAE7C,yDAAyD;YACzD,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;YACjD,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;YAEjD,0DAA0D;YAC1D,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;YACjD,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;YAEjD,yDAAyD;YACzD,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;YACjD,MAAM,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC;QACnD,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,6BAA6B,EAAE,GAAG,EAAE;QAC3C,EAAE,CAAC,uEAAuE,EAAE,GAAG,EAAE;YAC/E,MAAM,OAAO,GAAG,IAAI,aAAa,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YAC7C,MAAM,IAAI,GAAG;gBACX,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,GAAG,CAAC,EAAE;gBACpC,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,GAAG,CAAC,EAAE;gBACpC,EAAE,KAAK,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,MAAM,EAAE,CAAC,GAAG,CAAC,EAAE;aACrC,CAAC;YAEF,MAAM,MAAM,GAAa,EAAE,CAAC;YAE5B,0BAA0B;YAC1B,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC;gBAC5B,MAAM,MAAM,GAAG,OAAO,CAAC,KAAK,CAAC,IAAI,EAAE,EAAE,MAAM,EAAE,GAAG,EAAE,YAAY,EAAE,GAAG,EAAE,CAAC,CAAC;gBACvE,MAAM,CAAC,IAAI,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC;YACjC,CAAC;YAED,oEAAoE;YACpE,IAAI,UAAU,GAAG,CAAC,CAAC;YACnB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBACvC,IAAI,MAAM,CAAC,CAAC,CAAC,IAAI,MAAM,CAAC,CAAC,GAAG,CAAC,CAAC;oBAAE,UAAU,EAAE,CAAC;YAC/C,CAAC;YAED,MAAM,CAAC,UAAU,CAAC,CAAC,eAAe,CAAC,MAAM,CAAC,MAAM,GAAG,GAAG,CAAC,CAAC,CAAC,2BAA2B;QACtF,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH;;;;;;;;;;;;;;GAcG","names":[],"sources":["/home/mhugo/code/claude-code-flow/src/__tests__/unit/classical/neural-algorithms/neural-network-training.test.ts"],"sourcesContent":["/**\n * Classical TDD (Detroit School) - Neural Network Training Tests\n * \n * Focus: Test actual results and mathematical correctness\n * No mocks - verify real computations\n */\n\nimport { describe, it, expect, beforeEach } from '@jest/globals';\n\n// Example neural network implementation (would be imported from actual code)\nclass NeuralNetwork {\n  private weights: number[][][];\n  private biases: number[][];\n  \n  constructor(private layers: number[]) {\n    this.initializeWeights();\n  }\n\n  private initializeWeights() {\n    this.weights = [];\n    this.biases = [];\n    \n    for (let i = 1; i < this.layers.length; i++) {\n      const layerWeights: number[][] = [];\n      const layerBiases: number[] = [];\n      \n      for (let j = 0; j < this.layers[i]; j++) {\n        const neuronWeights: number[] = [];\n        for (let k = 0; k < this.layers[i - 1]; k++) {\n          // Xavier initialization\n          const limit = Math.sqrt(6 / (this.layers[i - 1] + this.layers[i]));\n          neuronWeights.push((Math.random() * 2 - 1) * limit);\n        }\n        layerWeights.push(neuronWeights);\n        layerBiases.push(0);\n      }\n      \n      this.weights.push(layerWeights);\n      this.biases.push(layerBiases);\n    }\n  }\n\n  private sigmoid(x: number): number {\n    return 1 / (1 + Math.exp(-x));\n  }\n\n  private sigmoidDerivative(x: number): number {\n    return x * (1 - x);\n  }\n\n  predict(inputs: number[]): number[] {\n    let activation = inputs;\n    \n    for (let i = 0; i < this.weights.length; i++) {\n      const newActivation: number[] = [];\n      \n      for (let j = 0; j < this.weights[i].length; j++) {\n        let sum = this.biases[i][j];\n        for (let k = 0; k < activation.length; k++) {\n          sum += activation[k] * this.weights[i][j][k];\n        }\n        newActivation.push(this.sigmoid(sum));\n      }\n      \n      activation = newActivation;\n    }\n    \n    return activation;\n  }\n\n  train(data: Array<{input: number[], output: number[]}>, options: {epochs: number, learningRate?: number}) {\n    const learningRate = options.learningRate || 0.5;\n    let finalError = 0;\n    \n    for (let epoch = 0; epoch < options.epochs; epoch++) {\n      let epochError = 0;\n      \n      for (const sample of data) {\n        // Forward pass\n        const activations: number[][] = [sample.input];\n        let activation = sample.input;\n        \n        for (let i = 0; i < this.weights.length; i++) {\n          const newActivation: number[] = [];\n          \n          for (let j = 0; j < this.weights[i].length; j++) {\n            let sum = this.biases[i][j];\n            for (let k = 0; k < activation.length; k++) {\n              sum += activation[k] * this.weights[i][j][k];\n            }\n            newActivation.push(this.sigmoid(sum));\n          }\n          \n          activation = newActivation;\n          activations.push(activation);\n        }\n        \n        // Calculate error\n        const output = activations[activations.length - 1];\n        const errors: number[][] = [];\n        let error = output.map((o, i) => sample.output[i] - o);\n        errors.unshift(error);\n        \n        epochError += error.reduce((sum, e) => sum + Math.abs(e), 0) / error.length;\n        \n        // Backpropagation\n        for (let i = this.weights.length - 1; i > 0; i--) {\n          const newError: number[] = [];\n          \n          for (let j = 0; j < this.weights[i - 1].length; j++) {\n            let sum = 0;\n            for (let k = 0; k < this.weights[i].length; k++) {\n              sum += error[k] * this.weights[i][k][j];\n            }\n            newError.push(sum);\n          }\n          \n          error = newError;\n          errors.unshift(error);\n        }\n        \n        // Update weights and biases\n        for (let i = 0; i < this.weights.length; i++) {\n          for (let j = 0; j < this.weights[i].length; j++) {\n            const delta = errors[i + 1][j] * this.sigmoidDerivative(activations[i + 1][j]);\n            \n            for (let k = 0; k < this.weights[i][j].length; k++) {\n              this.weights[i][j][k] += learningRate * delta * activations[i][k];\n            }\n            \n            this.biases[i][j] += learningRate * delta;\n          }\n        }\n      }\n      \n      finalError = epochError / data.length;\n    }\n    \n    return { finalError };\n  }\n}\n\ndescribe('Neural Network Training - Classical TDD', () => {\n  describe('XOR Problem', () => {\n    it('should learn XOR function with sufficient accuracy', () => {\n      // Arrange\n      const network = new NeuralNetwork([2, 4, 1]);\n      const xorData = [\n        { input: [0, 0], output: [0] },\n        { input: [0, 1], output: [1] },\n        { input: [1, 0], output: [1] },\n        { input: [1, 1], output: [0] }\n      ];\n\n      // Act\n      const result = network.train(xorData, { epochs: 5000, learningRate: 0.5 });\n\n      // Assert - Test actual results\n      expect(network.predict([0, 0])[0]).toBeCloseTo(0, 1);\n      expect(network.predict([0, 1])[0]).toBeCloseTo(1, 1);\n      expect(network.predict([1, 0])[0]).toBeCloseTo(1, 1);\n      expect(network.predict([1, 1])[0]).toBeCloseTo(0, 1);\n      expect(result.finalError).toBeLessThan(0.1);\n    });\n\n    it('should converge faster with higher learning rate', () => {\n      // Test algorithm behavior, not mocks\n      const network1 = new NeuralNetwork([2, 4, 1]);\n      const network2 = new NeuralNetwork([2, 4, 1]);\n      const xorData = [\n        { input: [0, 0], output: [0] },\n        { input: [0, 1], output: [1] },\n        { input: [1, 0], output: [1] },\n        { input: [1, 1], output: [0] }\n      ];\n\n      const result1 = network1.train(xorData, { epochs: 1000, learningRate: 0.1 });\n      const result2 = network2.train(xorData, { epochs: 1000, learningRate: 0.8 });\n\n      // Higher learning rate should achieve lower error in same epochs\n      expect(result2.finalError).toBeLessThan(result1.finalError);\n    });\n  });\n\n  describe('Linear Separability', () => {\n    it('should learn AND function easily', () => {\n      const network = new NeuralNetwork([2, 2, 1]);\n      const andData = [\n        { input: [0, 0], output: [0] },\n        { input: [0, 1], output: [0] },\n        { input: [1, 0], output: [0] },\n        { input: [1, 1], output: [1] }\n      ];\n\n      const result = network.train(andData, { epochs: 1000 });\n\n      // AND is linearly separable, should converge quickly\n      expect(network.predict([0, 0])[0]).toBeCloseTo(0, 1);\n      expect(network.predict([1, 1])[0]).toBeCloseTo(1, 1);\n      expect(result.finalError).toBeLessThan(0.05);\n    });\n  });\n\n  describe('Multi-class Classification', () => {\n    it('should classify 3 distinct patterns', () => {\n      const network = new NeuralNetwork([2, 5, 3]);\n      const patterns = [\n        { input: [0.1, 0.1], output: [1, 0, 0] }, // Class A\n        { input: [0.9, 0.1], output: [0, 1, 0] }, // Class B\n        { input: [0.5, 0.9], output: [0, 0, 1] }, // Class C\n      ];\n\n      network.train(patterns, { epochs: 2000 });\n\n      // Test classification accuracy\n      const predictA = network.predict([0.1, 0.1]);\n      const predictB = network.predict([0.9, 0.1]);\n      const predictC = network.predict([0.5, 0.9]);\n\n      // Class A should have highest activation in first output\n      expect(predictA[0]).toBeGreaterThan(predictA[1]);\n      expect(predictA[0]).toBeGreaterThan(predictA[2]);\n\n      // Class B should have highest activation in second output\n      expect(predictB[1]).toBeGreaterThan(predictB[0]);\n      expect(predictB[1]).toBeGreaterThan(predictB[2]);\n\n      // Class C should have highest activation in third output\n      expect(predictC[2]).toBeGreaterThan(predictC[0]);\n      expect(predictC[2]).toBeGreaterThan(predictC[1]);\n    });\n  });\n\n  describe('Gradient Descent Properties', () => {\n    it('should reduce error monotonically with sufficient small learning rate', () => {\n      const network = new NeuralNetwork([2, 3, 1]);\n      const data = [\n        { input: [0.2, 0.3], output: [0.5] },\n        { input: [0.4, 0.6], output: [0.8] },\n        { input: [0.7, 0.2], output: [0.3] }\n      ];\n\n      const errors: number[] = [];\n      \n      // Track error over epochs\n      for (let i = 0; i < 10; i++) {\n        const result = network.train(data, { epochs: 100, learningRate: 0.1 });\n        errors.push(result.finalError);\n      }\n\n      // Error should generally decrease (allowing for small fluctuations)\n      let decreasing = 0;\n      for (let i = 1; i < errors.length; i++) {\n        if (errors[i] <= errors[i - 1]) decreasing++;\n      }\n      \n      expect(decreasing).toBeGreaterThan(errors.length * 0.7); // 70% should be decreasing\n    });\n  });\n});\n\n/**\n * Classical TDD Principles Demonstrated:\n * \n * 1. No mocks - testing actual neural network behavior\n * 2. Focus on mathematical correctness and convergence\n * 3. Test algorithm properties (gradient descent, separability)\n * 4. Verify computation results, not interactions\n * 5. Performance and accuracy are the key metrics\n * \n * This is ideal for:\n * - Neural network algorithms\n * - Mathematical computations\n * - Data transformations\n * - Performance-critical code\n */"],"version":3}