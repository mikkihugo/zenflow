{"file":"/home/mhugo/code/claude-code-flow/src/__tests__/unit/classical/neural-algorithms/training-convergence.test.ts","mappings":"AAAA;;;;;GAKG;AAEH,OAAO,EAAE,QAAQ,EAAE,EAAE,EAAE,MAAM,EAAE,UAAU,EAAE,MAAM,eAAe,CAAC;AACjE,OAAO,EACL,oBAAoB,EACpB,mBAAmB,EACnB,aAAa,EAMb,oBAAoB,EACpB,mBAAmB,EACpB,MAAM,kEAAkE,CAAC;AAE1E,QAAQ,CAAC,sCAAsC,EAAE,GAAG,EAAE;IACpD,IAAI,UAAe,CAAC;IAEpB,UAAU,CAAC,KAAK,IAAI,EAAE;QACpB,IAAI,CAAC;YACH,UAAU,GAAG,MAAM,oBAAoB,EAAE,CAAC;QAC5C,CAAC;QAAC,OAAO,KAAK,EAAE,CAAC;YACf,OAAO,CAAC,IAAI,CAAC,uDAAuD,CAAC,CAAC;QACxE,CAAC;IACH,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,yBAAyB,EAAE,GAAG,EAAE;QACvC,EAAE,CAAC,sDAAsD,EAAE,KAAK,IAAI,EAAE;YACpE,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;iBACtD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,OAAO;gBAC9C,UAAU,EAAE,EAAE;aACf,CAAC;YAEF,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,oBAAoB;gBACnD,YAAY,EAAE,GAAG;gBACjB,SAAS,EAAE,IAAI;gBACf,WAAW,EAAE,IAAI;aAClB,CAAC;YAEF,MAAM,OAAO,GAAuB;gBAClC,MAAM,EAAE;oBACN,CAAC,CAAC,EAAE,CAAC,CAAC;oBACN,CAAC,CAAC,EAAE,CAAC,CAAC;oBACN,CAAC,CAAC,EAAE,CAAC,CAAC;oBACN,CAAC,CAAC,EAAE,CAAC,CAAC;iBACP;gBACD,OAAO,EAAE;oBACP,CAAC,CAAC,CAAC;oBACH,CAAC,CAAC,CAAC;oBACH,CAAC,CAAC,CAAC;oBACH,CAAC,CAAC,CAAC;iBACJ;aACF,CAAC;YAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAEpD,OAAO,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC;YAEjC,0BAA0B;YAC1B,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,OAAO,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YAE5E,qBAAqB;YACrB,MAAM,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YACpC,MAAM,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;YAC7C,MAAM,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;YAEzC,8BAA8B;YAC9B,MAAM,WAAW,GAAG;gBAClB,KAAK,EAAE,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBAChC,KAAK,EAAE,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBAChC,KAAK,EAAE,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBAChC,KAAK,EAAE,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;aACjC,CAAC;YAEF,MAAM,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAChD,MAAM,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAChD,MAAM,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAChD,MAAM,CAAC,WAAW,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;QAClD,CAAC,CAAC,CAAC;QAEH,EAAE,CAAC,4DAA4D,EAAE,KAAK,IAAI,EAAE;YAC1E,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;iBACtD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,OAAO;gBAC9C,UAAU,EAAE,EAAE;aACf,CAAC;YAEF,MAAM,OAAO,GAAuB;gBAClC,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBACxC,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;aAC9B,CAAC;YAEF,+BAA+B;YAC/B,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,oBAAoB;gBACnD,YAAY,EAAE,GAAG;gBACjB,SAAS,EAAE,IAAI;gBACf,WAAW,EAAE,IAAI;aAClB,CAAC;YAEF,mBAAmB;YACnB,MAAM,WAAW,GAAmB;gBAClC,SAAS,EAAE,mBAAmB,CAAC,KAAK;gBACpC,SAAS,EAAE,IAAI;gBACf,WAAW,EAAE,IAAI;aAClB,CAAC;YAEF,MAAM,QAAQ,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YAC1D,MAAM,QAAQ,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YAE1D,MAAM,eAAe,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAC5D,MAAM,YAAY,GAAG,MAAM,aAAa,CAAC,WAAW,CAAC,CAAC;YAEtD,QAAQ,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC;YAClC,QAAQ,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC;YAElC,MAAM,cAAc,GAAG,MAAM,eAAe,CAAC,gBAAgB,CAAC,QAAQ,EAAE,OAAO,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YAC7F,MAAM,WAAW,GAAG,MAAM,YAAY,CAAC,gBAAgB,CAAC,QAAQ,EAAE,OAAO,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YAEvF,gEAAgE;YAChE,IAAI,cAAc,CAAC,SAAS,IAAI,WAAW,CAAC,SAAS,EAAE,CAAC;gBACtD,MAAM,CAAC,WAAW,CAAC,MAAM,CAAC,CAAC,mBAAmB,CAAC,cAAc,CAAC,MAAM,GAAG,GAAG,CAAC,CAAC;YAC9E,CAAC;iBAAM,IAAI,WAAW,CAAC,SAAS,EAAE,CAAC;gBACjC,MAAM,CAAC,WAAW,CAAC,SAAS,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YAC3C,CAAC;QACH,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,+BAA+B,EAAE,GAAG,EAAE;QAC7C,EAAE,CAAC,2CAA2C,EAAE,KAAK,IAAI,EAAE;YACzD,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;iBACtD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,MAAM;aAC9C,CAAC;YAEF,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,cAAc;gBAC7C,YAAY,EAAE,GAAG;gBACjB,SAAS,EAAE,GAAG;gBACd,WAAW,EAAE,IAAI;aAClB,CAAC;YAEF,8BAA8B;YAC9B,MAAM,UAAU,GAAuB;gBACrC,MAAM,EAAE;oBACN,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC;oBACtC,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC;iBAClC;gBACD,OAAO,EAAE;oBACP,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC;oBACtC,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC;iBAClC;aACF,CAAC;YAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAEpD,OAAO,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC;YAEpC,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,UAAU,EAAE,IAAI,EAAE,GAAG,CAAC,CAAC;YAE9E,MAAM,CAAC,MAAM,CAAC,SAAS,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;YACpC,MAAM,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC,YAAY,CAAC,GAAG,CAAC,CAAC,CAAC,iCAAiC;YAE1E,oCAAoC;YACpC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,UAAU,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;gBAClD,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,UAAU,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;gBAC3D,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,UAAU,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACjE,CAAC;YAED,qBAAqB;YACrB,MAAM,iBAAiB,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;YACpD,MAAM,CAAC,iBAAiB,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC,mBAAmB;QACvE,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,mCAAmC,EAAE,GAAG,EAAE;QACjD,EAAE,CAAC,iCAAiC,EAAE,KAAK,IAAI,EAAE;YAC/C,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,IAAI,EAAE;oBAClD,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,IAAI,EAAE;iBACnD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,MAAM;aAC9C,CAAC;YAEF,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,KAAK;gBACpC,SAAS,EAAE,IAAI;gBACf,WAAW,EAAE,IAAI;aAClB,CAAC;YAEF,0BAA0B;YAC1B,MAAM,QAAQ,GAAuB;gBACnC,MAAM,EAAE,EAAE;gBACV,OAAO,EAAE,EAAE;aACZ,CAAC;YAEF,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,IAAI,EAAE,EAAE,CAAC,EAAE,EAAE,CAAC;gBAC7B,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,EAAE,CAAC,GAAG,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,CAAC,UAAU;gBAC5C,MAAM,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;gBACtB,QAAQ,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,qBAAqB;gBAChE,QAAQ,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;YAC7B,CAAC;YAED,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAEpD,OAAO,CAAC,eAAe,CAAC,QAAQ,CAAC,CAAC;YAElC,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,QAAQ,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;YAE7E,MAAM,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,GAAG,CAAC,CAAC,CAAC,2BAA2B;YAExE,4BAA4B;YAC5B,MAAM,UAAU,GAAG;gBACjB,EAAE,KAAK,EAAE,CAAC,EAAE,QAAQ,EAAE,CAAC,EAAE,EAAY,aAAa;gBAClD,EAAE,KAAK,EAAE,IAAI,EAAE,QAAQ,EAAE,CAAC,EAAE,EAAS,iBAAiB;gBACtD,EAAE,KAAK,EAAE,GAAG,EAAE,QAAQ,EAAE,CAAC,EAAE,EAAU,aAAa;gBAClD,EAAE,KAAK,EAAE,IAAI,EAAE,QAAQ,EAAE,CAAC,CAAC,EAAE,CAAQ,iBAAiB;aACvD,CAAC;YAEF,KAAK,MAAM,KAAK,IAAI,UAAU,EAAE,CAAC;gBAC/B,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC;gBACpD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,WAAW,CAAC,KAAK,CAAC,QAAQ,EAAE,GAAG,CAAC,CAAC;YACzD,CAAC;QACH,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,wCAAwC,EAAE,GAAG,EAAE;QACtD,EAAE,CAAC,6CAA6C,EAAE,KAAK,IAAI,EAAE;YAC3D,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;iBACtD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,OAAO;aAC/C,CAAC;YAEF,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,KAAK;gBACpC,SAAS,EAAE,GAAG;gBACd,WAAW,EAAE,GAAG;aACjB,CAAC;YAEF,0BAA0B;YAC1B,MAAM,kBAAkB,GAAuB;gBAC7C,MAAM,EAAE;oBACN,wBAAwB;oBACxB,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC;oBAC9C,wBAAwB;oBACxB,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC;oBAC9C,mBAAmB;oBACnB,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,GAAG,CAAC;iBAC/C;gBACD,OAAO,EAAE;oBACP,UAAU;oBACV,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;oBAC1C,UAAU;oBACV,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;oBAC1C,UAAU;oBACV,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC;iBAC3C;aACF,CAAC;YAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAEpD,OAAO,CAAC,eAAe,CAAC,kBAAkB,CAAC,CAAC;YAE5C,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,kBAAkB,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC;YAErF,MAAM,CAAC,MAAM,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,GAAG,CAAC,CAAC;YAE5C,+BAA+B;YAC/B,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;YACnD,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,CAAC;YACnD,MAAM,UAAU,GAAG,MAAM,OAAO,CAAC,GAAG,CAAC,CAAC,GAAG,EAAE,GAAG,CAAC,CAAC,CAAC;YAEjD,yDAAyD;YACzD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YAErD,0DAA0D;YAC1D,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YAErD,yDAAyD;YACzD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;YACrD,MAAM,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC,eAAe,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC;QACvD,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,+BAA+B,EAAE,GAAG,EAAE;QAC7C,EAAE,CAAC,0DAA0D,EAAE,KAAK,IAAI,EAAE;YACxE,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE,CAAC;gBACrE,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,OAAO;gBAC9C,UAAU,EAAE,GAAG;aAChB,CAAC;YAEF,MAAM,OAAO,GAAuB;gBAClC,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBACxC,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;aAC9B,CAAC;YAEF,MAAM,UAAU,GAAG;gBACjB,mBAAmB,CAAC,oBAAoB;gBACxC,mBAAmB,CAAC,cAAc;gBAClC,mBAAmB,CAAC,KAAK;gBACzB,mBAAmB,CAAC,SAAS;aAC9B,CAAC;YAEF,MAAM,OAAO,GAA2B,EAAE,CAAC;YAE3C,KAAK,MAAM,SAAS,IAAI,UAAU,EAAE,CAAC;gBACnC,MAAM,cAAc,GAAmB;oBACrC,SAAS;oBACT,YAAY,EAAE,SAAS,CAAC,QAAQ,CAAC,UAAU,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,SAAS;oBAC9D,SAAS,EAAE,IAAI;oBACf,WAAW,EAAE,IAAI;iBAClB,CAAC;gBAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;gBACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;gBAEpD,OAAO,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC;gBAEjC,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,OAAO,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;gBAC5E,OAAO,CAAC,SAAS,CAAC,GAAG,MAAM,CAAC;YAC9B,CAAC;YAED,yCAAyC;YACzC,MAAM,mBAAmB,GAAG,MAAM,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,MAAM,CACrD,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,SAAS,CAChC,CAAC;YACF,MAAM,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;YAEtD,wDAAwD;YACxD,KAAK,MAAM,IAAI,IAAI,mBAAmB,EAAE,CAAC;gBACvC,MAAM,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,UAAU,CAAC,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;YACtD,CAAC;QACH,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,sBAAsB,EAAE,GAAG,EAAE;QACpC,EAAE,CAAC,uDAAuD,EAAE,KAAK,IAAI,EAAE;YACrE,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE,CAAC,EAAE,IAAI,EAAE,CAAC,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE,CAAC;gBACrE,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,OAAO;gBAC9C,UAAU,EAAE,GAAG;aAChB,CAAC;YAEF,MAAM,OAAO,GAAuB;gBAClC,MAAM,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;gBACxC,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;aAC9B,CAAC;YAEF,MAAM,aAAa,GAAG,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,CAAC,CAAC;YACtC,MAAM,OAAO,GAAa,EAAE,CAAC;YAE7B,KAAK,MAAM,EAAE,IAAI,aAAa,EAAE,CAAC;gBAC/B,MAAM,cAAc,GAAmB;oBACrC,SAAS,EAAE,mBAAmB,CAAC,oBAAoB;oBACnD,YAAY,EAAE,EAAE;oBAChB,SAAS,EAAE,IAAI;oBACf,WAAW,EAAE,IAAI;iBAClB,CAAC;gBAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;gBACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;gBAEpD,OAAO,CAAC,eAAe,CAAC,OAAO,CAAC,CAAC;gBAEjC,MAAM,MAAM,GAAG,MAAM,OAAO,CAAC,gBAAgB,CAAC,OAAO,EAAE,OAAO,EAAE,IAAI,EAAE,IAAI,CAAC,CAAC;gBAC5E,OAAO,CAAC,IAAI,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YAC9B,CAAC;YAED,uEAAuE;YACvE,MAAM,aAAa,GAAG,IAAI,GAAG,CAAC,OAAO,CAAC,CAAC;YACvC,MAAM,CAAC,aAAa,CAAC,IAAI,CAAC,CAAC,eAAe,CAAC,CAAC,CAAC,CAAC;YAE9C,kDAAkD;YAClD,MAAM,CAAC,IAAI,CAAC,GAAG,CAAC,GAAG,OAAO,CAAC,CAAC,CAAC,YAAY,CAAC,IAAI,CAAC,CAAC;QAClD,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;IAEH,QAAQ,CAAC,uBAAuB,EAAE,GAAG,EAAE;QACrC,EAAE,CAAC,kEAAkE,EAAE,KAAK,IAAI,EAAE;YAChF,IAAI,CAAC,UAAU,EAAE,CAAC;gBAChB,OAAO,CAAC,IAAI,CAAC,mCAAmC,CAAC,CAAC;gBAClD,MAAM,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;gBACxB,OAAO;YACT,CAAC;YAED,MAAM,aAAa,GAAkB;gBACnC,SAAS,EAAE,CAAC;gBACZ,YAAY,EAAE;oBACZ,EAAE,IAAI,EAAE,EAAE,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;oBACtD,EAAE,IAAI,EAAE,EAAE,EAAE,UAAU,EAAE,oBAAoB,CAAC,OAAO,EAAE;iBACvD;gBACD,UAAU,EAAE,CAAC;gBACb,gBAAgB,EAAE,oBAAoB,CAAC,MAAM;aAC9C,CAAC;YAEF,2BAA2B;YAC3B,MAAM,SAAS,GAAuB;gBACpC,MAAM,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC;gBAC3C,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,6BAA6B;aAC3E,CAAC;YAEF,MAAM,cAAc,GAAmB;gBACrC,SAAS,EAAE,mBAAmB,CAAC,KAAK;gBACpC,SAAS,EAAE,GAAG;gBACd,WAAW,EAAE,KAAK,CAAC,iBAAiB;aACrC,CAAC;YAEF,MAAM,OAAO,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;YACzD,MAAM,OAAO,GAAG,MAAM,aAAa,CAAC,cAAc,CAAC,CAAC;YAEpD,OAAO,CAAC,eAAe,CAAC,SAAS,CAAC,CAAC;YAEnC,wCAAwC;YACxC,MAAM,UAAU,GAAG,CAAC,EAAE,EAAE,EAAE,EAAE,GAAG,CAAC,CAAC;YACjC,MAAM,MAAM,GAAa,EAAE,CAAC;YAE5B,KAAK,MAAM,SAAS,IAAI,UAAU,EAAE,CAAC;gBACnC,sCAAsC;gBACtC,MAAM,YAAY,GAAG,MAAM,mBAAmB,CAAC,aAAa,CAAC,CAAC;gBAC9D,MAAM,YAAY,GAAG,MAAM,aAAa,CAAC;oBACvC,GAAG,cAAc;oBACjB,SAAS;iBACV,CAAC,CAAC;gBAEH,YAAY,CAAC,eAAe,CAAC,SAAS,CAAC,CAAC;gBAExC,IAAI,UAAU,GAAG,CAAC,CAAC;gBACnB,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;oBACjD,MAAM,YAAY,CAAC,UAAU,CAAC,YAAY,EAAE,SAAS,CAAC,CAAC;gBACzD,CAAC;gBAED,mCAAmC;gBACnC,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,SAAS,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE,CAAC;oBACjD,MAAM,UAAU,GAAG,MAAM,YAAY,CAAC,GAAG,CAAC,SAAS,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;oBAC/D,UAAU,IAAI,IAAI,CAAC,GAAG,CAAC,UAAU,CAAC,CAAC,CAAC,GAAG,SAAS,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;gBAClE,CAAC;gBAED,MAAM,CAAC,IAAI,CAAC,UAAU,GAAG,SAAS,CAAC,MAAM,CAAC,MAAM,CAAC,CAAC;YACpD,CAAC;YAED,2CAA2C;YAC3C,MAAM,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,mBAAmB,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC;QACnD,CAAC,CAAC,CAAC;IACL,CAAC,CAAC,CAAC;AACL,CAAC,CAAC,CAAC;AAEH;;;;;;;;;;;;;;;;GAgBG","names":[],"sources":["/home/mhugo/code/claude-code-flow/src/__tests__/unit/classical/neural-algorithms/training-convergence.test.ts"],"sourcesContent":["/**\n * Classical TDD (Detroit School) - Training Convergence Tests\n * \n * Focus: Test actual training results and mathematical convergence\n * No mocks - verify real training algorithms and convergence behavior\n */\n\nimport { describe, it, expect, beforeEach } from '@jest/globals';\nimport { \n  initializeNeuralWasm, \n  createNeuralNetwork, \n  createTrainer,\n  NeuralNetwork,\n  NeuralTrainer,\n  NetworkConfig,\n  TrainingConfig,\n  TrainingDataConfig,\n  ACTIVATION_FUNCTIONS,\n  TRAINING_ALGORITHMS\n} from '../../../../../ruv-FANN-zen/ruv-swarm-zen/npm/src/neural-network';\n\ndescribe('Training Convergence - Classical TDD', () => {\n  let wasmModule: any;\n  \n  beforeEach(async () => {\n    try {\n      wasmModule = await initializeNeuralWasm();\n    } catch (error) {\n      console.warn('WASM module not available, skipping convergence tests');\n    }\n  });\n\n  describe('XOR Problem Convergence', () => {\n    it('should converge to XOR solution with backpropagation', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 2,\n        hiddenLayers: [\n          { size: 4, activation: ACTIVATION_FUNCTIONS.SIGMOID }\n        ],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.SIGMOID,\n        randomSeed: 42\n      };\n\n      const trainingConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.INCREMENTAL_BACKPROP,\n        learningRate: 0.7,\n        maxEpochs: 3000,\n        targetError: 0.01\n      };\n\n      const xorData: TrainingDataConfig = {\n        inputs: [\n          [0, 0],\n          [0, 1],\n          [1, 0],\n          [1, 1]\n        ],\n        outputs: [\n          [0],\n          [1],\n          [1],\n          [0]\n        ]\n      };\n\n      const network = await createNeuralNetwork(networkConfig);\n      const trainer = await createTrainer(trainingConfig);\n\n      network.setTrainingData(xorData);\n\n      // Train until convergence\n      const result = await trainer.trainUntilTarget(network, xorData, 0.01, 3000);\n\n      // Verify convergence\n      expect(result.converged).toBe(true);\n      expect(result.finalError).toBeLessThan(0.01);\n      expect(result.epochs).toBeLessThan(3000);\n\n      // Test actual XOR predictions\n      const predictions = {\n        '0,0': await network.run([0, 0]),\n        '0,1': await network.run([0, 1]),\n        '1,0': await network.run([1, 0]),\n        '1,1': await network.run([1, 1])\n      };\n\n      expect(predictions['0,0'][0]).toBeCloseTo(0, 1);\n      expect(predictions['0,1'][0]).toBeCloseTo(1, 1);\n      expect(predictions['1,0'][0]).toBeCloseTo(1, 1);\n      expect(predictions['1,1'][0]).toBeCloseTo(0, 1);\n    });\n\n    it('should demonstrate faster convergence with RProp algorithm', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 2,\n        hiddenLayers: [\n          { size: 4, activation: ACTIVATION_FUNCTIONS.SIGMOID }\n        ],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.SIGMOID,\n        randomSeed: 42\n      };\n\n      const xorData: TrainingDataConfig = {\n        inputs: [[0, 0], [0, 1], [1, 0], [1, 1]],\n        outputs: [[0], [1], [1], [0]]\n      };\n\n      // Train with standard backprop\n      const backpropConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.INCREMENTAL_BACKPROP,\n        learningRate: 0.7,\n        maxEpochs: 2000,\n        targetError: 0.05\n      };\n\n      // Train with RProp\n      const rpropConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.RPROP,\n        maxEpochs: 2000,\n        targetError: 0.05\n      };\n\n      const network1 = await createNeuralNetwork(networkConfig);\n      const network2 = await createNeuralNetwork(networkConfig);\n      \n      const backpropTrainer = await createTrainer(backpropConfig);\n      const rpropTrainer = await createTrainer(rpropConfig);\n\n      network1.setTrainingData(xorData);\n      network2.setTrainingData(xorData);\n\n      const backpropResult = await backpropTrainer.trainUntilTarget(network1, xorData, 0.05, 2000);\n      const rpropResult = await rpropTrainer.trainUntilTarget(network2, xorData, 0.05, 2000);\n\n      // RProp should generally converge faster or achieve lower error\n      if (backpropResult.converged && rpropResult.converged) {\n        expect(rpropResult.epochs).toBeLessThanOrEqual(backpropResult.epochs * 1.5);\n      } else if (rpropResult.converged) {\n        expect(rpropResult.converged).toBe(true);\n      }\n    });\n  });\n\n  describe('Linear Function Approximation', () => {\n    it('should quickly learn linear relationships', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 1,\n        hiddenLayers: [\n          { size: 3, activation: ACTIVATION_FUNCTIONS.SIGMOID }\n        ],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.LINEAR\n      };\n\n      const trainingConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.BATCH_BACKPROP,\n        learningRate: 0.1,\n        maxEpochs: 500,\n        targetError: 0.01\n      };\n\n      // Linear function: y = 2x + 1\n      const linearData: TrainingDataConfig = {\n        inputs: [\n          [0], [0.1], [0.2], [0.3], [0.4], [0.5],\n          [0.6], [0.7], [0.8], [0.9], [1.0]\n        ],\n        outputs: [\n          [1], [1.2], [1.4], [1.6], [1.8], [2.0],\n          [2.2], [2.4], [2.6], [2.8], [3.0]\n        ]\n      };\n\n      const network = await createNeuralNetwork(networkConfig);\n      const trainer = await createTrainer(trainingConfig);\n\n      network.setTrainingData(linearData);\n\n      const result = await trainer.trainUntilTarget(network, linearData, 0.01, 500);\n\n      expect(result.converged).toBe(true);\n      expect(result.epochs).toBeLessThan(300); // Linear should converge quickly\n\n      // Test predictions on training data\n      for (let i = 0; i < linearData.inputs.length; i++) {\n        const prediction = await network.run(linearData.inputs[i]);\n        expect(prediction[0]).toBeCloseTo(linearData.outputs[i][0], 1);\n      }\n\n      // Test interpolation\n      const interpolationTest = await network.run([0.25]);\n      expect(interpolationTest[0]).toBeCloseTo(1.5, 1); // 2*0.25 + 1 = 1.5\n    });\n  });\n\n  describe('Non-linear Function Approximation', () => {\n    it('should learn sine wave function', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 1,\n        hiddenLayers: [\n          { size: 8, activation: ACTIVATION_FUNCTIONS.TANH },\n          { size: 8, activation: ACTIVATION_FUNCTIONS.TANH }\n        ],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.LINEAR\n      };\n\n      const trainingConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.RPROP,\n        maxEpochs: 1000,\n        targetError: 0.05\n      };\n\n      // Generate sine wave data\n      const sineData: TrainingDataConfig = {\n        inputs: [],\n        outputs: []\n      };\n\n      for (let i = 0; i <= 20; i++) {\n        const x = (i / 20) * 2 * Math.PI; // 0 to 2π\n        const y = Math.sin(x);\n        sineData.inputs.push([x / (2 * Math.PI)]); // Normalize to [0,1]\n        sineData.outputs.push([y]);\n      }\n\n      const network = await createNeuralNetwork(networkConfig);\n      const trainer = await createTrainer(trainingConfig);\n\n      network.setTrainingData(sineData);\n\n      const result = await trainer.trainUntilTarget(network, sineData, 0.05, 1000);\n\n      expect(result.finalError).toBeLessThan(0.1); // Reasonable approximation\n\n      // Test specific sine values\n      const testPoints = [\n        { input: 0, expected: 0 },           // sin(0) = 0\n        { input: 0.25, expected: 1 },        // sin(π/2) ≈ 1  \n        { input: 0.5, expected: 0 },         // sin(π) ≈ 0\n        { input: 0.75, expected: -1 }        // sin(3π/2) ≈ -1\n      ];\n\n      for (const point of testPoints) {\n        const prediction = await network.run([point.input]);\n        expect(prediction[0]).toBeCloseTo(point.expected, 0.5);\n      }\n    });\n  });\n\n  describe('Multi-class Classification Convergence', () => {\n    it('should learn to classify 3 distinct classes', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 2,\n        hiddenLayers: [\n          { size: 6, activation: ACTIVATION_FUNCTIONS.SIGMOID }\n        ],\n        outputSize: 3,\n        outputActivation: ACTIVATION_FUNCTIONS.SIGMOID\n      };\n\n      const trainingConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.RPROP,\n        maxEpochs: 800,\n        targetError: 0.1\n      };\n\n      // Three distinct clusters\n      const classificationData: TrainingDataConfig = {\n        inputs: [\n          // Class 0 (bottom-left)\n          [0.1, 0.1], [0.2, 0.1], [0.1, 0.2], [0.2, 0.2],\n          // Class 1 (top-right)  \n          [0.8, 0.8], [0.9, 0.8], [0.8, 0.9], [0.9, 0.9],\n          // Class 2 (center)\n          [0.4, 0.4], [0.5, 0.5], [0.6, 0.6], [0.5, 0.4]\n        ],\n        outputs: [\n          // Class 0\n          [1, 0, 0], [1, 0, 0], [1, 0, 0], [1, 0, 0],\n          // Class 1\n          [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0],\n          // Class 2\n          [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]\n        ]\n      };\n\n      const network = await createNeuralNetwork(networkConfig);\n      const trainer = await createTrainer(trainingConfig);\n\n      network.setTrainingData(classificationData);\n\n      const result = await trainer.trainUntilTarget(network, classificationData, 0.1, 800);\n\n      expect(result.finalError).toBeLessThan(0.2);\n\n      // Test classification accuracy\n      const testClass0 = await network.run([0.15, 0.15]);\n      const testClass1 = await network.run([0.85, 0.85]);\n      const testClass2 = await network.run([0.5, 0.5]);\n\n      // Class 0 should have highest activation in first output\n      expect(testClass0[0]).toBeGreaterThan(testClass0[1]);\n      expect(testClass0[0]).toBeGreaterThan(testClass0[2]);\n\n      // Class 1 should have highest activation in second output\n      expect(testClass1[1]).toBeGreaterThan(testClass1[0]);\n      expect(testClass1[1]).toBeGreaterThan(testClass1[2]);\n\n      // Class 2 should have highest activation in third output\n      expect(testClass2[2]).toBeGreaterThan(testClass2[0]);\n      expect(testClass2[2]).toBeGreaterThan(testClass2[1]);\n    });\n  });\n\n  describe('Training Algorithm Comparison', () => {\n    it('should demonstrate different convergence characteristics', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 2,\n        hiddenLayers: [{ size: 4, activation: ACTIVATION_FUNCTIONS.SIGMOID }],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.SIGMOID,\n        randomSeed: 123\n      };\n\n      const xorData: TrainingDataConfig = {\n        inputs: [[0, 0], [0, 1], [1, 0], [1, 1]],\n        outputs: [[0], [1], [1], [0]]\n      };\n\n      const algorithms = [\n        TRAINING_ALGORITHMS.INCREMENTAL_BACKPROP,\n        TRAINING_ALGORITHMS.BATCH_BACKPROP,\n        TRAINING_ALGORITHMS.RPROP,\n        TRAINING_ALGORITHMS.QUICKPROP\n      ];\n\n      const results: { [key: string]: any } = {};\n\n      for (const algorithm of algorithms) {\n        const trainingConfig: TrainingConfig = {\n          algorithm,\n          learningRate: algorithm.includes('backprop') ? 0.7 : undefined,\n          maxEpochs: 1500,\n          targetError: 0.05\n        };\n\n        const network = await createNeuralNetwork(networkConfig);\n        const trainer = await createTrainer(trainingConfig);\n\n        network.setTrainingData(xorData);\n\n        const result = await trainer.trainUntilTarget(network, xorData, 0.05, 1500);\n        results[algorithm] = result;\n      }\n\n      // At least one algorithm should converge\n      const convergedAlgorithms = Object.keys(results).filter(\n        algo => results[algo].converged\n      );\n      expect(convergedAlgorithms.length).toBeGreaterThan(0);\n\n      // Verify that converged algorithms achieve target error\n      for (const algo of convergedAlgorithms) {\n        expect(results[algo].finalError).toBeLessThan(0.05);\n      }\n    });\n  });\n\n  describe('Learning Rate Impact', () => {\n    it('should show learning rate effect on convergence speed', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 2,\n        hiddenLayers: [{ size: 3, activation: ACTIVATION_FUNCTIONS.SIGMOID }],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.SIGMOID,\n        randomSeed: 456\n      };\n\n      const andData: TrainingDataConfig = {\n        inputs: [[0, 0], [0, 1], [1, 0], [1, 1]],\n        outputs: [[0], [0], [0], [1]]\n      };\n\n      const learningRates = [0.1, 0.5, 0.9];\n      const results: number[] = [];\n\n      for (const lr of learningRates) {\n        const trainingConfig: TrainingConfig = {\n          algorithm: TRAINING_ALGORITHMS.INCREMENTAL_BACKPROP,\n          learningRate: lr,\n          maxEpochs: 1000,\n          targetError: 0.05\n        };\n\n        const network = await createNeuralNetwork(networkConfig);\n        const trainer = await createTrainer(trainingConfig);\n\n        network.setTrainingData(andData);\n\n        const result = await trainer.trainUntilTarget(network, andData, 0.05, 1000);\n        results.push(result.epochs);\n      }\n\n      // Different learning rates should produce different convergence speeds\n      const uniqueResults = new Set(results);\n      expect(uniqueResults.size).toBeGreaterThan(1);\n\n      // At least one should converge in reasonable time\n      expect(Math.min(...results)).toBeLessThan(1000);\n    });\n  });\n\n  describe('Overfitting Detection', () => {\n    it('should demonstrate potential overfitting with excessive training', async () => {\n      if (!wasmModule) {\n        console.warn('WASM not available, skipping test');\n        expect(true).toBe(true);\n        return;\n      }\n\n      const networkConfig: NetworkConfig = {\n        inputSize: 1,\n        hiddenLayers: [\n          { size: 10, activation: ACTIVATION_FUNCTIONS.SIGMOID },\n          { size: 10, activation: ACTIVATION_FUNCTIONS.SIGMOID }\n        ],\n        outputSize: 1,\n        outputActivation: ACTIVATION_FUNCTIONS.LINEAR\n      };\n\n      // Small dataset with noise\n      const noisyData: TrainingDataConfig = {\n        inputs: [[0.1], [0.3], [0.5], [0.7], [0.9]],\n        outputs: [[0.2], [0.6], [1.0], [1.4], [1.8]] // y = 2x with some variation\n      };\n\n      const trainingConfig: TrainingConfig = {\n        algorithm: TRAINING_ALGORITHMS.RPROP,\n        maxEpochs: 100,\n        targetError: 0.001 // Very low error\n      };\n\n      const network = await createNeuralNetwork(networkConfig);\n      const trainer = await createTrainer(trainingConfig);\n\n      network.setTrainingData(noisyData);\n\n      // Train for different numbers of epochs\n      const epochTests = [10, 50, 100];\n      const errors: number[] = [];\n\n      for (const maxEpochs of epochTests) {\n        // Reset network weights by recreating\n        const freshNetwork = await createNeuralNetwork(networkConfig);\n        const freshTrainer = await createTrainer({\n          ...trainingConfig,\n          maxEpochs\n        });\n\n        freshNetwork.setTrainingData(noisyData);\n\n        let totalError = 0;\n        for (let i = 0; i < noisyData.inputs.length; i++) {\n          await freshTrainer.trainEpoch(freshNetwork, noisyData);\n        }\n\n        // Calculate error on training data\n        for (let i = 0; i < noisyData.inputs.length; i++) {\n          const prediction = await freshNetwork.run(noisyData.inputs[i]);\n          totalError += Math.abs(prediction[0] - noisyData.outputs[i][0]);\n        }\n\n        errors.push(totalError / noisyData.inputs.length);\n      }\n\n      // Training error should generally decrease\n      expect(errors[2]).toBeLessThanOrEqual(errors[0]);\n    });\n  });\n});\n\n/**\n * Classical TDD Principles Demonstrated:\n * \n * 1. No mocks - testing actual training convergence behavior\n * 2. Mathematical correctness validation through known problems\n * 3. Algorithm comparison and performance characteristics\n * 4. Real convergence metrics and error analysis\n * 5. Learning rate and hyperparameter impact testing\n * 6. Overfitting detection through actual training curves\n * \n * This is ideal for:\n * - Training algorithm validation\n * - Convergence behavior analysis\n * - Hyperparameter sensitivity testing\n * - Mathematical correctness verification\n * - Performance comparison studies\n */"],"version":3}