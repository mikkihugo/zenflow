{
  "model_name": "LSTMCodingOptimizer",
  "version": "1.0.0",
  "description": "Optimized LSTM model for coding swarm tasks including bug fixing and code generation",
  "architecture": {
    "type": "sequence_to_sequence",
    "encoder": {
      "layers": [
        {
          "type": "embedding",
          "input_dim": 50000,
          "output_dim": 512,
          "mask_zero": true
        },
        {
          "type": "lstm",
          "units": 1024,
          "return_sequences": true,
          "dropout": 0.3,
          "recurrent_dropout": 0.3,
          "activation": "tanh",
          "recurrent_activation": "sigmoid"
        },
        {
          "type": "lstm",
          "units": 1024,
          "return_sequences": true,
          "dropout": 0.3,
          "recurrent_dropout": 0.3
        },
        {
          "type": "lstm",
          "units": 512,
          "return_sequences": false,
          "dropout": 0.2,
          "recurrent_dropout": 0.2
        }
      ]
    },
    "decoder": {
      "layers": [
        {
          "type": "repeat_vector",
          "n": 100
        },
        {
          "type": "lstm",
          "units": 512,
          "return_sequences": true,
          "dropout": 0.2,
          "recurrent_dropout": 0.2
        },
        {
          "type": "lstm",
          "units": 1024,
          "return_sequences": true,
          "dropout": 0.3,
          "recurrent_dropout": 0.3
        },
        {
          "type": "lstm",
          "units": 1024,
          "return_sequences": true,
          "dropout": 0.3,
          "recurrent_dropout": 0.3
        },
        {
          "type": "time_distributed",
          "layer": {
            "type": "dense",
            "units": 50000,
            "activation": "softmax"
          }
        }
      ]
    }
  },
  "hyperparameters": {
    "sequence_length": 100,
    "vocabulary_size": 50000,
    "embedding_dim": 512,
    "hidden_units": [1024, 1024, 512],
    "learning_rate": 0.001,
    "batch_size": 32,
    "epochs": 100,
    "patience": 10,
    "validation_split": 0.2,
    "optimizer": {
      "type": "adam",
      "beta_1": 0.9,
      "beta_2": 0.999,
      "epsilon": 1e-8,
      "amsgrad": false
    },
    "loss_function": "sparse_categorical_crossentropy",
    "metrics": ["accuracy", "perplexity"]
  },
  "training_config": {
    "gradient_clipping": 1.0,
    "weight_decay": 0.0001,
    "early_stopping": {
      "monitor": "val_loss",
      "patience": 10,
      "restore_best_weights": true
    },
    "reduce_lr_on_plateau": {
      "monitor": "val_loss",
      "factor": 0.5,
      "patience": 5,
      "min_lr": 1e-6
    }
  },
  "specialized_layers": {
    "attention_mechanism": {
      "type": "bahdanau",
      "units": 256,
      "use_coverage": true
    },
    "copy_mechanism": {
      "enabled": true,
      "copy_prob_threshold": 0.5
    },
    "syntax_aware_encoding": {
      "enabled": true,
      "ast_embedding_dim": 128
    }
  },
  "task_specific_optimizations": {
    "bug_fixing": {
      "context_window": 200,
      "error_focus_weight": 2.0,
      "diff_aware_training": true
    },
    "code_generation": {
      "beam_search_width": 5,
      "length_penalty": 0.6,
      "coverage_penalty": 0.4
    },
    "code_completion": {
      "prefix_length": 50,
      "suffix_context": 25,
      "incremental_decoding": true
    }
  }
}
