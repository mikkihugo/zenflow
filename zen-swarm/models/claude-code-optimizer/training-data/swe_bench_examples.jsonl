{"timestamp": 1640995800000, "instance_id": "django__django-12345", "category": "bug_fixing", "difficulty": "medium", "prompt": {"original": "There's a bug in Django's admin interface where the date picker widget doesn't properly handle timezone conversions when displaying dates in different locales. The issue occurs when users in different timezones view the same model instance - they see different dates due to improper timezone handling in the JavaScript widget. Please analyze the Django admin date widget code, identify the root cause of the timezone conversion issue, and implement a fix that ensures consistent date display across all timezones while maintaining backward compatibility. Include comprehensive tests that cover edge cases like daylight saving time transitions and multiple timezone scenarios.", "tokens": 645}, "optimized_prompt": {"content": "Fix Django admin date picker timezone bug: inconsistent dates across locales. Analyze widget code, fix timezone conversion, maintain backward compatibility, add DST + multi-timezone tests", "tokens": 198}, "performance": {"execution_time_ms": 8200, "quality_score": 0.91, "task_completion": true, "files_modified": 4, "tests_added": 12}, "optimization_metrics": {"token_reduction": 0.693, "semantic_preservation": 0.93, "solve_success": true}}
{"timestamp": 1640995860000, "instance_id": "flask__flask-5678", "category": "bug_fixing", "difficulty": "hard", "prompt": {"original": "Our Flask application is experiencing intermittent crashes with a 'RuntimeError: working outside of application context' error that only occurs under high load conditions. The error seems to be related to background tasks that are trying to access the Flask application context after the request has ended. The issue is difficult to reproduce in development but consistently happens in production. Please investigate the Flask application context management, identify scenarios where background tasks lose context, and implement a robust solution that properly handles context propagation for asynchronous operations. Include proper error handling and logging to help diagnose similar issues in the future.", "tokens": 712}, "optimized_prompt": {"content": "Fix Flask 'working outside application context' crash under load. Background tasks lose context after request ends. Investigate context mgmt, fix async context propagation, add error handling + logging", "tokens": 213}, "performance": {"execution_time_ms": 9800, "quality_score": 0.88, "task_completion": true, "files_modified": 6, "tests_added": 18}, "optimization_metrics": {"token_reduction": 0.701, "semantic_preservation": 0.90, "solve_success": true}}
{"timestamp": 1640995920000, "instance_id": "requests__requests-9012", "category": "feature_implementation", "difficulty": "medium", "prompt": {"original": "Implement support for HTTP/2 in the Python requests library. This should include automatic protocol negotiation with servers that support HTTP/2, connection multiplexing for improved performance, and proper handling of HTTP/2-specific features like server push and stream prioritization. The implementation should be backward compatible with existing HTTP/1.1 functionality and not break any existing APIs. Add comprehensive documentation explaining how to enable HTTP/2 support and what performance improvements users can expect. Include benchmarks comparing HTTP/1.1 vs HTTP/2 performance for typical use cases.", "tokens": 598}, "optimized_prompt": {"content": "Add HTTP/2 support to requests library: auto protocol negotiation, connection multiplexing, server push, stream priority. Maintain HTTP/1.1 compatibility, add docs + benchmarks", "tokens": 179}, "performance": {"execution_time_ms": 12400, "quality_score": 0.86, "task_completion": true, "files_modified": 8, "tests_added": 24}, "optimization_metrics": {"token_reduction": 0.701, "semantic_preservation": 0.88, "solve_success": true}}
{"timestamp": 1640995980000, "instance_id": "numpy__numpy-3456", "category": "feature_implementation", "difficulty": "hard", "prompt": {"original": "Implement a new NumPy function called 'rolling_window' that creates sliding window views of arrays efficiently without copying data. The function should support multi-dimensional arrays, configurable window sizes for each axis, and various stepping patterns. Include support for padding options, edge handling strategies, and integration with existing NumPy broadcasting rules. The implementation should leverage NumPy's stride tricks for memory efficiency and provide comparable performance to specialized libraries. Add comprehensive documentation with usage examples, performance characteristics, and comparison with alternative approaches.", "tokens": 678}, "optimized_prompt": {"content": "Implement NumPy 'rolling_window' function: sliding windows without data copy. Support multi-dim arrays, configurable window sizes, stepping, padding, edge handling. Use stride tricks for efficiency + docs", "tokens": 197}, "performance": {"execution_time_ms": 14200, "quality_score": 0.84, "task_completion": true, "files_modified": 5, "tests_added": 28}, "optimization_metrics": {"token_reduction": 0.709, "semantic_preservation": 0.86, "solve_success": false}}
{"timestamp": 1641006040000, "instance_id": "pandas__pandas-7890", "category": "refactoring", "difficulty": "medium", "prompt": {"original": "Refactor the pandas DataFrame groupby implementation to improve memory efficiency and performance for large datasets. The current implementation creates unnecessary intermediate objects and doesn't efficiently handle memory allocation for grouped operations. Identify bottlenecks in the groupby code path, optimize memory usage patterns, and implement lazy evaluation where possible. Ensure all existing functionality is preserved and that the refactoring doesn't break any existing APIs or behaviors. Include performance benchmarks showing improvement in memory usage and execution time for typical groupby operations on large datasets.", "tokens": 587}, "optimized_prompt": {"content": "Refactor pandas DataFrame groupby: improve memory efficiency + performance for large datasets. Remove intermediate objects, optimize allocation, add lazy evaluation. Preserve APIs + add benchmarks", "tokens": 174}, "performance": {"execution_time_ms": 11600, "quality_score": 0.89, "task_completion": true, "files_modified": 12, "tests_added": 20}, "optimization_metrics": {"token_reduction": 0.704, "semantic_preservation": 0.91, "solve_success": true}}
{"timestamp": 1641006100000, "instance_id": "scikit-learn__scikit-learn-1234", "category": "refactoring", "difficulty": "hard", "prompt": {"original": "Refactor the scikit-learn RandomForestClassifier to support incremental learning and online training scenarios. The current implementation requires loading the entire dataset into memory and retraining from scratch when new data arrives. Modify the algorithm to support partial fitting with new data while maintaining the statistical properties of the ensemble. This involves implementing tree updating mechanisms, managing forest composition over time, and ensuring that the incremental updates don't degrade model performance. Include comprehensive evaluation comparing batch vs incremental training performance and accuracy.", "tokens": 698}, "optimized_prompt": {"content": "Refactor scikit-learn RandomForestClassifier for incremental learning: support partial fitting, tree updates, dynamic forest composition. Maintain ensemble properties + compare batch vs incremental performance", "tokens": 201}, "performance": {"execution_time_ms": 16800, "quality_score": 0.82, "task_completion": false, "files_modified": 15, "tests_added": 32}, "optimization_metrics": {"token_reduction": 0.712, "semantic_preservation": 0.84, "solve_success": false}}
{"timestamp": 1641006160000, "instance_id": "tensorflow__tensorflow-5678", "category": "bug_fixing", "difficulty": "easy", "prompt": {"original": "Fix a bug in TensorFlow's gradient computation where certain edge cases with zero-dimensional tensors cause incorrect gradients to be calculated. The issue occurs when using tf.reduce_sum on scalar tensors in custom gradient functions, leading to shape mismatches and runtime errors. Identify the specific conditions that trigger this bug, implement a proper fix that handles zero-dimensional tensors correctly, and add regression tests to prevent similar issues in the future.", "tokens": 432}, "optimized_prompt": {"content": "Fix TensorFlow gradient bug: zero-dim tensors cause incorrect gradients with tf.reduce_sum on scalars. Fix shape mismatches + add regression tests", "tokens": 123}, "performance": {"execution_time_ms": 4200, "quality_score": 0.93, "task_completion": true, "files_modified": 3, "tests_added": 8}, "optimization_metrics": {"token_reduction": 0.715, "semantic_preservation": 0.95, "solve_success": true}}
{"timestamp": 1641006220000, "instance_id": "pytorch__pytorch-9012", "category": "feature_implementation", "difficulty": "easy", "prompt": {"original": "Add support for a new activation function called 'Swish' to PyTorch's neural network module. The Swish activation function is defined as f(x) = x * sigmoid(x) and has shown promising results in various deep learning applications. Implement the function with proper forward and backward passes, ensure it's compatible with automatic differentiation, and add it to the standard activation functions available in torch.nn. Include comprehensive unit tests and documentation with usage examples.", "tokens": 456}, "optimized_prompt": {"content": "Add Swish activation (f(x) = x * sigmoid(x)) to PyTorch nn module: implement forward/backward passes, autograd compatibility, unit tests + docs", "tokens": 119}, "performance": {"execution_time_ms": 3600, "quality_score": 0.95, "task_completion": true, "files_modified": 4, "tests_added": 15}, "optimization_metrics": {"token_reduction": 0.739, "semantic_preservation": 0.97, "solve_success": true}}
{"timestamp": 1641006280000, "instance_id": "matplotlib__matplotlib-3456", "category": "refactoring", "difficulty": "easy", "prompt": {"original": "Refactor matplotlib's color handling code to improve consistency and maintainability. The current implementation has scattered color parsing logic across multiple modules, making it difficult to maintain and extend. Consolidate the color handling functionality into a centralized module, standardize the color validation and conversion processes, and ensure backward compatibility with existing color specifications. Update all references throughout the codebase to use the new centralized color handling system.", "tokens": 478}, "optimized_prompt": {"content": "Refactor matplotlib color handling: consolidate scattered parsing logic into central module, standardize validation/conversion, maintain backward compatibility", "tokens": 138}, "performance": {"execution_time_ms": 5800, "quality_score": 0.91, "task_completion": true, "files_modified": 18, "tests_added": 12}, "optimization_metrics": {"token_reduction": 0.711, "semantic_preservation": 0.93, "solve_success": true}}
{"timestamp": 1641006340000, "instance_id": "sqlalchemy__sqlalchemy-7890", "category": "bug_fixing", "difficulty": "medium", "prompt": {"original": "There's a performance regression in SQLAlchemy's query optimization where certain complex JOIN operations with subqueries are generating inefficient SQL that causes significant slowdowns in production databases. The issue appears to be related to the query planner not properly recognizing optimization opportunities when dealing with nested subqueries and multiple table joins. Analyze the SQL generation code, identify the cause of the regression, and implement optimizations that generate more efficient SQL while maintaining query correctness and existing functionality.", "tokens": 534}, "optimized_prompt": {"content": "Fix SQLAlchemy query optimization regression: complex JOINs with subqueries generate inefficient SQL. Analyze generation code, optimize nested subqueries + multi-table joins", "tokens": 148}, "performance": {"execution_time_ms": 7200, "quality_score": 0.87, "task_completion": true, "files_modified": 7, "tests_added": 16}, "optimization_metrics": {"token_reduction": 0.723, "semantic_preservation": 0.89, "solve_success": true}}